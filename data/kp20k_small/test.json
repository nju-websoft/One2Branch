[
  {
    "source": "a feedback vertex set of <digit> degenerate graphs . <eos> a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is <digit> degenerate if each subgraph g g of g has a vertex v such that dg ( v ) <digit> d g ( v ) <digit> . in this paper , we prove that ( g ) 2n <digit> ( g ) <digit> n <digit> for any <digit> degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given <digit> degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n <digit> <digit> n <digit> .",
    "target": "feedback vertex set;<digit> degenerate graphs;decycling set"
  },
  {
    "source": "hybrid analytical modeling of pending cache hits , data prefetching , and mshrs . <eos> this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .",
    "target": "analytical modeling;data prefetching;performance;miss status holding register;pending hit"
  },
  {
    "source": "autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome . <eos> autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .",
    "target": "apeced;aire;chronic mucocutaneous candidiasis;il <digit>;il <digit>"
  },
  {
    "source": "numerical solution of a three dimensional solidification problem in aluminium casting . <eos> in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .",
    "target": "casting;convection;finite element;thermal;conduction"
  },
  {
    "source": "definition and recognition of rib features in aircraft structural part . <eos> in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .",
    "target": "rib;aircraft structural part;feature recognition;machining feature"
  },
  {
    "source": "an algebraic approach to guarantee harmonic balance method using grobner base . <eos> harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .",
    "target": "harmonic balance method;grobner base;error bound;algebraic representation;singular point;quadratic approximation"
  },
  {
    "source": "a graph coloring based tdma scheduling algorithm for wireless sensor networks . <eos> wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance <digit> coloring algorithm and can get colors near to ( ( updelta <digit> ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .",
    "target": "graph coloring;tdma;distributed;real time;clustering"
  },
  {
    "source": "building model as a service to support geosciences . <eos> model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .",
    "target": "cloud computing;web service;geospatial data;model web;earthcube;big data"
  },
  {
    "source": "shot change detection using scene based constraint . <eos> a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .",
    "target": "shot change detection;optical flow;video segmentation"
  },
  {
    "source": "tail asymptotics for hol priority queues handling a large number of independent stationary sources . <eos> in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .",
    "target": "priority queues;bahadur rao theorem;large deviations;cell loss;stationary processes;tail distributions"
  },
  {
    "source": "a variant of parallel plane sweep algorithm for multicore systems . <eos> parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .",
    "target": "plane sweep;multicore;physical design;rectangle intersection;computational geometry"
  },
  {
    "source": "the antecedents and consequents of user perceptions in information technology adoption . <eos> a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .",
    "target": "information technology adoption;personal innovativeness;communication channels;expert system adoption"
  },
  {
    "source": "improving classification with latent variable models by sequential constraint optimization . <eos> in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .",
    "target": "latent variable models;support vector machines;semi supervised learning;pca;vector quantization;image and character recognition"
  },
  {
    "source": "a framework for a real time intelligent and interactive brain computer interface . <eos> a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .",
    "target": "event related potentials;data collection;classification;electroencephalography;braincomputer interface"
  },
  {
    "source": "characterizing output processes of e m e k <digit> queues . <eos> our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k <digit> queues . we show , by numerical examples , that the interdeparture time of e m e k <digit> retains the ifr property if m > k. ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "ifr;erlang distribution;departure process;ph g <digit>;queueing theory"
  },
  {
    "source": "a low complexity down mixing structure on quadraphonic headsets for surround audio . <eos> this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately <digit> % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .",
    "target": "surround audio;head related transfer function;reverberation;virtual loudspeaker"
  },
  {
    "source": "security personalization for internet and web services . <eos> the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .",
    "target": "security;personalization;web services;internet services;security policy;negotiation"
  },
  {
    "source": "two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures . <eos> this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a <digit> nm cmos fabrication process . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "synchronization;networks on chip;globally asynchronous locally synchronous;multi processor systems on chip;asynchronous fifo"
  },
  {
    "source": "simulation aided design of organizational structures in manufacturing systems using structuring strategies . <eos> this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .",
    "target": "modeling and simulation of manufacturing systems;strategies for production systems design"
  },
  {
    "source": "explicit constructions of selectors and related combinatorial structures , with applications . <eos> in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .",
    "target": "applications;paper;object;randomization;optimality;direct;test;use"
  },
  {
    "source": "selective finite element refinement in torsional problems based on the membrane analogy . <eos> this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .",
    "target": "finite elements;torsion;membrane analogy;maximum shearing stress;selective h refinement"
  },
  {
    "source": "rns montgomery multiplication algorithm for duplicate processing of base transformations . <eos> this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for <digit> bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .",
    "target": "montgomery multiplication;base transformation;modular exponentiation;residue number systems;rsa cryptography"
  },
  {
    "source": "from quality in use to value in the world . <eos> this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .",
    "target": "quality;value;design;fit"
  },
  {
    "source": "comparative study of family <digit> gpcrs in fugu rubripes . <eos> abstract in this study , members of family <digit> gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family <digit> gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .",
    "target": "family <digit> gpcrs;teleost;duplicated genes;evolution"
  },
  {
    "source": "blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks . <eos> this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .",
    "target": "blotto game;complexity;fairness;subcarrier allocation;ofdma;efficiency;correlated fading"
  },
  {
    "source": "polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers . <eos> the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of <digit> under ambient conditions can be obtained .",
    "target": "dielectric properties;unsaturated polyester resin;embedded capacitors"
  },
  {
    "source": "an integration of online and pseudo online information for cursive word recognition . <eos> in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .",
    "target": "online;cursive;word recognition;offline;handwriting;classifier combination"
  },
  {
    "source": "generation of quasi gaussian pulses based on correlation techniques . <eos> the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .",
    "target": "gaussian pulse;correlation techniques;digital signal processing;spectral analysis;ultra wideband"
  },
  {
    "source": "learning linear pca with convex semi definite programming . <eos> the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .",
    "target": "semi definite programming;support vector machines;margin;robustness;principal component analysis;statistical learning theory;maximal margin algorithm"
  },
  {
    "source": "the neighborhood auditing tool a hybrid interface for auditing the umls . <eos> the umls 's integration of more than <digit> source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) <digit> elsevier inc. all rights reserved .",
    "target": "auditing tool;software tool;unified medical language system;auditing of terminologies;auditing of ontologies;auditing of the umls;user interface;hybrid diagram text user interface"
  },
  {
    "source": "exclusion regions for optimization problems . <eos> branch and bound methods for finding all solutions of a global optimization problem in a box frequently have the difficulty that subboxes containing no solution can not be easily eliminated if they are close to the global minimum . this has the effect that near each global minimum , and in the process of solving the problem also near the currently best found local minimum , many small boxes are created by repeated splitting , whose processing often dominates the total work spent on the global search . this paper discusses the reasons for the occurrence of this so called cluster effect , and how to reduce the cluster effect by defining exclusion regions around each local minimum found , that are guaranteed to contain no other local minimum and hence can safely be discarded . in addition , we will introduce a method for verifying the existence of a feasible point close to an approximate local minimum . these exclusion regions are constructed using uniqueness tests based on the krawczyk operator and make use of first , second and third order information on the objective and constraint functions .",
    "target": "exclusion region;branch and bound;global optimization;cluster effect;uniqueness test;krawczyk operator;validated enclosure;existence test;inclusion region;kantorovich theorem;backboxing;affine invariant;primary 65h20;65g30"
  },
  {
    "source": "learning about meetings . <eos> most people participate in meetings almost every day , multiple times a day . the study of meetings is important , but also challenging , as it requires an understanding of social signals and complex interpersonal dynamics . our aim in this work is to use a data driven approach to the science of meetings . we provide tentative evidence that ( i ) it is possible to automatically detect when during the meeting a key decision is taking place , from analyzing only the local dialogue acts , ( ii ) there are common patterns in the way social dialogue acts are interspersed throughout a meeting , ( iii ) at the time key decisions are made , the amount of time left in the meeting can be predicted from the amount of time that has passed , ( iv ) it is often possible to predict whether a proposal during a meeting will be accepted or rejected based entirely on the language ( the set of persuasive words ) used by the speaker .",
    "target": "persuasive words;analysis of meetings;applications of machine learning"
  },
  {
    "source": "design and implementation of an expert interface system for integration of photogrammetric and geographic information systems for intelligent preparation and structuring of spatial data . <eos> preparation of spatial data for geographic information system ( gis ) simultaneously during feature digitizing process from photogrammetric models reduces data editing phases after feature digitizing process . therefore , the problems , caused by separating spatial data production process from preparation of this data , are overcome as far as possible . to achieve this purpose , specialty and expertise required for spatial data structuring and preparation for gis , should be available in an interface system which establishes a direct connection between photogrammetric and gis systems . in this case , when a user digitizes a feature from a photogrammetric model , decision making process about the method of editing , structuring , layering , and storing of the feature in gis database , can be carried out by such an interface system . thus , according to the capabilities of expert systems for modeling the knowledge and deduction methods of experts , generating an expert interface system between photogrammetric and gis systems , offers a suitable solution for this integration . in this paper , the capabilities of expert systems for intelligent spatial data structuring and preparation simultaneously during feature digitizing process from photogrammetric models , have been investigated . also , design , implementation and test of an expert interface system for integration of photogrammetric and gis systems in order to take advantages of capabilities of both systems simultaneously as one integrated system , has been described .",
    "target": "integration;spatial data;gis;expert system;photogrammetry"
  },
  {
    "source": "wevan a mechanism for evidence creation and verification in vanets . <eos> there are traffic situations ( e.g. incorrect speeding tickets ) in which a given vehicles driving behavior at some point in time has to be proved to a third party . vehicle mounted sensorial devices are not suitable for this matter since they can be maliciously manipulated . however , surrounding vehicles may give their vision on another ones behavior . furthermore , these data may be shared with the affected vehicle through vanets . in this paper , a vanet enabled data exchange mechanism called wevan is presented . the goal of this mechanism is to build and verify evidences based on surrounding vehicles ( called witnesses ) testimonies . due to the short range nature of vanets , the connectivity to witnesses may be reduced with time the later their testimonies are requested , the lower the amount of witnesses may be . simulation results show that if testimonies are ordered 5s later , an average of <digit> testimonies may be collected in highway scenarios . other intervals and road settings are studied as well .",
    "target": "driving behavior;witness;digital evidence;vehicular ad hoc networks"
  },
  {
    "source": "extensional normalisation and type directed partial evaluation for typed lambda calculus with sums . <eos> we present a notion of eta long beta normal term for the typed lambda calculus with sums and prove , using grothendieck logical relations , that every term is equivalent to one in normal form . based on this development we give the first type directed partial evaluator that constructs normal forms of terms in this calculus .",
    "target": "normalisation;type directed partial evaluation;typed lambda calculus;grothendieck logical relations;strong sums"
  },
  {
    "source": "yet another write optimized dbms layer for flash based solid state storage . <eos> flash based solid state storage ( flashsss ) has write oriented problems such as low write throughput , and limited life time . especially , flashssds have a characteristic vulnerable to random writes , due to its control logic utilizing parallelism between the flash memory chips . in this paper , we present a write optimized layer of dbmss to address the write oriented problems of flashsss in on line transaction processing environments . the layer consists of a write optimized buffer , a corresponding log space , and an in memory mapping table , closely associated with a novel logging scheme called incremental logging ( icl ) . the icl scheme enables dbmss to reduce page writes at the least expense of additional page reads , while replacing random writes into sequential writes . through experiments , our approach demonstrated up to an order of magnitude performance enhancement in i o processing time compared to the original dbms , increasing the longevity of flashsss by approximately a factor of two .",
    "target": "flash memory;incremental logging;icl;ssd;write performance;database"
  },
  {
    "source": "wrinkle development analysis in thin sail like structures using mitc shell finite elements . <eos> we propose a method of modelling sail type structures which captures the wrinkling behaviour of such structures . the method is validated through experimental and analytical test cases , particularly in terms of wrinkling prediction . an enhanced wrinkling index is proposed as a valuable measure characterizing the global wrinkling development on the deformed structure . the method is based on a pseudo dynamic finite element procedure involving non linear mitc shell elements . the major advantage compared to membrane models generally used for this type of analysis is that no ad hoc wrinkling model is required to control the stability of the structure . we demonstrate our approach to analyse the behaviour of various structures with spherical and cylindrical shapes , characteristic of downwind sails over a rather wide range of shape and constitutive parameters . in all cases convergence is reached and the overall flying shape is most adequately represented , which shows that our approach is a most valuable alternative to standard techniques to provide deeper insight into the physical behaviour . limitations appear only in some very special instances in which local wrinkling related instabilities are extremely high and would require specific additional treatments , out of the scope of the present study .",
    "target": "wrinkling;mitc shells;wrinkling index;sail modelling"
  },
  {
    "source": "cliques , holes and the vertex coloring polytope . <eos> certain subgraphs of a given graph g restrict the minimum number chi ( g ) of colors that can be assigned to the vertices of g such that the endpoints of all edges receive distinct colors . some of such subgraphs are related to the celebrated strong perfect graph theorem . as it implies that every graph g contains a clique of size chi ( g ) , or an odd hole or an odd anti hole as an induced subgraph . in this paper , we investigate the impact of induced maximal cliques , odd holes and odd anti holes on the polytope associated with a new <digit> <digit> integer programming formulation of the graph coloring problem . we show that they induce classes of facet defining , inequalities . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "integer programming;graph colorings;combinatorial problems;facets of polyhedra"
  },
  {
    "source": "an experimental validation of a novel clustering approach to pwarx identification . <eos> in this paper , the problem of clustering based procedure for the identification of piecewise auto regressive exogenous ( pwarx ) models is addressed . this problem involves both the estimation of the parameters of the affine sub models and the hyperplanes defining the partitions of the state input regression . in fact , we propose the use of the chiu 's clustering algorithm in order to overcome the main drawbacks of the existing methods which are the poor initialization and the presence of outliers . in addition , our approach is able to generate automatically the number of sub models . simulation results are presented to illustrate the performance of the proposed method . an application of the developed approach to an olive oil esterification reactor is also suggested in order to validate simulation results .",
    "target": "experimental validation;clustering;identification;hybrid systems;pwarx models;chiu 's clustering technique"
  },
  {
    "source": "a primal dual approximation algorithm for the asymmetric prize collecting tsp . <eos> we present a primal dual log ( n ) approximation algorithm for the version of the asymmetric prize collecting traveling salesman problem , where the objective is to find a directed tour that visits a subset of vertices such that the length of the tour plus the sum of penalties associated with vertices not in the tour is as small as possible . the previous algorithm for the problem ( v.h. nguyen and t.t nguyen in int . j. math . oper . res . <digit> ( <digit> ) <digit> , <digit> ) which is not combinatorial , is based on the held karp relaxation and heuristic methods such as the frieze et al. s heuristic ( frieze et al. in networks <digit> <digit> , <digit> ) or the recent asadpour et al. s heuristic for the atsp ( asadpour etal . in 21st acm siam symposium on discrete algorithms , <digit> ) . depending on which of the two heuristics is used , it gives respectively <digit> log ( n ) and ( <digit> <digit> frac log ( n ) log ( log ( n ) ) ) as an approximation ratio . our algorithm achieves an approximation ratio of log ( n ) which is weaker than ( <digit> <digit> frac log ( n ) log ( log ( n ) ) ) but represents the first combinatorial approximation algorithm for the asymmetric prize collecting tsp .",
    "target": "approximation algorithm;prize collecting traveling salesman;primal dual algorithm"
  },
  {
    "source": "second order ambient intelligence . <eos> this text attempts to describe an imagined future of ambient intelligence . it assumes that one day most of the current issues within ambient intelligence will be solved and that a second order ambient intelligence will be formulated , one with new research agendas . it describes several topics and ideas that might be part of this agenda and surmises on the prerequisites for this change .",
    "target": "second order ambient intelligence;critique of ambient intelligence;temporal design;adaptive systems;long term behavior;animal machine interaction;critical futurism"
  },
  {
    "source": "pdms prism glass optical coupling for surface plasmon resonance sensors based on mems technology . <eos> a miniaturized surface plasmon resonance ( spr ) chip has been developed for biomedical and chemical analysis with low cost and high performance . the techniques of bulk silicon micromachining and polymer replication were used to fabricate the kretschmann spr sensor composed of a polydimethylsiloxane ( pdms ) prism , a coupling glass and microchannels . the plasmon properties of thin metal films were investigated theoretically based on fresnel analysis , with optical boundary conditions pertaining to the surface plasmon resonance at the gold water and gold air interfaces . the theoretical results show that difference in the refractive index ( ri ) between the pdms prism and the coupling glass layer affect the precision of the spr angle and the spr curve . meanwhile , the period of the interference fringe attaching on the spr curve increases with an increase in wavelength and a decrease in the refractive index of the coupling glass layer . the gold thickness of <digit> nm is required while employing a fixed incident wavelength of <digit> nm , to achieve optimum spr excitation conditions and the sensor sensitivity . the characteristics of this spr sensor were evaluated in the angular interrogation mode of employing the incident wavelength of <digit> nm in air and water media , respectively . the obtained spr angles were approximately consistent with the theoretical ones .",
    "target": "pdms;polymer;surface plasmon resolance;microfluidic"
  },
  {
    "source": "isogeometric analysis for strain field measurements . <eos> in this paper , the potential of isogeometric analysis for strain field measurement by digital image correlation is investigated . digital image correlation ( dic ) is a full field kinematics measurement technique based on gray level conservation principle and the formulation we adopt allows for using arbitrary displacement bases . the high continuity properties of non uniform rational b spline ( nurbs ) functions are exploited herein as an additional regularization of the initial ill posed problem . k refinement is analyzed on an artificial test case where the proposed methodology is shown to outperform the usual finite element based dic . finally a fatigue tensile test on a thin aluminum sheet is analyzed . strain localization occurs after a certain number of cycles and combination of nurbs into a dic algorithm clearly shows a great potential to improve the robustness of non linear constitutive law identification .",
    "target": "isogeometric analysis;strain field measurement;digital image correlation;nurbs"
  },
  {
    "source": "stable computation of the functional variation of the dirichletneumann operator . <eos> this paper presents an accurate and stable numerical scheme for computation of the first variation of the dirichletneumann operator in the context of eulers equations for ideal free surface fluid flows . the transformed field expansion methodology we use is not only numerically stable , but also employs a spectrally accurate fourier chebyshev collocation method which delivers high fidelity solutions . this implementation follows directly from the authors previous theoretical work on analyticity properties of functional variations of dirichletneumann operators . these variations can be computed in a number of ways , but we establish , via a variety of computational experiments , the superior effectiveness of our new approach as compared with another popular boundary perturbation algorithm ( the method of operator expansions ) .",
    "target": "functional variations;dirichletneumann operators;boundary perturbation methods;high order spectral methods;water waves"
  },
  {
    "source": "optimal tool selection for 2.5 d milling , part <digit> a solid modeling approach for construction of the voronoi mountain . <eos> cutter selection is a critical subtask of machining process planning . in this two part series , we develop a robust approach for the selection of an optimal set of milling cutters for a 2.5 d generalized pocket . in the first article ( part <digit> ) , we present a solid modeling approach for the construction of the voronoi mountain for the pocket geometry , which is a 3d extension of the voronoi diagram . the major contributions of this work include ( <digit> ) the development of a robust and systematic procedure for construction of the voronoi mountain for a multiply connected curvilinear polygon and ( b ) an extension of the voronoi mountain concept to handle open edges .",
    "target": "2.5 d milling;solid modelling;voronoi mountain;cutter selection;open edges"
  },
  {
    "source": "evaluating the novelty of text mined rules using lexical knowledge . <eos> in this paper , we present a new method of estimating the novelty of rules discovered by data mining methods using wordnet , a lexical knowledge base of english words . we assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule the more the average distance , more is the novelty of the rule . the novelty of rules extracted by the discotex text mining system on amazon.com book descriptions were evaluated by both human subjects and by our algorithm . by computing correlation coefficients between pairs of human ratings and between human and automatic ratings , we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another . text mining",
    "target": "novelty;wordnet;semantic distance;knowledge hierarchy;interesting rules"
  },
  {
    "source": "visor vast independence system optimization routine . <eos> an algorithm is sketched that generates all k maximal independent sets and all m minimal dependent sets of an arbitrary independence system , based on a set of cardinality n having at most <digit> ( n ) subsets . with access to an oracle that decides if a set is independent or not . because the algorithm generates all those sets , it solves the problems of finding all maximum independent and minimum dependent sets . those problems are known to be impossible to solve in general in time polynomial in n , k , and m , and they are np hard . the algorithm proposed and used is efficient in the sense that it requires only o ( nk m ) or o ( k nm ) visits to the oracle , the nonpolynomial part is only related to bitstring comparisons and the like , which can be performed rather quickly and , to some degree , in parallel on a sequential machine . this complexity compares favorably with another algorithm that is o ( n ( <digit> ) k ( <digit> ) ) . the design of a computer routine that implements the algorithm in a highly optimized way is discussed . the routine behaves as expected , as is shown by numerical experiments on a range of randomly generated independence systems with n up to n <digit> . application on an engineering design problem with n <digit> shows the routine requires almost <digit> ( <digit> ) times less visits to the oracle than an exhaustive search , while the time spent in visiting the oracle is still significantly larger than that spent for all other computations together .",
    "target": "independence system;maximal independent set;combinatorial optimization;input output selection"
  },
  {
    "source": "superconvergence in high order galerkin finite element methods . <eos> in this paper , we shall use local estimates to give the superconvergence of high order galerkin finite element method for the elliptic equation of second order with constant coefficients by using the symmetric technique and integral identity . we get improved superconvergence on the inner locally symmetric mesh with respect to a point x0 for rectangular and triangular meshes .",
    "target": "superconvergence;elliptic equations of second order;integral identities;locally symmetric meshes"
  },
  {
    "source": "a review of design pattern mining techniques . <eos> the quality of a software system highly depends on its architectural design . high quality software systems typically apply expert design experience which has been captured as design patterns . as demonstrated solutions to recurring problems , design patterns help to reuse expert experience in software system design . they have been extensively applied in the industry . mining the instances of design patterns from the source code of software systems can assist in the understanding of the systems and the process of re engineering them . more importantly , it also helps to trace back to the original design decisions , which are typically missing in legacy systems . this paper presents a review on current techniques and tools for mining design patterns from source code or design of software systems . we classify different approaches and analyze their results in a comparative study . we also examine the disparity of the discovery results of different approaches and analyze possible reasons with some insight .",
    "target": "design pattern;discovery;reverse engineering"
  },
  {
    "source": "stabilization of second order nonholonomic systems in canonical chained form . <eos> stabilization of a class of second order nonholonomic systems in canonical chained form is investigated in this paper . first , the models of two typical second order nonholonomic systems , namely , a three link planar manipulator with the third joint unactuated , and a kinematic redundant manipulator with all joints free and driven by forces torques imposing on the end effector , are presented and converted to second order chained form by transformations of coordinate and input . a discontinuous control law is then proposed to stabilize all states of the system to the desired equilibrium point exponentially . computer simulation is given to show the effectiveness of the proposed controller .",
    "target": "second order nonholonomic systems;canonical second order chained form;underactuated manipulator;discontinuous coordinate transformation;discontinuous stabilization"
  },
  {
    "source": "truthful mechanisms for two range values variant of unrelated scheduling . <eos> in this paper , we consider a restricted variant of the scheduling problem , where the machines are the strategic players . for this multi parameter mechanism design problem , the only known truthful mechanisms use task independent allocation algorithms and only have approximation ratio o ( m ) n. nisan , ronen . algorithmic mechanism design ( extended abstract ) , in stoc '99 proceedings of the thirty first annual acm symposium on theory of computing , acm , new york , ny , usa , <digit> . pp. <digit> <digit> a. mu'alem , m. schapira , setting lower bounds on truthfulness extended abstract , in soda '07 proceedings of the eighteenth annual acm siam symposium on discrete algorithms , society for industrial and applied mathematics , philadelphia . pa , usa , <digit> , pp. <digit> <digit> p. lu , c. yu , an improved randomized truthful mechanism for scheduling unrelated machines , in 25th international symposium on theoretical aspects of computer science , stacs , <digit> , pp. <digit> <digit> p. lu , c. yu , randomized truthful mechanisms for scheduling unrelated machines , in c.h. papadimitriou , s. zhang ( eds . ) , proceedings of wine , in lecture notes in computer science , vol . <digit> , springer , <digit> , pp. <digit> <digit> . lavi and swamy first use the cycle monotone condition and design a <digit> approximation truthful mechanism for a two value variant in r. lavi , c. swamy , truthful mechanism design for multi dimensional scheduling via cycle monotonicity , in ec '07 proceedings of the 8th acm conference on electronic commerce , acm , new york , ny , usa , <digit> , pp. <digit> <digit> , where the processing time of task j on machine i , say t ( ij ) , can only be either a lower value l ( j ) or a higher value h ( j ) . we consider a generalized variant . where t ( ij ) lies in l ( j ) , l ( j ) ( <digit> epsilon ) boolean or h ( j ) , h ( j ) ( <digit> epsilon ) and epsilon is a parameter satisfying some condition . we consider two special cases , case a when h ( j ) l ( j ) > <digit> , for all ( j ) and case b when h ( j ) l ( j ) < <digit> , for all j , and give randomized truthful mechanisms with approximation ratio <digit> ( <digit> epsilon ) for both cases . based on these two cases ' results , we are also able to deal with the general case of our two range values scheduling problem . we use a combination of two mechanisms , which is also a novel method in mechanism design for scheduling problems , and finally we give a randomized truthful mechanism with approximation ratio <digit> ( <digit> epsilon ) . although the generalization seems a little incremental , we actually use a very novel technique in the key step of proving truthfulness for case a , as well as a new mechanism scheme for case b. besides , the results in this paper are the first truthful mechanisms with constant approximation ratios when a machine ( player ) can report infinitely possible values , which is quite different from the two value variant , in which only finite values are available . furthermore , together with lavi and swamy 's work , our results suggest that such a task dependent approach can really do much better for the scheduling unrelated machines problem . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "truthful mechanism;scheduling;approximation algorithm"
  },
  {
    "source": "a note on the relationships among certified discrete log cryptosystems . <eos> the certified discrete logarithm problem modulo p prime is a discrete logarithm problem under the conditions that the complete factorization of p <digit> is given and by which the base g is certified to be a primitive root mod p. for the cryptosystems based on the intractability of certified discrete logarithm problem , sakurai shizuya showed that breaking the diffie hellman key exchange scheme reduces to breaking the shamir <digit> pass key transmission scheme with respect to the expected polynomial time turing reducibility . in this paper , we show that we can remove randomness from the reduction above , and replace the reducibility with the polynomial time many one . since the converse reduction is known to hold with respect to the polynomial time many one reducibility , our result gives a stronger evidence for that the two schemes are completely equivalent as certified discrete log cryptosystems .",
    "target": "certified discrete logarithm problem;primitive root;order;probabilistic reducibility;deterministic reducibility"
  },
  {
    "source": "hydraulic performance of a large slanted axial flow pump . <eos> purpose the pump of the taipuhe pump station , larger flow discharge , lower head , is one of the largest <digit> slanted axial flow pumps in the world . however , few studies have been done for the larger slanted axial flow pump on safe operation . the purpose of this paper is to analyze the impeller elevation , unsteady flow , hydraulic thrust and the zero head flow characteristics of the pump . design methodology approach the flow field in and through the pump was analyzed numerically during the initial stages of the pump design process , then the entire flow passage through the pump was analyzed to calculate the hydraulic thrust to prevent damage to the bearings and improve the operating stability the zero head pump flow characteristics were analyzed to ensure that the pump will work reliably at much lower heads . findings the calculated results are in good agreement with experimental data for the pump elevation effects , the performance curve , pressure oscillations , hydraulic thrust and zero head performance . research limitations implications since it is assumed that there is no gap between blades and shroud , gap cavitations are beyond the scope of the paper . originality value the paper indicates the slanted axial flow pump characteristics including the characteristic curves , pressure fluctuations , hydraulic thrust and radial force for normal operating conditions and zero head conditions . it shows how to guarantee the pump safety operating by computational fluid dynamics .",
    "target": "pumps;fluid dynamics;force measurement;water supply engineering;china"
  },
  {
    "source": "high radix montgomery modular exponentiation on reconfigurable hardware . <eos> it is widely recognized that security issues will play a crucial role in the majority of future computer and communication systems . central tools for achieving system security are cryptographic algorithms . this contribution proposes arithmetic architectures which are optimized for modern field programmable gate arrays ( fpgas ) . the proposed architectures perform modular exponentiation with very long integers . this operation is at the heart of many practical public key algorithms such as rsa and discrete logarithm schemes . we combine a high radix montgomery modular multiplication algorithm with a new systolic array design . the designs are flexible , allowing any choice of operand and modulus . the new architecture also allows the use of high radices . unlike previous approaches , we systematically implement and compare several variants of our new architecture for different bit lengths . we provide absolute area and timing measures for each architecture . the results allow conclusions about the feasibility and time space trade offs of our architecture for implementation on commercially available fpgas . we found that 1,024 bit rsa decryption can be done in 3.1 ms with our fastest architecture .",
    "target": "montgomery;exponentiation;fpga;rsa;systolic array;modular arithmetic"
  },
  {
    "source": "combined use of supervised and unsupervised learning for power system dynamic security mapping . <eos> this paper proposes a new methodology which combines supervised and unsupervised learning for evaluating power system dynamic security . based on the concept of stability margin , pre fault power system conditions are assigned to the output neurons on the two dimensional grid with the growing hierarchical self organizing map technique ( ghsom ) via supervised artificial neural networks ( anns ) which perform an estimation of post fault power system state . the technique estimates the dynamic stability index that corresponds to the most critical value of synchronizing and damping torques of multimachine power systems . ann based pattern recognition is carried out with the growing hierarchical self organizing feature mapping in order to provide adaptive neural network architecture during its unsupervised training process . numerical tests , carried out on a ieee <digit> bus power system are presented and discussed . the analysis using such method provides accurate results and improves the effectiveness of system security evaluation .",
    "target": "supervised and unsupervised learning;synchronizing and damping torques;growing hierarchical self organizing feature map;dynamic security assessment;stability criteria"
  },
  {
    "source": "0.35 mu m cmos t r switch for 2.4 ghz short range wireless applications . <eos> this paper describes the design and implementation of a transmit receive switch for 2.4 ghz ism band applications . the t r switch is implemented in a 0.35 mum bulk cmos process and it occupies <digit> mum . <digit> mum of die area . a parasitic mosfet model including bulk resistance is used to optimize the physical dimensions of the transistors with regard to insertion loss and isolation . the measured insertion loss is 1.3 db without port matching . simulations using measured s parameters indicate that an insertion loss of 0.8 db can be obtained with a conjugate match . the measured isolation is <digit> db and the maximum transmit power is <digit> dbm .",
    "target": "t r switch;mosfet switch;rf cmos;spdt switch"
  },
  {
    "source": "probabilistic equivalence checking of multiple valued functions . <eos> this paper describes a probabilistic method for verifying the equivalence of two multiple valued functions . each function is hashed to an integer code by transforming it to a integer valued polynomial and evaluating it for values of variables taken independently and uniformly at random from a finite field . since the polynomial is unique for a given function , if two hash codes are different , then the functions are not equivalent . however , if two hash codes are the same , the functions may or may not be equivalent , because different polynomials may happen to hash to the same code . thus , the method presented in this paper determines the equivalence of two functions with a known ( small ) probability of error , arising from collisions between inequivalent functions . such a method seems to be an attractive alternative for verifying functions that are too large to be handled by deterministic equivalence checking methods .",
    "target": "equivalence checking;multiple valued function;probabilistic verification"
  },
  {
    "source": "a model and environment for improving multimedia scholarly reading practices . <eos> the evolution of multimedia document production and diffusion technologies has lead to a significant spread of knowledge in form of pictures and recordings . however , scholarly reading tasks are still principally performed on textual contents . we argue that this is due to a lack of critical and structured tools ( <digit> ) to handle the wide spectrum of interpretive operations involved by the polymorphous scholarly reading process ( <digit> ) to perform these operations on a heterogeneous multimedia corpus . this firstly calls for identifying fundamental document requirements for such reading practices . then , we present a flexible model and a software environment which enable the reader to structure , annotate , link , fragment , compare , freely organise and spatially lay out documents , and to prepare the writing of their critical comment . we eventually discuss experiments with humanities scholars , and explore new academic reading practices which take advantage of document engineering principles such as multimedia document structuring , publication or sharing .",
    "target": "multimedia scholarly reading;multimedia corpus modelling;document annotation and structuring;spatial hypertexts;graphical user interfaces for critical reading"
  },
  {
    "source": "sensor selection for energy efficient ambulatory medical monitoring . <eos> epilepsy affects over three million americans of all ages . despite recent advances , more than <digit> % of individuals with epilepsy never achieve adequate control of their seizures . the use of a small , portable , non invasive seizure monitor could benefit these individuals tremendously . however , in order for such a device to be suitable for long term wear , it must be both comfortable and lightweight . typical state of the art non invasive seizure onset detection algorithms require <digit> scalp electrodes to be placed on the head . these electrodes are used to generate <digit> data streams , called channels . the large number of electrodes is inconvenient for the patient and processing <digit> channels can consume a considerable amount of energy , a problem for a battery powered device . in this paper , we describe an automated way to construct detectors that use fewer channels , and thus fewer electrodes . starting from an existing technique for constructing <digit> channel patient specific detectors , we use machine learning to automatically construct reduced channel detectors . we evaluate our algorithm on data from <digit> patients used in an earlier study . on average , our algorithm reduced the number of channels from <digit> to 4.6 while decreasing the mean fraction of seizure onsets detected from <digit> % to <digit> % . for <digit> out of the <digit> patients , there was no degradation in the detection rate . while the average detection latency increased from 7.8 s to 11.2 s , the average rate of false alarms per hour decreased from 0.35 to 0.19 . we also describe a prototype implementation of a single channel eeg monitoring device built using off the shelf components , and use this implementation to derive an energy consumption model . using fewer channels reduced the average energy consumption by <digit> % , which amounts to a 3.3 x increase in battery lifetime . finally , we show how additional energy savings can be realized by using a low power screening detector to rule out segments of data that are obviously not seizures . though this technique does not reduce the number of electrodes needed , it does reduce the energy consumption by an additional <digit> % .",
    "target": "ambulatory medical monitoring;epilepsy;reducing energy consumption;channel selection;electroencephalography"
  },
  {
    "source": "process synchronization without long term interlock . <eos> a technique is presented for replacing long term interlocking of shared data by the possible repetition of unprivileged code in case a version number ( associated with the shared data ) has been changed by another process . four principles of operating system architecture ( which have desirable effects on the intrinsic reliability of a system ) are presented implementation of a system adhering to these principles requires that long term lockout be avoided .",
    "target": "process;synchronization;sharing;data;code;case;version;association;operating system;systems;architecture;effect;reliability;implementation"
  },
  {
    "source": "application of an artificial immune system based fuzzy neural network to a rfid based positioning system . <eos> due to the rapid development of globalization , which makes supply chain management more complicated , more companies are applying radio frequency identification ( rfid ) , in warehouse management . the obvious advantages of rfid are its ability to scan at high speed , its penetration and memory . in addition to recycling , use of a rfid system can also reduce business costs , by indentifying the position of goods and picking carts . this study proposes an artificial immune system ( ais ) based fuzzy neural network ( fnn ) , to learn the relationship between the rfid signals and the picking cart 's position . since the proposed network has the merits of both ais and fnn . it is able to avoid falling into the local optimum and possesses a learning capability . the results of the evaluation of the model show that the proposed ais based fnn really can predict the picking cart position more precisely than conventional fnn and , unlike an artificial neural network , it is much easier to interpret the training results , since they are in the form of fuzzy if then rules . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "artificial immune system;fuzzy neural network;radio frequency identification;genetic algorithms"
  },
  {
    "source": "single dimension multidimensional software pipelining for loops . <eos> traditionally , software pipelining is applied either to the innermost loop of a given loop nest or from the innermost loop to outer loops . this paper proposes a three step approach , called single dimension software pipelining ( ssp ) , to software pipeline a loop nest at an arbitrary loop level that has a rectangular iteration space and contains no sibling inner loops in it . the first step identifies the most profitable loop level for software pipelining in terms of initiation rate , data reuse potential , or any other optimization criteria . the second step simplifies the multidimensional data dependence graph ( ddg ) of the selected loop level into a one dimensional ddg and constructs a one dimensional ( 1d ) schedule . based on the one dimensional schedule , the third step derives a simple mapping function that specifies the schedule time for the operation instances in the multidimensional loop . the classical modulo scheduling is subsumed by ssp as a special case . ssp is also closely related to hyperplane scheduling , and , in fact , extends it to be resource constrained . we prove that ssp schedules are correct and at least as efficient as those schedules generated by traditional modulo scheduling methods . we extend ssp to schedule imperfect loop nests , which are most common at the instruction level . multiple initiation intervals are naturally allowed to improve execution efficiency . feasibility and correctness of our approach are verified by a prototype implementation in the orc compiler for the ia <digit> architecture , tested with loop nests from livermore and spec2000 floating point benchmarks . preliminary experimental results reveal that , compared to modulo scheduling , software pipelining at an appropriate loop level results in significant performance improvement . software pipelining is beneficial even with prior loop transformations .",
    "target": "software pipelining;modulo scheduling;loop transformation;algorithms;languages"
  },
  {
    "source": "a geometric based method for recognizing overlapping polygonal shaped and semi transparent particles in gray tone images . <eos> a geometric based method is proposed to recognize the overlapping particles of different polygonal shapes such as rectangular , regular and or irregular prismatic particles in a gray tone image . the first step consists in extracting the salient corners , identified by their locations and orientations , of the overlapping particles . although there are certain difficulties like the perspective geometric projection , out of focus , transparency and superposition of the studied particles . then , a new clustering technique is applied to detect the shape by grouping its correspondent salient corners according to the geometric properties of each shape . a simulation process is carried out for evaluating the performance of the proposed method . then , it is particularly applied on a real application of batch cooling crystallization of the ammonium oxalate in pure water . the experimental results show that the method is efficient to recognize the overlapping particles of different shapes and sizes .",
    "target": "salient corner detection;contour detection;clustering method;overlapping particles recognition"
  },
  {
    "source": "explicit dimension reduction and its applications . <eos> we construct a small set of explicit linear transformations mapping r n to r t , where t o ( log ( gamma ( <digit> ) ) epsilon ( <digit> ) ) , such that the l <digit> norm of any vector in r n is distorted by at most <digit> epsilon in at least a fraction of <digit> gamma of the transformations in the set . albeit the tradeoff between the size of the set and the success probability is suboptimal compared with probabilistic arguments , we nevertheless are able to apply our construction to a number of problems . in particular , we use it to construct an epsilon sample ( or pseudorandom generator ) for linear threshold functions on sn <digit> for epsilon o ( <digit> ) . we also use it to construct an epsilon sample for spherical digons in sn <digit> for epsilon o ( <digit> ) . this construction leads to an efficient oblivious derandomization of the goemans williamson max cut algorithm and similar approximation algorithms ( i.e. , we construct a small set of hyperplanes such that for any instance we can choose one of them to generate a good solution ) . our technique for constructing an epsilon sample for linear threshold functions on the sphere is considerably different than previous techniques that rely on k wise independent sample spaces .",
    "target": "dimension reduction;pseudorandom generator;linear threshold functions;digons;max cut;johnson lindenstrauss"
  },
  {
    "source": "capital one financial and a decade of experience with newly vulnerable markets some propositions concerning the competitive advantage of new entrants . <eos> market share and brand recognition have historically provided advantage to established players in mature industries . the success of capital one , an attacker in the mature credit card industry is therefore interesting , both to researchers and to executives developing strategies . a partial explanation is offered by the theory of newly vulnerable markets . the success of capital one can be partially attributed to its application of information based strategies to several newly vulnerable markets , allowing it to target and retain the most profitable customers . these strategies sustained double digit return on equity and double digit increase in sales volume and profits every year of our study .",
    "target": "capital one financial;newly vulnerable markets;information based strategy;market entry;differential pricing;customer profitability gradient;information economics"
  },
  {
    "source": "scene analysis and geometric homology . <eos> during the last <digit> <digit> years there has been a dramatic revival of interest in applied geometric problems . geometers have reconsidered a number of questions in infinitesimal mechanics , questions treated by j.c. maxwell and l. cremona <digit> , <digit> , <digit> in <digit> <digit> , further developed under the banner of graphical statics <digit> , <digit> , but left largely untouched since the end of the nineteenth century . at the same time , computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis . a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal . the work of this group , reported in the pages of the journal structural topology <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> ( and elsewhere ) , was a biproduct of research on infinitesimal mechanics , using methods derived from graphical statics , as well as from exterior algebra and its modern offspring , the doubilet rota stein double algebra <digit> , <digit> . independently , huffman <digit> , duda and hart <digit> , and others recognized that maxwell 's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene . more recently , sugihara <digit> and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry . they wrote a software package capable of modifying a rough plane sketch , so as to make it a true projection of a 3d scene . the starting point of the during the last <digit> <digit> years there has been a dramatic revival of interest in applied geometric problems . geometers have reconsidered a number of questions in infinitesimal mechanics , questions treated by j.c. maxwell and l. cremona <digit> , <digit> , <digit> in <digit> <digit> , further developed under the banner of graphical statics <digit> , <digit> , but left largely untouched since the end of the nineteenth century . at the same time , computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis . a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal . the work of this group , reported in the pages of the journal structural topology <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> ( and elsewhere ) , was a biproduct of research on infinitesimal mechanics , using methods derived from graphical statics , as well as from exterior a <digit> , <digit> . independently , huffman <digit> , duda and hart <digit> , and others recognized that maxwell 's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene . more recently , sugihara <digit> and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry . they wrote a software package capable of modifying a rough plane sketch , so as to make it a true projection of a 3d scene . the starting point of the projective geometric analysis of scenes is the observation that the set of all three dimensional realizations ( scenes ) having a given two dimensional projection ( a drawing , or image ) form a linear space . much information about an image , and about its possible spatial interpretations , can be obtained simply by calculating ( either locally or globally ) the linear dimension ( or rank ) of its linear space of scenes . in practice , the image is a pattern on a cathode ray tube , an aerial photograph , an engineer 's or architect 's drawing , or an x ray or nmr scan . the rank of its space of scenes will reveal whether there is ambiguity or uniqueness in the construction of its spatial interpretation , or whether such a construction is in fact impossible , as would be the case for a poorly conceived engineering drawing , or even in an otherwise correctly conceived drawing , if too many hypotheses are made concerning the 3d structure of the scene . calculation of the rank of the space of scenes having a given image should , in principle , be accomplished using simple combinatorial algorithms based on easily remembered rules of thumb . this is the goal , and it shows every sign of being achievable . the problem has , however , a certain degree of unavoidable difficulty . the requirement that a given image be an accurate projection of a non trivial ( non planar ) 3d scene imposes conditions on the image , conditions which are perhaps best described in terms of not always elementary constructions with straight edge and pencil . in this paper , we begin to sort out the interplay of these projective conditions by creating a new homology theory for geometric configurations . the new homology theory applies to geometric objects which are more rigid , less pliable , than the rubber sheets studied by the topology of henri poincar and his school . the passage to this higher degree of invariance is made possible by the creation of a homology theory with ( restricted ) vector , rather than ( unrestricted ) scalar , coefficients , or equivalently , by the use of a cohomology theory based on locally linear , rather than on locally constant , functions . we have verified that the new theory agrees with the cohomology theory for the sheaf of locally linear functions on a certain ( combinatorially defined ) topological space . the basic objects about which this new homology theory has something non trivial to say are extremely general . from the geometric point of view , they are simply finite sets of points in a projective space or finite sets of vectors in a vector space . in order to emphasize the departure we take from linear algebra as it is usually practiced , we should say that we study vector spaces with a selected basis , that is , concrete vector spaces , in their usual representation as spaces f p of functions from a set p into a field f. finally , we might say we are simply studying rectangular matrices . since such objects are found throughout applied mathematics , the resulting homology theory has a very broad range of potential application . indeed , potential applications of this new homology theory are to any domain where one is interested in the global behavior of systems determined locally by linear constraints .",
    "target": "analysis;graphics;timing;computation;tools;project;geometry;research;structure;topologies;group;use;method;algebra;help;image;3d;automation;software;packaging;sketching;point;observability;drawing;space;informal;spatial;interpretation;global;ranking;practical;pattern;engine;scan;ambiguities;case;algorithm;rules;requirements;paper;sorting;theory;configurability;object;invariance;vectorization;general;order;linear algebra;representation;mathematics;applications;behavior;systems;constraint"
  },
  {
    "source": "imagesense towards contextual image advertising . <eos> the daunting volumes of community contributed media contents on the internet have become one of the primary sources for online advertising . however , conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the web page , without considering the inherent characteristics of visual contents . this article presents a contextual advertising system driven by images , which automatically associates relevant ads with an image rather than the entire text in a web page and seamlessly inserts the ads in the nonintrusive areas within each individual image . the proposed system , called imagesense , supports scalable advertising of , from root to node , web sites , pages , and images . in imagesense , the ads are selected based on not only textual relevance but also visual similarity , so that the ads yield contextual relevance to both the text in the web page and the image content . the ad insertion positions are detected based on image salience , as well as face and text detection , to minimize intrusiveness to the user . we evaluate imagesense on a large scale real world images and web pages , and demonstrate the effectiveness of imagesense for online image advertising .",
    "target": "algorithms;experimentation;human factors"
  },
  {
    "source": "construction and blind estimation of phase sequences for subcarrier phase control based papr reduction in ldpc coded ofdm systems . <eos> as described in this paper construction and blind estimation methods of phase sequences are proposed for subcarrier phase control based peak to average power ratio ( papr ) reduction in low density parity check ( ldpc ) coded orthogonal frequency division multiplexing ( ofdm ) systems . on the transmitter side phase sequence patterns are constructed based on a given parity check matrix . the papr of the ofdm signal is reduced by multiplying the constructed phase sequence selected from the same number of candidates as the number of weighting factor ( wf ) combinations in a partial transmit sequence ( pts ) method . on the receiver side the phase sequence is estimated blindly using the decoding function i e the most likely phase sequence among a limited number of possible phase sequence candidates is inferred by comparing the sum product calculation results of each candidate . computer simulation results show that papr of qpsk ofdm and 16qam ofdm signals can be reduced respectively by about <digit> <digit> db and <digit> <digit> db without marked degradation of the block error rate ( bler ) performance as compared to perfect estimation in an attenuated <digit> path rayleigh fading condition .",
    "target": "papr reduction;ldpc code;ofdm;peak to average power ratio"
  },
  {
    "source": "mathsat tight integration of sat and mathematical decision procedures . <eos> recent improvements in propositional satisfiability techniques ( sat ) made it possible to tackle successfully some hard real world problems ( e.g. , model checking , circuit testing , propositional planning ) by encoding into sat . however , a purely boolean representation is not expressive enough for many other real world applications , including the verification of timed and hybrid systems , of proof obligations in software , and of circuit design at rtl level . these problems can be naturally modeled as satisfiability in linear arithmetic logic ( lal ) , that is , the boolean combination of propositional variables and linear constraints over numerical variables . in this paper we present mathsat , a new , sat based decision procedure for lal , based on the ( known approach ) of integrating a state of the art sat solver with a dedicated mathematical solver for lal . we improve mathsat in two different directions . first , the top level line procedure is enhanced and now features a tighter integration between the boolean search and the mathematical solver . in particular , we allow for theory driven backjumping and learning , and theory driven deduction we use static learning in order to reduce the number of boolean models that are mathematically inconsistent we exploit problem clustering in order to partition mathematical reasoning and we define a stack based interface that allows us to implement mathematical reasoning in an incremental and backtrackable way . second , the mathematical solver is based on layering that is , the consistency of ( partial ) assignments is checked in theories of increasing strength ( equality and uninterpreted functions , linear arithmetic over the reals , linear arithmetic over the integers ) . for each of these layers , a dedicated ( sub ) solver is used . cheaper solvers are called first , and detection of inconsistency makes call of the subsequent solvers superfluous . we provide a through experimental evaluation of our approach , by taking into account a large set of previously proposed benchmarks . we first investigate the relative benefits and drawbacks of each proposed technique by comparison with respect to a reference option setting . we then demonstrate the global effectiveness of our approach by a comparison with several state of the art decision procedures . we show that the behavior of mathsat is often superior to its competitors , both on lal and in the subclass of difference logic .",
    "target": "propositional satisfiability;linear arithmetic logic;satisfiability module theory;integrated decision procedures"
  },
  {
    "source": "strongly regular graphs with the ( <digit> ) vertex condition . <eos> the ( t ) vertex condition , for an integer ( t ge <digit> ) , was introduced by hestenes and higman ( siam am math soc proc <digit> <digit> , <digit> ) providing a combinatorial invariant defined on edges and non edges of a graph . finite rank <digit> graphs satisfy the condition for all values of ( t ) . moreover , a long standing conjecture of klin asserts the existence of an integer ( t_0 ) such that a graph satisfies the ( t_0 ) vertex condition if and only if it is a rank <digit> graph . we present the first infinite family of non rank <digit> strongly regular graphs satisfying the ( <digit> ) vertex condition . this implies that the klin parameter ( t_0 ) is at least <digit> . the examples are the point graphs of a certain family of generalized quadrangles .",
    "target": "strongly regular graph;generalized quadrangle;t vertex condition"
  },
  {
    "source": "( ( epsilon ) ) efficiency in difference vector optimization . <eos> the paper deals with the problem of characterizing pareto optima ( efficient solutions ) for the difference of two mappings vector valued in a finite or infinite dimensional preordered space . closely related to the well known optimality criterion of scalar dc optimization , a mixed vectorial condition is obtained in terms of both strong ( fenchel ) and weak ( pareto ) ( epsilon ) subdifferentials that completely characterizes the exact or approximate weak efficiency . this condition also allows to deal with some special restricted mappings . moreover , the condition established in the literature in terms of strong ( epsilon ) subdifferentials for characterizing the strongly efficient solutions ( usual optima ) , is shown here to remain valid without assuming that the objective space is order complete .",
    "target": "efficiency;vector optimization;solutions;dc objective;optimality criteria;vector subdifferentials"
  },
  {
    "source": "iterative visual clustering for unstructured text mining . <eos> this paper proposes the iterative visual clustering ( ivc ) on unstructured text sequences to form and evaluate keyword clusters , based on which users can use visual analysis , domain knowledge to discover knowledge in the text . the text sequence data are broken down into a list representative keywords after textual evaluation , and the keywords are then grouped to form keyword clusters via an iterative stochastic process and are visualized as distributions over the time lines . the visual evaluation model provides shape evaluations as quantitative tools and users ' interactions as qualitative tools to visually investigate the trends , patterns represented by the keyword clusters ' distributions . the keyword clustering model , guided by the feedback of visual evaluations , step wisely enumerates newer generations of keyword clusters and their patterns , therefore narrows down the search space . then the proposed ivc is applied onto nursing narratives and is able to identify interesting keyword clusters implying hidden knowledge regarding to the working patterns and environment of registered nurses . the loop of producing next generation of keyword clusters in ivc is driven and controlled by users ' perception , domain knowledge and interactions , and it is also guided by a stochastic search model . so both semantic and distribution features enable ivc to have significant applications as a text mining tool , on many other data sets , such as biomedical literatures .",
    "target": "text and document visualization;nursing data processing"
  },
  {
    "source": "economic growth , telecommunications development and productivity growth of the telecommunications sector evidence around the world . <eos> this paper studies the relationships between economic growth , telecommunications development and productivity growth of the telecommunications sector in different countries and regions of the world . in particular , this study assesses the impact of mobile telecommunications on economic growth and telecommunications productivity . the results indicate that there is a bidirectional relationship between real gross domestic product ( gdp ) and telecommunications development ( as measured by teledensity ) for european and high income countries . however , when the impact of mobile telecommunications development on economic growth is measured separately , the bi directional relationship is no longer restricted to european and high income countries . this study also finds that countries in the upper middle income group have achieved a higher average total factor productivity ( tfp ) growth than other countries . countries with competition and privatization in telecommunications have achieved a higher tfp growth than those without competition and privatization . the diffusion of mobile telecommunications services is found to be a significant factor that has improved the tfp growth of the telecommunications sector in central and eastern europe ( cee ) .",
    "target": "economic growth;telecommunications;total factor productivity"
  },
  {
    "source": "examining learning from text and pictures for different task types does the multimedia effect differ for conceptual , causal , and procedural tasks . <eos> the multimedia effect ( me ) is a well researched effect in the field of learning and instruction . in this article , two views that explain the me are compared . the outcome oriented view focuses on the beneficial effect of text and pictures on mental representations , whereas the process oriented view focuses on the beneficial effect of text and pictures for information processing . to contrast these views , the me sizes for different task types were compared ( i.e. , conceptual , causal , procedural tasks ) . whereas the outcome oriented view predicts no differences in me size , the process oriented view predicts that the me is largest in causal tasks , smaller in procedural tasks , and smallest in conceptual tasks . sixty five students learnt with text only or with text and pictures . task type and information source ( i.e. , whether the text , picture , or text and picture provided the answer to a post test question ) were varied within subjects . the results showed that , in line with the process oriented view , the me was smaller for conceptual tasks than for procedural tasks . contrary to the expectations , the me was larger in procedural tasks than in causal tasks . moreover , the pattern of results varied with information source . research and practical implications are described , so that pictures can be deployed optimally .",
    "target": "multimedia effect;conceptual;learning with text and pictures;causal and procedural tasks;static visualisations"
  },
  {
    "source": "sufficient conditions for lambda ' optimality of graphs with small conditional diameter . <eos> a restricted edge cut s of a connected graph g is an edge cut such that g s has no isolated vertex . the restricted edge connectivity lambda ' ( g ) is the minimum cardinality over all restricted edge cuts . a graph is said to lambda ' optimal if lambda ' ( g ) xi ( g ) , where xi ( g ) denotes the minimum edge degree of g defined as xi ( g ) min d ( u ) d ( nu ) <digit> u nu is an element of e ( g ) . the p diameter of g measures how far apart a pair of subgraphs satisfying a given property p can be , and hence it generalizes the standard concept of diameter . in this paper we prove two kind of results , according to which property p is chosen . first , let d <digit> ( resp . d <digit> ) be the p diameter where p is the property that the corresponding subgraphs have minimum degree at least one ( resp . two ) . we prove that a graph with odd girth g is lambda ' optimal if d <digit> <digit> , being the minimum degree of g. using the property q of being vertices of g f we prove that a graph with girth g is not an element of <digit> , <digit> , <digit> is lambda ' optimal if this q diameter is at most <digit> ( g <digit> ) <digit> . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "conditional diameter;restricted edge connectivity;fault tolerance"
  },
  {
    "source": "comparative analysis of clicks and judgments for ir evaluation . <eos> queries and click through data taken from search engine transaction logs is an attractive alternative to traditional test collections , due to its volume and the direct relation to end user querying . the overall aim of this paper is to answer the question how does click through data differ from explicit human relevance judgments in information retrieval evaluation we compare a traditional test collection with manual judgments to transaction log based test collections by using queries as topics and subsequent clicks as pseudo relevance judgments for the clicked results . specifically , we investigate the following two research questions firstly , are there significant differences between clicks and relevance judgments . earlier research suggests that although clicks and explicit judgments show reasonable agreement , clicks are different from static absolute relevance judgments . secondly , are there significant differences between system ranking based on clicks and based on relevance judgments this is an open question , but earlier research suggests that comparative evaluation in terms of system ranking is remarkably robust .",
    "target": "transaction log analysis;wikipedia;web information retrieval"
  },
  {
    "source": "splitting the difference . <eos> so , nat ' ralists observe , a flea hath smaller fleas that on him prey and these have smaller still to bite 'em and so proceed ad infinitum . jonathan swift , on poetry a rhapsody , <digit>",
    "target": "reward;anticipation;fmri;human;computation;accumbens;prefrontal"
  },
  {
    "source": "s2 s <digit> quasicontinuous posets . <eos> in this paper , we consider a common generalization of both s2 s <digit> continuous posets and quasicontinuous domains , and we introduce new concepts of way below relations and s2 s <digit> quasicontinuous posets . the main results are ( <digit> ) the way below relation on an s2 s <digit> quasicontinuous poset has the interpolation property ( <digit> ) the <digit> <digit> topology on an s2 s <digit> quasicontinuous poset is completely regular ( <digit> ) a poset is s2 s <digit> continuous iff it is meet s2 s <digit> continuous and s2 s <digit> quasicontinuous .",
    "target": "s2 s <digit> continuous poset s2 s <digit> s2 s <digit> s <digit>;meet s2 s <digit> continuous poset s2 s <digit> s2 s <digit> s <digit>;s2 s <digit> quasicontinuous poset s2 s <digit> s2 s <digit> s <digit>;<digit> <digit> topology <digit> <digit> <digit> <digit> <digit>"
  },
  {
    "source": "does the polynomial hierarchy collapse if onto functions are invertible . <eos> the class tfnp , defined by megiddo and papadimitriou , consists of multivalued functions with values that are polynomially verifiable and guaranteed to exist . do we have evidence that such functions are hard , for example , if tfnp is computable in polynomial time does this imply the polynomial time hierarchy collapses by computing a multivalued function in deterministic polynomial time we mean on every input producing one of the possible values of the function on that input . we give a relativized negative answer to this question by exhibiting an oracle under which tfnp functions are easy to compute but the polynomial time hierarchy is infinite . we also show that relative to this same oracle , p not equal up and tfnp ( np ) functions are not computable in polynomial time with an np oracle .",
    "target": "polynomial time hierarchy;computational complexity;multi valued functions;kolmogorov complexity"
  },
  {
    "source": "unsupervised object segmentation with a hybrid graph model ( hgm ) . <eos> in this work , we address the problem of performing class specific unsupervised object segmentation , i.e. , automatic segmentation without annotated training images . object segmentation can be regarded as a special data clustering problem where both class specific information and local texture color similarities have to be considered . to this end , we propose a hybrid graph model ( hgm ) that can make effective use of both symmetric and asymmetric relationship among samples . the vertices of a hybrid graph represent the samples and are connected by directed edges and or undirected ones , which represent the asymmetric and or symmetric relationship between them , respectively . when applied to object segmentation , vertices are superpixels , the asymmetric relationship is the conditional dependence of occurrence , and the symmetric relationship is the color texture similarity . by combining the markov chain formed by the directed subgraph and the minimal cut of the undirected subgraph , the object boundaries can be determined for each image . using the hgm , we can conveniently achieve simultaneous segmentation and recognition by integrating both top down and bottom up information into a unified process . experiments on <digit> object classes ( 9,415 images in total ) show promising results .",
    "target": "segmentation;graph theoretic methods;spectral clustering"
  },
  {
    "source": "dispersion free wave splittings for structural elements . <eos> wave splittings are derived for three types of structural elements membranes , timoshenko beams , and mindlin plates . the timoshenko beam equation and the mindlin plate equation are inherently dispersive , as is each fourier component of the membrane equation in an angular decomposition of the field . the distinctive feature of the wave splittings derived in the present paper is that , in homogeneous regions , they transform the dispersive wave equations into simple one way wave equations without dispersion . such splittings have uses both for radial scattering problems in the 2d cases and for scattering problems in dispersive media . as an example of how the splittings may be applied , a direct scattering problem is solved for a membrane with radially varying density . the imbedding method is utilized , and agreement is obtained with an fe simulation .",
    "target": "wave splitting;membrane;timoshenko beam;mindlin plate;imbedding;time domain methods;greens operator"
  },
  {
    "source": "designing a practical data filter cache to improve both energy efficiency and performance . <eos> conventional data filter cache ( dfc ) designs improve processor energy efficiency , but degrade performance . furthermore , the single cycle line transfer suggested in prior studies adversely affects level <digit> data cache ( l1 dc ) area and energy efficiency . we propose a practical dfc that is accessed early in the pipeline and transfers a line over multiple cycles . our dfc design improves performance and eliminates a substantial fraction of l1 dc accesses for loads , l1 dc tag checks on stores , and data translation lookaside buffer accesses for both loads and stores . our evaluation shows that the proposed dfc can reduce the data access energy by 42.5 % and improve execution time by 4.2 % .",
    "target": "filter cache;speculation"
  },
  {
    "source": "analytical model for anomalous positive bias temperature instability in la based hfo2 nfets based on independent characterization of charging components . <eos> pbti improvement in hfo2 nfets achieved by a controlled insertion of la . anomalous negative vth due to charge exchange between high k and metal gate . anomalous and conventional pbti components are decoupled and studied separately . analytical model including both components for lifetime extrapolation is presented .",
    "target": "bias temperature instability;metaloxidesemiconductor field effect transistor;hafnium oxide;silicon oxide"
  },
  {
    "source": "a social behaviour evolution approach for evolutionary optimisation . <eos> evolutionary algorithms were originally designed to locate basins of optimum solutions in a stationary environment . therefore , additional techniques and modifications have been introduced to deal with further requirements such as handling dynamic fitness functions or finding multiple optima . in this paper , we present a new approach for building evolutionary algorithms that is based on concepts borrowed from social behaviour evolution . algorithms built with the proposed paradigm operate on a population of individuals that move in the search space as they interact and form groups . the interaction follows a set of social behaviours evolved by each group to enhance its adaptation to the environment ( and other groups ) and to achieve different desirable goals such as finding multiple optima , maintaining diversity , or tracking a moving peak in a changing environment . each group has two sets of behaviours one for intra group interactions and one for inter group interactions . these behaviours are evolved using mathematical models from the field of evolutionary game theory . this paper describes the proposed paradigm and starts studying it characteristics by building a new evolutionary algorithm and studying its behavior . the algorithm has been tested using a benchmark problem generator with promising initial results , which are also reported .",
    "target": "social behaviour evolution;evolutionary optimisation;evolutionary algorithms;evolutionary game theory;social adaptive groups;dynamic optimisation problems"
  },
  {
    "source": "planar c1 c <digit> hermite interpolation with ph cuts of degree ( 1,3 ) ( <digit> , <digit> ) of laurent series . <eos> we introduce a new class of ph curves , ph cuts of degree ( 1,3 ) ( <digit> , <digit> ) of laurent series . we show how to find ph skew cut interpolants to a c1 c <digit> hermite data set . we show that two of these interpolants are short , simple curves with stable shape . our curves are fair with different shapes to those of other interpolants . we can obtain regular ph interpolants for collinear c1 c <digit> hermite data sets .",
    "target": "<digit> );ph skew cut;ph skew cut interpolant;pythagorean hodograph curve;c1 c <digit> hermite interpolation c1 c <digit> c1 c <digit> c <digit>;complex representation;cut of degree ( <digit>;<digit> ) ( <digit>;<digit> ) of a laurent series ( <digit>;<digit> ) ( <digit>;<digit> ) ( <digit>;<digit> ) ( <digit>;<digit> ) ( <digit>"
  },
  {
    "source": "bicepstrum based blind identification of the acoustic emission ( ae ) signal in precision turning . <eos> it is believed that the acoustic emissions ( ae ) signal contains potentially valuable information for monitoring precision cutting processes , as well as to be employed as a control feedback signal . however , ae stress waves produced in the cutting zone are distorted by the transmission path and the measurement systems . in this article , a bicepstrum based blind system identification technique is proposed as a valid tool for estimating both , transmission path and sensor impulse response . assumptions under which application of bicepstrum is valid are discussed and diamond turning experiments are presented , which demonstrate the feasibility of employing bicepstrum for ae blind identification .",
    "target": "blind identification;acoustic emissions;higher order statistics;precision machining"
  },
  {
    "source": "on solving hierarchical problems with top down control . <eos> we review recent work on the hierarchical if and only if problem and present a new hierarchical problem , hiff m that does not fit with previous explanations for evolutionary difficulty on hierarchical problems decomposed by levels for rmhc2 . rmhc2 is a hill climbing algorithm augmented with a multi level selection scheme . when used with the ideal sieve for a problem , as is done in this paper , rmhc2 exerts top down control on the evolutionary dynamics , in the sense that adaptation of higher levels are given priority over adaptation of lower levels , and creates stabilizing selection pressure with potential to increase evolvability . through hiff m , we discovered that the summary statistic , fitness distance correlation by level , is not a reliable indicator of when a hierarchical problem is solvable by rmhc2 , and that the two properties proposed to explain search easiness for rmhc2 are inadequate . our investigation of this anomaly led us to propose an additional property for hierarchical evolution difficulty under rmhc2 inter level conflict . we also discuss how hierarchical control can be subverted through the information transfer capacity of the transposition operation .",
    "target": "hierarchical control;transposition;hierarchical test problems;level decomposition"
  },
  {
    "source": "extracting semantic frames from thai medical symptom unstructured text with unknown target phrase boundaries . <eos> due to the limitations of language processing tools for the thai language , pattern based information extraction from thai documents requires supplementary techniques . based on sliding window rule application and extraction filtering , we present a framework for extracting semantic information from medical symptom phrases with unknown boundaries in thai unstructured text information entries . a supervised rule learning algorithm is employed for automatic construction of information extraction rules from hand tagged training symptom phrases . two filtering components are introduced one uses a classification model to predict rule application across a symptom phrase boundary based on instantiation features of rule internal wildcards , the other uses weighted classification confidence to resolve conflicts arising from overlapping extractions . in our experimental study , we focus our attention on two basic types of symptom phrasal descriptions one is concerned with abnormal characteristics of some observable entities and the other with human body locations at which primitive symptoms appear . the experimental results show that the filtering components improve precision while preserving recall satisfactorily .",
    "target": "information extraction;rule learning;medical informatics"
  },
  {
    "source": "robust doa estimation for uncorrelated and coherent signals . <eos> a new direction of arrival ( doa ) estimation method is introduced with arbitrary array geometry when uncorrelated and coherent signals coexist . the doas of uncorrelated signals are first estimated via subspace based high resolution doa estimation technique . then a matrix that only contains the information of coherent signals can be formulated by eliminating the contribution of uncorrelated signals . finally a subspace block sparse reconstruction approach is taken for doa estimations of the coherent signals .",
    "target": "coherent signals;direction of arrival;sparse reconstruction"
  },
  {
    "source": "checkpoint allocation and release . <eos> out of order speculative processors need a bookkeeping method to recover from incorrect speculation . in recent years , several microarchitectures that employ checkpoints have been proposed , either extending the reorder buffer or entirely replacing it . this work presents an in dept study of checkpointing in checkpoint based microarchitectures , from the desired content of a checkpoint , via implementation trade offs , and to checkpoint allocation and release policies . a major contribution of the article is a novel adaptive checkpoint allocation policy that outperforms known policies . the adaptive policy controls checkpoint allocation according to dynamic events , such as second level cache misses and rollback history . it achieves 6.8 % and 2.2 % speedup for the integer and floating point benchmarks , respectively , and does not require a branch confidence estimator . the results show that the proposed adaptive policy achieves most of the potential of an oracle policy whose performance improvement is 9.8 % and 3.9 % for the integer and floating point benchmarks , respectively . we exploit known techniques for saving leakage power by adapting and applying them to checkpoint based microarchitectures . the proposed applications combine to reduce the leakage power of the register file to about one half of its original value .",
    "target": "checkpoint;rollback;performance;leakage;design;misprediction;out of order execution;early register release"
  },
  {
    "source": "a quadratic spline approximation using detail multi layer for soft shadow generation in augmented reality . <eos> implementation of shadows is crucial to enhancement of images in ar environments . without shadows , virtual objects would look floating over the scene resulting in unrealistic rendering of ar environments . casting hard shadows would provide only spatial information while soft shadows help improve realism of ar environments . several algorithms have been proposed to render realistic shadows which often incurred high computational costs . little attention has been directed towards the balanced trade off between shadow quality and computational costs . in this study , two approaches are proposed quadratic spline interpolation ( qsi ) to soften the outline of the shadow and detail multi layer ( dml ) technique to optimize the volume of computations for the generation of soft shadows based on real light sources . qsi estimates boarder hard shadow samples while dml involves three main phases real light sources estimation , soft shadow production and reduction of the complexity of <digit> dimensional objects shadows . to be more precise , a reflective hemisphere is used to capture real light and to create an environment map . the median cut algorithm is implemented to locate the direction of real light sources on the environment map . subsequently , the original hard shadows are retrieved and a sample of multilayer hard shadows is produced where each layer has its unique size and colour . these layers overlap to produce soft shadows based on the real light sources directions . finally , the level of details ( lod ) algorithm is implemented to increase the efficiency of soft shadows by decreasing the complexity of vertex transformations . the proposed technique is tested using three samples of multilayer hard shadows with varying numbers of light sources generated from the median cut algorithm . the experimental results show that the proposed technique successfully produces realistic soft shadows at low computational costs .",
    "target": "soft shadows;shadow generation;augmented reality;environment map;reflective sphere"
  },
  {
    "source": "the enhanced optical coupling in a quantum well infrared photodetector based on a resonant mode of an airdielectricmetal waveguide . <eos> the hybrid structure consisting of periodic gold stripes and an overlaying gold film is proposed to enhance the optical coupling of a quantum well infrared photodetector . an airdielectricmetal waveguide is formed when the hybrid structure is integrated on the top of the quantum well detector with the substrate being removed . finite difference time domain method is used to numerically obtain the reflection spectrum and the field distribution of the waveguide . the results show that a strong electric field component is induced in parallel to the growth direction of quantum well when the waveguide resonant mode occurs at the detective wavelength of the quantum well infrared photodetector . the relationship between the structural parameters and the resonant wavelength is derived by using the effective refractive index method of the airdielectricmetal waveguide . a high coupling efficiency can be obtained and the performance of the qwip can be greatly improved .",
    "target": "quantum well infrared photodetector;periodic gold stripes;effective refractive index;coupling efficiency;airdielectricmetal waveguide resonance"
  },
  {
    "source": "redirection based recovery for mpls network systems . <eos> to provide a reliable backbone network , fault tolerance should be considered in the network design . for a multiprotocol label switching ( mpls ) based backbone network , the fault tolerant issue focuses on how to protect the traffic of a label switched paths ( lsp ) against node and link failures . in ietf , two well known recovery mechanisms ( protection switching and rerouting ) have been proposed . to further enhance the fault tolerant performance of the two recovery mechanisms , the proposed approach utilizes the failure free lsps to transmit the traffic of the failed lsp ( the affected traffic ) . to avoid affecting the original traffic of each failure free lsp , the proposed approach applies the solution of the minimum cost flow to determine the amount of affected traffic to be transmitted by each failure free lsp . for transmitting the affected traffic along a failure free working lsp , ip tunneling technique is used . we also propose a permission token scheme to solve the packet disorder problem . finally , simulation experiments are performed to show the effectiveness of the proposed approach .",
    "target": "mpls;fault tolerance;label switched path;affected traffic;minimum cost flow"
  },
  {
    "source": "designing a cross language comparison shopping agent . <eos> this research pertains to the design and development of a shopbot called webshopper . this shopbot is intended to help customers find and compare e tailers that market their wares using different languages . webshopper is built with a multilingual ontology to overcome the language barriers that arise with global e commerce . this research proposes a semi automatic method of constructing a multilingual ontology by using the formal concept analysis and association analysis . it also proposes an automatic method for the categorization of product data into predefined classes , with the aim of alleviating administrators ' task load . additionally , a semantic search mechanism based on concept similarity is designed to assist customers in finding more desirable products . the experimental results show that these methods perform well and the shopbot can help customers find real bargains on the web and to find products that can not be bought locally .",
    "target": "comparison shopping;shopbot;ontology;formal concept analysis;semantic similarity"
  },
  {
    "source": "medical informaticsthe state of the art in the hospital authority . <eos> since its inception in <digit> , the hospital authority ( ha ) has strongly supported the development and implementation of information systems both to improve the delivery of care and to make better information available to managers . this paper summarizes the progress to date and discusses current and future developments . following the first two phases of the ha information technology strategy the basic infrastructural elements were laid in place . these included the foundation administrative and financial systems and databases establishment of a wide area network linking all hospitals and clinics together laboratory , radiology and pharmacy systems with access to results in the ward . a major push into clinical systems began in <digit> with the clinical management system ( cms ) , which established a clinical workstation for use in both ward and ambulatory settings . the cms is now running at all major hospitals , and provides single logon access to almost all the electronically collected clinical data in the ha . the next phase of development is focussed on further support for clinical activities in the cms . key elements include the longitudinal electronic patient record ( epr ) , clinical order entry , generic support for clinical reports , broadening the scope to include allied health and the rehabilitative phase , clinical decision support , an improved clinical documentation framework , sharing of clinical information with other health care providers and a comprehensive data repository for analysis and reporting purposes .",
    "target": "hospital information systems;clinical information systems;hong kong"
  },
  {
    "source": "prioritization of potential candidate disease genes by topological similarity of proteinprotein interaction network and phenotype data . <eos> we construct a reliable heterogeneous network by fusing multiple networks . we devise a random walk based algorithm on the reliable heterogeneous network . combining topological similarity with phenotype data helps to predict causal genes . the algorithm is still in good performance at low parameter values .",
    "target": "disease genes;topological similarity;proteinprotein interaction networks;phenotype;random walk"
  },
  {
    "source": "extended beta regression in r shaken , stirred , mixed , and partitioned . <eos> beta regression an increasingly popular approach for modeling rates and proportions is extended in various directions ( a ) bias correction reduction of the maximum likelihood estimator , ( b ) beta regression tree models by means of recursive partitioning , ( c ) latent class beta regression by means of finite mixture models . all three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved latent heterogeneity in the data . using the analogy of smithson and verkuilen ( <digit> ) , these extensions make beta regression not only a better lemon squeezer ( compared to classical least squares regression ) but a full fledged modern juicer offering lemon based drinks shaken and stirred ( bias correction and reduction ) , mixed ( finite mixture model ) , or partitioned ( tree model ) . all three extensions are provided in the r package betareg ( at least 2.4 <digit> ) , building on generic algorithms and implementations for bias correction reduction , model based recursive partioning , and finite mixture models , respectively . specifically , the new functions betatree ( ) and betamix ( ) reuse the object oriented flexible implementation from the r packages party and flexmix , respectively .",
    "target": "beta regression;r;bias correction;recursive partitioning;finite mixture;bias reduction"
  },
  {
    "source": "four layer framework for combinatorial optimization problems domain . <eos> four layer framework for combinatorial optimization problems models domain is suggested for applied problems structuring and solving ( <digit> ) basic combinatorial models and multicriteria decision making problems ( e.g. , clustering , knapsack problem , multiple choice problem , multicriteria ranking , assignment allocation ) ( <digit> ) composite models procedures ( e.g. , multicriteria combinatorial problems , morphological clique problem ) ( <digit> ) basic ( standard ) solving frameworks , e.g. ( i ) hierarchical morphological multicriteria design ( hmmd ) ( ranking , combinatorial synthesis based on morphological clique problem ) , ( ii ) multi stage design ( two level hmmd ) , ( iii ) special multi stage composite framework ( clustering , assignment location , multiple choice problem ) and ( <digit> ) domain oriented solving frameworks , e.g. ( a ) design of modular software , ( b ) design of test inputs for multi function system testing , ( c ) combinatorial planning of medical treatment , ( d ) design and improvement of communication network topology , ( e ) multi stage framework for information retrieval , ( f ) combinatorial evolution and forecasting of software , devices . the multi layer approach covers decision cycle , i.e. , problem statement , models , algorithms procedures , solving schemes , decisions , decision analysis and improvement .",
    "target": "combinatorial optimization;problem structuring;decision making;problem solving environment;system architecture;system design"
  },
  {
    "source": "computing monodromy via continuation methods on random riemann surfaces . <eos> we consider a riemann surface x defined by a polynomial f ( x , y ) of degree d , whose coefficients are chosen randomly . hence , we can suppose that x is smooth , that the discriminant delta ( x ) of f has d ( d <digit> ) simple roots , delta , and that delta ( <digit> ) not equal <digit> , i.e. the corresponding fiber has d distinct points y ( <digit> ) , ... , y ( d ) . when we lift a loop <digit> is an element of gamma subset of c delta by a continuation method , we get d paths in x connecting y ( <digit> ) , ... , y ( d ) , hence defining a permutation of that set . this is called monodromy . here we present experimentations in maple to get statistics on the distribution of transpositions corresponding to loops around each point of delta . multiplying families of neighbor transpositions , we construct permutations and the subgroups of the symmetric group they generate . this allows us to establish and study experimentally two conjectures on the distribution of these transpositions and on transitivity of the generated subgroups . assuming that these two conjectures are true , we develop tools allowing fast probabilistic algorithms for absolute multivariate polynomial factorization , under the hypothesis that the factors behave like random polynomials whose coefficients follow uniform distributions . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "monodromy;continuation methods;random riemann surface;symmetric group;algorithms;bivariate polynomial;plane curve;absolute factorization;algebraic geometry;maple code"
  },
  {
    "source": "feature based decision aggregation in modular neural network classifiers . <eos> in several modular neural network ( mnn ) architectures , the individual decisions at the module level have to be integrated together using a voting scheme . all these voting schemes use the outputs of the individual modules to produce a global output without inferring explicit information from the problem feature space . this makes the choice of the aggregation procedure very subjective . in this work , a new mnn architecture will be presented . this architecture integrates learning into the voting scheme . we will be focusing on making the decision fusion a more dynamic process . in this context , dynamic means the aggregation procedure which has the flexibility to adapt to changes in the input . this approach requires the aggregation procedure to gather information about the input to help better understand how to dynamically aggregate decisions .",
    "target": "modular neural networks;classification;classifier combination;dynamic decision fusion"
  },
  {
    "source": "efficient bootstrap with weakly dependent processes . <eos> the efficient bootstrap methodology is developed for overidentified moment conditions models with weakly dependent observation . the resulting bootstrap procedure is shown to be asymptotically valid and can be used to approximate the distributions of t t statistics , the j j statistic for overidentifying restrictions , and wald , lagrange multiplier and distance statistics for nonlinear hypotheses . the asymptotic validity of the efficient bootstrap based on a computationally less demanding approximate k k step estimator is also shown . the finite sample performance of the proposed bootstrap is assessed using simulations in an intertemporal consumption based asset pricing model .",
    "target": "mixing;consumption capm;gel;gmm;hypothesis testing"
  },
  {
    "source": "multi relay cooperative diversity protocol with improved spectral efficiency . <eos> cooperative diversity protocols have attracted a great deal of attention since they are thought to be capable of providing diversity multiplexing tradeoff among single antenna wireless devices . in the high signal to noise ratio ( snr ) region , cooperation is rarely required hence , the spectral efficiency of the cooperative protocol can be improved by applying a proper cooperation selection technique . in this paper , we present a simple cooperation selection technique based on instantaneous channel measurement to improve the spectral efficiency of cooperative protocols . we show that the same instantaneous channel measurement can also be used for relay selection . in this paper two protocols are proposed proactive and reactive the selection of one of these protocols depends on whether the decision of cooperation selection is made before or after the transmission of the source . these protocols can successfully select cooperation along with the best relay from a set of available m relays . if the instantaneous source to destination channel is strong enough to support the system requirements , then the source simply transmits to the destination as a noncooperative direct transmission otherwise , a cooperative transmission with the help of the selected best relay is chosen by the system . analysis and simulation results show that these protocols can achieve higher order diversity with improved spectral efficiency , i.e. , a higher diversity multiplexing tradeoff in a slow fading environment .",
    "target": "cooperative diversity;spectral efficiency;diversity multiplexing tradeoff;relay selection;fading channel;outage probability"
  },
  {
    "source": "a motion tolerant dissolve detection algorithm . <eos> gradual shot change detection is one of the most important research issues in the field of video indexing retrieval . among the numerous types of gradual transitions , the dissolve type gradual transition is considered the most common one , but it is also the most difficult one to detect . in most of the existing dissolve detection algorithms , the false miss detection problem caused by motion is very serious . in this paper , we present a novel dissolve type transition detection algorithm that can correctly distinguish dissolves from disturbance caused by motion . we carefully model a dissolve based on its nature and then use the model to filter out possible confusion caused by the effect of motion . experimental results show that the proposed algorithm is indeed powerful .",
    "target": "dissolve detection;shot change detection;fade detection"
  },
  {
    "source": "gmm based evaluation of emotional style transformation in czech and slovak . <eos> in the development of the voice conversion and the emotional speech style transformation in the text to speech systems , it is very important to obtain feedback information about the users opinion on the resulting synthetic speech quality . for this reason , the evaluations of the quality of the produced synthetic speech must often be performed for comparison . the main aim of the experiments described in this paper was to find out whether the classifier based on gaussian mixture models ( gmms ) could be applied for evaluation of male and female resynthesized speech that had been transformed from neutral to four emotional states ( joy , surprise , sadness , and anger ) spoken in czech and slovak languages . we suppose that it is possible to combine this gmm based statistical evaluation with the classical one in the form of listening tests or it can replace them . for verification of our working hypothesis , a simple gmm emotional speech classifier with a one level structure was realized . the next task of the performed experiment was to investigate the influence of different types and values ( mean , median , standard deviation , relative maximum , etc. ) of the used speech features ( spectral and or supra segmental ) on the gmm classification accuracy . the obtained gmm evaluation scores are compared with the results of the conventional listening tests based on the mean opinion scores . in addition , correctness of the gmm classification is analyzed with respect to the influence of the setting of the parameters during the gmm trainingthe number of mixture components and the types of speech features . the paper also describes the comparison experiment with the reference speech corpus taken from the berlin database of emotional speech in german language as the benchmark for the evaluation of the performance of our one level gmm classifier . the obtained results confirm practical usability of the developed gmm classifier , so we will continue in this research with the aim to increase the classification accuracy and compare it with other approaches like the support vector machines .",
    "target": "emotional speech transformation;spectral and prosodic features of speech;gmm based emotion classification"
  },
  {
    "source": "collage of two dimensional words . <eos> we consider a new operation on one dimensional ( resp . two dimensional ) word languages , obtained by piling up , one on top of the other , words of a given recognizable language ( resp . two dimensional recognizable language ) on a previously empty one dimensional ( resp . two dimensional ) array . the resulting language is the set of words seen from above a position in the array is labeled by the topmost letter . we show that in the one dimensional case , the language is always recognizable . this is no longer true in the two dimensional case which is shown by a counter example , and we investigate in which particular cases the result may still hold . ( c ) <digit> published by elsevier b.v.",
    "target": "regular languages;picture languages"
  },
  {
    "source": "inference management , trust and obfuscation principles for quality of information in emerging pervasive environments . <eos> the emergence of large scale , distributed , sensor enabled , machine to machine pervasive applications necessitates engaging with providers of information on demand to collect the information , of varying quality levels , to be used to infer about the state of the world and decide actions in response . in these highly fluid operational environments , involving information providers and consumers of various degrees of trust and intentions , information transformation , such as obfuscation , is used to manage the inferences that could be made to protect providers from misuses of the information they share , while still providing benefits to their information consumers . in this paper , we develop the initial principles for relating to inference management and the role that trust and obfuscation plays in it within the context of this emerging breed of applications . we start by extending the definitions of trust and obfuscation into this emerging application space . we , then , highlight their role as we move from the tightly coupled to loosely coupled sensory inference systems and describe how quality , value and risk of information relate in collaborative and adversarial systems . next , we discuss quality distortion illustrated through a human activity recognition sensory system . we then present a system architecture to support an inference firewall capability in a publish subscribe system for sensory information and conclude with a discussion and closing remarks .",
    "target": "inference management;obfuscation;quality of information;risk of information;value of information;qoi;voi;roi"
  },
  {
    "source": "digging in the digg social news website . <eos> the rise of social media aggregating websites provides platforms where users can actively publish , evaluate , and disseminate content in a collaborative way . in this paper , we present a large scale empirical study about digg.com , one of the biggest social media aggregating websites . our analysis is based on crawls of 1.5 million users and <digit> million published stories on digg . we study the distinct network structure , the collaborative user characteristics , and the content dissemination process on digg . we empirically illustrate that friendship relations are used effectively in disseminating half of the content , although there exists a high overlap between the interests of friends . a successful content dissemination process can also be performed by random users who are browsing and digging stories . since <digit> % of the published content on digg is defined as news , it is important for the content to obtain sufficient votes in a short period of time before becoming obsolete . finally , we show that the synchronization of users ' activities in time is the key to a successful content dissemination process . the dynamics between users ' voting activities consequently decrease the efficiency of friendship relations during content dissemination . the results presented in this paper define basic observations and measurements to understand the underlying mechanism of disseminating content in current online social news aggregators . these findings are helpful to understand the influence of service interfaces and user behaviors on content dissemination .",
    "target": "user characteristics;content dissemination;friendship relations;social media website"
  },
  {
    "source": "usage of agents in document management . <eos> extensible java based agent framework ( xjaf ) is a pluggable architecture of the hierarchical intelligent agents system with communication based on kqml . workers , inc. is a workflow management system implemented using mobile agents . it is especially suited for highly distributed and heterogeneous environments . the application of the above mentioned systems will be considered in the area of document management systems .",
    "target": "document management;workflow management systems;mobile agents"
  },
  {
    "source": "ezpal environment for composing constraint axioms by instantiating templates . <eos> many ontology development tools allow users to supplement frame based representations with arbitrary logical sentences . however , few users actually take advantage of this opportunity . for example , in the ontolingua ontology library , only <digit> % of the ontologies have any user defined axioms . we believe the difficulty of composing axioms primarily accounts for the lack of axioms in these knowledge bases many domain experts can not translate their thoughts into abstract and symbolic representations . we attempt to remedy the difficulties by identifying groups of axioms that manifest common patterns , creating templates that allow users to compose axioms by filling in the blanks . we studied axioms in two public ontology libraries , and derived <digit> templates that cover <digit> % of all the user defined axioms . we describe our methodology for identifying the templates and present examples . we constructed an interface that allows users to create constraints on knowledge bases by filling in blanks our usability testing shows that users could use templates to encode axioms with a success rate similar to that of experts writing directly in an axiom language . our approach should foster the introduction of axioms and constraints that are currently missing in many ontologies .",
    "target": "frame based system;knowledge acquisition;knowledge representation"
  },
  {
    "source": "critical infrastructure dependencies a holistic , dynamic and quantitative approach . <eos> the proper functioning of critical infrastructures is crucial to societal well being . however , critical infrastructures are not isolated , but instead are tightly coupled , creating a complex system of interconnected infrastructures . dependencies between critical infrastructures can cause a failure to propagate from one critical infrastructure to other critical infrastructures , aggravating and prolonging the societal impact . for this reason , critical infrastructure operators must understand the complexity of critical infrastructures and the effects of critical infrastructure dependencies . however , a major problem is posed by the fact that detailed information about critical infrastructure dependencies is highly sensitive and is usually not publicly available . moreover , except for a small number of holistic and dynamic research efforts , studies are limited to a few critical infrastructures and generally do not consider time dependent behavior . this paper analyzes how a failed critical infrastructure that can not deliver products and services impacts other critical infrastructures , and how a critical infrastructure is affected when another critical infrastructure fails . the approach involves a holistic analysis involving multiple critical infrastructures while incorporating a dynamic perspective based on the time period that a critical infrastructure is non operational and how the impacts evolve over time . this holistic approach , which draws on the results of a survey of critical infrastructure experts from several countries , is intended to assist critical infrastructure operators in preparing for future crises .",
    "target": "critical infrastructure dependencies;holistic treatment;dynamic analysis;quantitative analysis"
  },
  {
    "source": "traffic distribution for end to end qos routing with multicast multichannel services . <eos> with the development of multimedia group applications and multicasting demands , the construction of multicast routing tree satisfying quality of service ( qos ) is more important . a multicast tree , which is constructed by existing multicast algorithms , suffers three major weaknesses ( <digit> ) it can not be constructed by multichannel routing , transmitting a message using all available links , thus the data traffic can not be preferably distributed ( <digit> ) it does not formulate duplication capacity consequently , duplication capacity in each node can not be optimally distributed ( <digit> ) it can not change the number of links and nodes used optimally . in fact , it can not employ and cover unused backup multichannel paths optimally . to overcome these weaknesses , this paper presents a polynomial time algorithm for distributed optimal multicast routing and quality of service ( qos ) guarantees in networks with multichannel paths which is called distributed optimal multicast multichannel routing algorithm ( dommr ) . the aim of this algorithm is ( <digit> ) to minimize end to end delay across the multichannel paths , ( <digit> ) to minimize consumption of bandwidth by using all available links , and ( <digit> ) to maximize data rate by formulating network resources . dommr is based on the linear programming formulation ( lpf ) and presents an iterative optimal solution to obtain the best distributed routes for traffic demands between all edge nodes . computational experiments and numerical simulation results will show that the proposed algorithm is more efficient than the existing methods . the simulation results are obtained by applying network simulation tools such as qsb , opnet and matlb to some samples of network . we then introduce a generalized problem , called the delay constrained multicast multichannel routing problem , and show that this generalized problem can be solved in polynomial time .",
    "target": "traffic distribution;multicasting;quality of services;multichannel path;linear programming;optimized routing"
  },
  {
    "source": "improved error exponent for time invariant and periodically time variant convolutional codes . <eos> an improved upper bound on the error probability ( first error event ) of time invariant convolutional codes , and the resulting error exponent , is derived ill this paper . the improved error bound depends on both the delay of the code k and its width ( the number of symbols that enter the delay line in parallel ) b. determining the error exponent of time invariant convolutional codes is an open problem . while the previously known bounds on the error probability of time invariant codes led to the block coding exponent , obtain a better error exponent ( strictly better for b > <digit> ) . in the limit b > infinity our error exponent equals the yudkin viterbi exponent derived for time variant convolutional codes . these results are also used to derive an improved error exponent for periodically time variant codes .",
    "target": "error exponent;convolutional codes;error probability;time invariant codes;yudkin viterbi exponent;periodically time variant codes"
  },
  {
    "source": "online reputation management for improving marketing by using a hybrid mcdm model . <eos> online reputation management ( orm ) has been considered as a significant tool of internet marketing . the purpose of this paper is to construct a decision model for evaluating performances and improving professional services of marketing . to investigate the interrelationship and influential weights among criteria , this study uses a hybrid mcdm model including decision making trial and evaluation laboratory ( dematel ) , dematel based analytic network process ( called danp ) . the empirical findings reveal that criteria have self effect relationships based on dematel technique . according to the network relation map ( nrm ) , the dimension that professional services of marketing should improve first when carrying out orm is online reputation . in the five criteria for evaluation , distributed reputation systems is the most important criterion impacting orm , followed by employees and social responsibility .",
    "target": "online reputation management;mcdm;professional services of marketing;dematel;danp"
  },
  {
    "source": "feasibility of a primarily digital research library . <eos> this position paper explores the issues related to the feasibility of having a primarily digital research library support the teaching and research needs of a university . the asian university for women ( auw ) , a new university in chittagong , bangladesh , will open in september <digit> . it must make a decision regarding the investment to be made in research resources to support the university . mass digitization efforts now make it possible to consider establishing a research library that consists primarily of digital resources rather than print . there are , however , many issues that make this consideration quite complex and far from certain . in this paper we explore the issues at a preliminary level . we focus on four broad perspectives in order to begin addressing the complex interactions that must be considered in transitioning to a primarily digital research environment technical , economic , policy and social issues . the purpose of this paper is to begin to explore a research agenda for transitioning from a model for libraries where resources are primarily print to one that is predominantly digital . our research in this area is just beginning , so our purpose is to raise the issues rather than offer firm conclusions .",
    "target": "digital research;libraries;mass digitization;digital libraries"
  },
  {
    "source": "targeting multiple myeloma cells and their bone marrow microenvironment . <eos> although multiple myeloma ( mm ) is sensitive to chemotherapy and radiation therapy , long term disease free survival is rare , and mm remains incurable despite conventional and high dose therapies . direct ( cell cell contact ) and soluble ( via cytokines ) forms of interactions between mm cells and bone marrow stroma regulate growth , survival , and homing of mm cells . these interactions also play a critical role in angiogenesis and in myeloma bone disease . in recent years , several studies have established the biologic significance of cytokines in mm pathogenesis and delineated signaling cascades mediating their effects , providing the framework for related novel therapies targeting not only the mm cell , but also the bone marrow microenvironment .",
    "target": "multiple myeloma;bone marrow microenvironment;novel therapies"
  },
  {
    "source": "a network service curve approach for the stochastic analysis of networks . <eos> the stochastic network calculus is an evolving new methodology for backlog and delay analysis of networks that can account for statistical multiplexing gain . this paper advances the stochastic network calculus by deriving a network service curve , which expresses the service given to a flow by the network as a whole in terms of a probabilistic bound . the presented network service curve permits the calculation of statistical end to end delay and backlog bounds for broad classes of arrival and service distributions . the benefits of the derived service curve are illustrated for the exponentially bounded burstiness ( ebb ) traffic model . it is shown that end to end performance measures computed with a network service curve are bounded by o ( h log h ) , where h is the number of nodes traversed by a flow . using currently available techniques that compute end to end bounds by adding single node results , the corresponding performance measures are bounded by o ( h <digit> ) .",
    "target": "network service curve;stochastic network calculus;quality of service"
  },
  {
    "source": "mitigating kinematic locking in the material point method . <eos> the material point method exhibits kinematic locking when traditional linear shape functions are used with a rectangular grid . the locking affects both the strain and the stress fields , which can lead to inaccurate results and nonphysical behavior . this paper presents a new anti locking approach that mitigates the accumulation of fictitious strains and stresses , significantly improving the kinematic response and the quality of all field variables . the technique relies on the huwashizu multi field variational principle , with separate approximations for the volumetric and the deviatoric portions of the strain and stress fields . the proposed approach is validated using a series of benchmark examples from both solid and fluid mechanics , demonstrating the broad range of modeling possibilities within the mpm framework when combined with appropriate anti locking techniques and algorithms .",
    "target": "locking;material point method;meshfree methods;particle methods"
  },
  {
    "source": "computational dialectic and rhetorical invention . <eos> this paper has three dimensions , historical , theoretical and social . the historical dimension is to show how the ciceronian system of dialectical argumentation served as a precursor to computational models of argumentation schemes such as araucaria and carneades . the theoretical dimension is to show concretely how these argumentation schemes reveal the interdependency of rhetoric and logic , and so the interdependency of the normative with the empirical . it does this by identifying points of disagreement in a dialectical format through using argumentation schemes and critical questions . the social dimension is to show how the ciceronian dialectical viewpoint integrates with the use of computational tools that can be used to support the principle of reason based deliberation and facilitate deliberative democracy .",
    "target": "argumentation schemes;deliberative democracy;informal logic;ciceronian rhetoric;carneades model;fallacies;persuasion"
  },
  {
    "source": "the roadmaker 's algorithm for the discrete pulse transform . <eos> the discrete pulse transform ( dpt ) is a decomposition of an observed signal into a sum of pulses , i.e. , signals that are constant on a connected set and zero elsewhere . originally developed for <digit> d signal processing , the dpt has recently been generalized to more dimensions . applications in image processing are currently being investigated . the time required to compute the dpt as originally defined via the successive application of lulu operators ( members of a class of minimax filters studied by rohwer ) has been a severe drawback to its applicability . this paper introduces a fast method for obtaining such a decomposition , called the roadmaker 's algorithm because it involves filling pits and razing bumps . it acts selectively only on those features actually present in the signal , flattening them in order of increasing size by sub tracing an appropriate positive or negative pulse , which is then appended to the decomposition . the implementation described here covers <digit> d signal as well as two and <digit> d image processing in a single framework . this is achieved by considering the signal or image as a function defined on a graph , with the geometry specified by the edges of the graph . whenever a feature is flattened , nodes in the graph are merged , until eventually only one node remains . at that stage , a new set of edges for the same nodes as the graph , forming a tree structure , defines the obtained decomposition . the roadmaker 's algorithm is shown to be equivalent to the dpt in the sense of obtaining the same decomposition . however , its simpler operators are not in general equivalent to the lulu operators in situations where those operators are not applied successively . a by product of the roadmaker 's algorithm is that it yields a proof of the so called highlight conjecture , stated as an open problem in <digit> . we pay particular attention to algorithmic details and complexity , including a demonstration that in the <digit> d case , and also in the case of a complete graph , the roadmaker 's algorithm has optimal complexity it runs in time o ( m ) , where m is the number of arcs in the graph .",
    "target": "clustering algorithms;digital filters;digital signal processing;discrete transforms;multidimensional signal processing;nonlinear filters;signal analysis;tree graphs"
  },
  {
    "source": "implementation relations and test generation for systems with distributed interfaces . <eos> some systems interact with their environment at physically distributed interfaces called ports and we separately observe sequences of inputs and outputs at each port . as a result we can not reconstruct the global sequence that occurred and this reduces our ability to distinguish different systems in testing or in use . in this paper we explore notions of conformance for an input output transition system that has multiple ports , adapting the widely used ioco implementation relation to this situation . we consider two different scenarios . in the first scenario the agents at the different ports are entirely independent . alternatively , it may be feasible for some external agent to receive information from more than one of the agents at the ports of the system , these local behaviours potentially being brought together and here we require a stronger implementation relation . we define implementation relations for these scenarios and prove that in the case of a single port system the new implementation relations are equivalent to ioco . in addition , we define what it means for a test case to be controllable and give an algorithm that decides whether this condition holds . we give a test generation algorithm to produce sound and complete test suites . finally , we study two implementation relations to deal with partially specified systems .",
    "target": "formal approaches to testing;systems with distributed ports;formal methodologies to develop distributed software systems"
  },
  {
    "source": "dual centers type <digit> fuzzy clustering framework and its verification and validation indices . <eos> the clustering model considers dual centers rather than single centers . the dual centers type <digit> clustering model and algorithm are proposed . the relations among parameters of the proposed model are explained . the degrees of belonging to the clusters are defined by type <digit> fuzzy numbers . the verification and verification indices are developed for model evaluation .",
    "target": "dual centers clustering;interval type <digit> fuzzy clustering;pcm;cluster center uncertainty;validation index;verification index"
  },
  {
    "source": "generalized sharing in survivable optical networks . <eos> shared path protection has been demonstrated to be a very efficient survivability scheme for optical networking . in this scheme , multiple backup paths can share a given optical channel if their corresponding primary routes are not expected to fail simultaneously . the focus in this area has been the optimization of the total channels ( i.e. , bandwidth ) provisioned in the network through the intelligent routing of primary and backup routes . in this work , we extend the current path protection sharing scheme and introduce the generalized sharing concept . in this concept , we allow for additional sharing of important node devices . these node devices ( e.g. , optical electronic optical regenerators ( oeos ) , pure all optical converters , etc. ) constitute the dominant cost factor in an optical backbone network and the reduction of their number is of paramount importance . for demonstration purposes , we extend the concept of <digit> n shared path protection to allow for the sharing of electronic regenerators needed for coping with optical transmission impairments . both design and control plane issues are discussed through numerical examples . considerable cost reductions in electronic budget are demonstrated .",
    "target": "optical networks;shared protection"
  },
  {
    "source": "on the usefulness of knowledge of error variances in the consistent estimation of an unreplicated ultrastructural model . <eos> this article considers an unreplicated ultrastructural model and discusses the asymptotic properties of three consistent estimators of slope parameter arising from the knowledge of measurement error variances . conditions are deduced when knowing the error variances associated with both the study and the explanatory variables is more less beneficial than using a single error variance in the formulation of slope estimators .",
    "target": "error variance;ultrastructural model;measurement errors;reliability ratio"
  },
  {
    "source": "exact solution of the heat equation with boundary condition of the fourth kind by hes variational iteration method . <eos> in this paper , solutions of the heat equation with the boundary condition of the fourth kind are presented . the proposed solution is based on hes variational iteration method , after the application of which the exact solution of the problem is obtained .",
    "target": "heat equation;variational iteration method"
  },
  {
    "source": "efficient neighborhood search for the one machine earlinesstardiness scheduling problem . <eos> this paper addresses the one machine scheduling problem where the objective is to minimize a sum of costs such as earlinesstardiness costs . since the sequencing problem is np hard , local search is very useful for finding good solutions . unlike scheduling problems with regular cost functions , the scheduling ( or timing ) problem is not trivial when the sequence is fixed . therefore , the local search approaches must deal with both job interchanges in the sequence and the timing of the sequenced jobs . we present a new approach that efficiently searches in a large neighborhood and always returns a solution for which the timing is optimal .",
    "target": "neighborhoods;scheduling;earlinesstardiness cost;single machine;search method"
  },
  {
    "source": "power characteristics of inductive interconnect . <eos> the width of an interconnect line affects the total power consumed by a circuit . the effect of wire sizing on the power characteristics of an inductive interconnect line is presented in this paper . the matching condition between the driver and the load affects the power consumption since the short circuit power dissipation may decrease and the dynamic power will increase with wider lines . a tradeoff , therefore , exists between short circuit and dynamic power in inductive interconnects . the short circuit power increases with wider linewidths only if the line is underdriven . the power characteristics of inductive interconnects therefore may have a great influence on wire sizing optimization techniques . an analytic solution of the transition time of a signal propagating along an inductive interconnect with an error of less than <digit> % is presented . the solution is useful in wire sizing synthesis techniques to decrease the overall power dissipation . the optimum linewidth that minimizes the total transient power dissipation is determined . an analytic solution for the optimum width with an error of less than <digit> % is presented . for a specific set of line parameters and resistivities , a reduction in power approaching <digit> % is achieved as compared to the minimum wire width . considering the driver size in the design process , the optimum wire and driver size that minimizes the total transient power is also determined .",
    "target": "inductive interconnect;short circuit power;dynamic power;transient power dissipation;characteristic impedance;underdamped systems"
  },
  {
    "source": "convergence acceleration of rungekutta schemes for solving the navierstokes equations . <eos> the convergence of a rungekutta ( rk ) scheme with multigrid is accelerated by preconditioning with a fully implicit operator . with the extended stability of the rungekutta scheme , cfl numbers as high as <digit> can be used . the implicit preconditioner addresses the stiffness in the discrete equations associated with stretched meshes . this rk implicit scheme is used as a smoother for multigrid . fourier analysis is applied to determine damping properties . numerical dissipation operators based on the roe scheme , a matrix dissipation , and the cusp scheme are considered in evaluating the rk implicit scheme . in addition , the effect of the number of rk stages is examined . both the numerical and computational efficiency of the scheme with the different dissipation operators are discussed . the rk implicit scheme is used to solve the two dimensional ( <digit> d ) and three dimensional ( <digit> d ) compressible , reynolds averaged navierstokes equations . turbulent flows over an airfoil and wing at subsonic and transonic conditions are computed . the effects of the cell aspect ratio on convergence are investigated for reynolds numbers between 5.7106 5.7 <digit> <digit> and <digit> <digit> <digit> <digit> . it is demonstrated that the implicit preconditioner can reduce the computational time of a well tuned standard rk scheme by a factor between <digit> and <digit> .",
    "target": "65n22;76h05"
  },
  {
    "source": "realizations of the game domination number . <eos> domination game is a game on a finite graph which includes two players . first player , dominator , tries to dominate a graph in as few moves as possible meanwhile the second player , staller , tries to hold him back and delay the end of the game as long as she can . in each move at least one additional vertex has to be dominated . the number of all moves in the game in which dominator makes the first move and both players play optimally is called the game domination number and is denoted by ( gamma _ g ) . the total number of moves in a staller start game is denoted by ( gamma _ g prime ) . it is known that ( gamma _ g ( g ) gamma _ g prime ( g ) le <digit> ) for any graph ( g ) . graph ( g ) realizes a pair ( ( k , l ) ) if ( gamma _ g ( g ) k ) and ( gamma _ g prime ( g ) l ) . it is shown that pairs ( ( 2k ,2 k <digit> ) ) for all ( k ge <digit> ) can be realized by a family of <digit> connected graphs . we also present <digit> connected classes which realize pairs ( ( k , k ) ) and ( ( k , k <digit> ) ) . exact game domination number for combs and <digit> connected realization of the pair ( ( 2k 1,2 k ) ) are also given .",
    "target": "realizations;game domination number;domination game"
  },
  {
    "source": "lumiproxy a hybrid representation of image based models . <eos> in this paper , we present a hybrid representation of image based models combining the textured planes and the hierarchical points . taking a set of depth images as input , our method starts from classifying input pixels into two categories , indicating the planar and non planar surfaces respectively . for the planar surfaces , the geometric coefficients are reconstructed to form the uniformly sampled textures . for nearly planar surfaces , some textured planes , called lumiproxies , are constructed to represent the equivalent visual appearance . the hough transform is used to find the positions of these textured planes , and optic flow measures are used to determine their textures . for remaining pixels corresponding to the non planar geometries , the point primitive is applied , reorganized as the obb tree structure . then , texture mapping and point splatting are employed together to render the novel views , with the hardware acceleration .",
    "target": "lumiproxy;sampling;surface fitting;image based rendering"
  },
  {
    "source": "constraint based methods for biological sequence analysis . <eos> the need for processing biological information is rapidly growing , owing to the masses of new information in digital form being produced at this time . old methodologies for processing it can no longer keep up with this rate of growth . the methods of artificial intelligence ( ai ) in general and of language processing in particular can offer much towards solving this problem . however , interdisciplinary research between language processing and molecular biology is not yet widespread , partly because of the effort needed for each specialist to understand the other one 's jargon . we argue that by looking at the problems of molecular biology from a language processing perspective , and using constraint based logic methodologies we can shorten the gap and make interdisciplinary collaborations more effective . we shall discuss several sequence analysis problems in terms of constraint based formalisms such concept formation rules , constraint handling rules ( chr ) and their grammatical counterpart , chrg . we postulate that genetic structure analysis can also benefit from these methods , for instance to reconstruct from a given rna secondary structure , a nucleotide sequence that folds into it . our proposed methodologies lend direct executability to high level descriptions of the problems at hand and thus contribute to rapid while efficient prototyping .",
    "target": "concept formation;constraint handling rules;rna secondary structure;protein structure;gene prediction;constraint handling rule grammars"
  },
  {
    "source": "adaptive load balancing algorithm for multiple homing mobile nodes . <eos> in places where mobile users can access multiple wireless networks simultaneously , a multipath scheduling algorithm can benefit the performance of wireless networks and improve the experience of mobile users . however , existing literature shows that it may not be the case , especially for tcp flows . according to early investigations , there are mainly two reasons that result in bad performance of tcp flows in wireless networks . one is the occurrence of out of order packets due to different delays in multiple paths . the other is the packet loss which is resulted from the limited bandwidth of wireless networks . to better exploit multipath scheduling for tcp flows , this paper presents a new scheduling algorithm named adaptive load balancing algorithm ( albam ) to split traffic across multiple wireless links within the isp infrastructure . targeting at solving the two adverse impacts on tcp flows , albam develops two techniques . firstly , albam takes advantage of the bursty nature of tcp flows and performs scheduling at the flowlet granularity where the packet interval is large enough to compensate for the different path delays . secondly , albam develops a packet number estimation algorithm ( pnea ) to predict the buffer usage in each path . with pnea , albam can prevent buffer overflow and schedule the tcp flow to a less congested path before it suffers packet loss . simulations show that albam can provide better performance to tcp connections than its other counterparts .",
    "target": "multiple path scheduling;multiple interface;local domain"
  },
  {
    "source": "query by output . <eos> it has recently been asserted that the usability of a database is as important as its capability . understanding the database schema , the hidden relationships among attributes in the data all play an important role in this context . subscribing to this viewpoint , in this paper , we present a novel data driven approach , called query by output ( qbo ) , which can enhance the usability of database systems . the central goal of qbo is as follows given the output of some query q on a database d , denoted by q ( d ) , we wish to construct an alternative query q such that q ( d ) and q ( d ) are instance equivalent . to generate instance equivalent queries from q ( d ) , we devise a novel data classification based technique that can handle the at least one semantics that is inherent in the query derivation . in addition to the basic framework , we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility . our framework is evaluated comprehensively on three real data sets and the results show that the instance equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities .",
    "target": "query by output;instance equivalent queries;at least one semantics"
  },
  {
    "source": "three dimensional quantitative structureactivity relationships study on hiv <digit> reverse transcriptase inhibitors in the class of dipyridodiazepinone derivatives , using comparative molecular field analysis1 . <eos> a three dimensional quantitative structureactivity relationships ( 3d qsar ) method , comparative molecular field analysis ( comfa ) , was applied to a set of dipyridodiazepinone ( nevirapine ) derivatives active against wild type ( wt ) and mutant type ( y181c ) hiv <digit> reverse transcriptase . the starting geometry of dipyridodiazepinone was taken from x ray crystallographic data . all <digit> derivatives , divided into a training set of <digit> compounds and a test set of <digit> molecules , were then constructed and full geometrical optimizations were performed , based on a semiempirical molecular orbital method ( am1 ) . comfa was used to discriminate between structural requirements for wt and y181c inhibitory activities . the resulting comfa models yield satisfactory predictive ability regarding wt and y181c inhibitions , with r2cv 0.624 and 0.726 , respectively . comfa contour maps reveal that steric and electrostatic interactions corresponding to the wt inhibition amount to 58.5 % and 41.5 % , respectively , while steric and electrostatic effects have approximately equal contributions for the explanation of inhibitory activities against y181c . the contour maps highlight different characteristics for different types of wild type and mutant type hiv <digit> rt. in addition , these contour maps agree with experimental data for the binding topology . consequently , the results obtained provide information for a better understanding of the inhibitorreceptor interactions of dipyridodiazepinone analogs . <digit> elsevier science inc .",
    "target": "3d qsar;comfa;nevirapine;hiv <digit> rt;nnrti;quantum chemical calculations;molecular modeling"
  },
  {
    "source": "a shared memory implementation of the hierarchical radiosity method . <eos> the radiosity method is a simulation method from computer graphics to visualize the global illumination in scenes containing diffuse objects within an enclosure . a variety of realizations ( including parallel approaches ) were proposed to achieve a high efficiency while guaranteeing the same accuracy of the graphical representation . the hierarchical radiosity method reduces the computational costs considerably but results in a highly irregular algorithm which makes a parallel implementation more difficult . we investigate a task oriented shared memory implementation and present optimizations with different behavior concerning locality and granularity . to be able to concentrate on load balancing and scalability issues , we use a shared memory machine with uniform memory access time , the sb pram . ( c ) <digit> elsevier science b.v. all rights reserved .",
    "target": "shared memory implementation;hierarchical radiosity method;granularity;scalability;task parallelism"
  },
  {
    "source": "a transaction mapping algorithm for frequent itemsets mining . <eos> in this paper , we present a novel algorithm for mining complete frequent itemsets . this algorithm is referred to as the tm ( transaction mapping ) algorithm from hereon . in this algorithm , transaction ids of each itemset are mapped and compressed to continuous transaction intervals in a different space and the counting of itemsets is performed by intersecting these interval lists in a depth first order along the lexicographic tree . when the compression coefficient becomes smaller than the average number of comparisons for intervals intersection at a certain level , the algorithm switches to transaction id intersection . we have evaluated the algorithm against two popular frequent itemset mining algorithms , fp growth and declat , using a variety of data sets with short and long frequent patterns . experimental data show that the tm algorithm outperforms these two algorithms .",
    "target": "algorithms;frequent itemsets;association rule mining;data mining"
  },
  {
    "source": "semi continuous network flow problems . <eos> we consider semi continuous network flow problems , that is , a class of network flow problems where some of the variables are restricted to be semi continuous . we introduce the semi continuous inflow set with variable upper bounds as a relaxation of general semi continuous network flow problems . two particular cases of this set are considered , for which we present complete descriptions of the convex hull in terms of linear inequalities and extended formulations . we consider a class of semi continuous transportation problems where inflow systems arise as substructures , for which we investigate complexity questions . finally , we study the computational efficacy of the developed polyhedral results in solving randomly generated instances of semi continuous transportation problems .",
    "target": "network flow problems;mixed integer programming;semi continuous variables;90c11;90c35;90c57"
  },
  {
    "source": "construct message authentication code with one way hash functions and block ciphers . <eos> we suggest an mac scheme which combines a hash function and an block cipher in order . we strengthen this scheme to prevent the problem of leaking the intermediate hash value between the hash function and the block cipher by additional random bits . the requirements to the used hash function are loosely . security of the proposed scheme is heavily dependent on the underlying block cipher . this scheme is efficient on software implementation for processing long messages and has clear security properties .",
    "target": "one way hash function;block cipher;mac;cryptography"
  },
  {
    "source": "estimation of uncertainty in dynamic simulation results . <eos> this paper presents a new approach for calculation of uncertainty in dynamic simulation results . the statistical moments ( mean , variance , skewness etc. ) of the simulation results are calculated using gaussian quadrature with '' customized '' weight function . based on these moments , an approximating probability density function ( pdf ) is created by expansion into orthogonal polynomial series . the percentiles of the distribution can then be calculated . the method is computationally less demanding than monte carlo simulation when the number of uncertain parameters are limited . a number of examples are used to illustrate the applicability of the proposed framework .",
    "target": "dynamic simulation;uncertainty propagation;stochastic simulation"
  },
  {
    "source": "a neural implementation of the jade algorithm ( njade ) using higher order neurons . <eos> a neural implementation of the jade algorithm , called njade , is developed which adaptively determines the mixing matrices to be jointly diagonalized with the jade algorithm . this alleviates the problem of algebraically determining these mixing matrices which becomes a very tedious if not impossible undertaking with high dimensional data . the new learning rule uses higher order neurons and generalizes oja 's pca learning rule . as a test case the new njade algorithm is applied to high dimensional natural image ensembles to learn appropriate edge filter structures . quantitative comparison concerning various filter characteristics is made with results obtained with a probabilistic ica algorithm with kernel based source density estimation .",
    "target": "njade;higher order neurons;natural images;independent component analysis;neural network"
  },
  {
    "source": "jpeg <digit> encoding method for reducing tiling artifacts . <eos> this paper proposes an effective jpeg <digit> encoding method for reducing tiling artifacts , which cause one of the biggest problems in jpeg <digit> encoders . symmetric pixel extension is generally thought to be the main factor in causing artifacts . however this paper shows that differences in quantization accuracy between tiles are a more significant reason for tiling artifacts at middle or low bit rates . this paper also proposes an algorithm that predicts whether tiling artifacts will occur at a tile boundary in the rate control process and that locally improves quantization accuracy by the original post quantization control . this paper further proposes a method for reducing processing time which is yet another serious problem in the jpeg <digit> encoder . the method works by predicting truncation points using the entropy of wavelet transform coefficients prior to the arithmetic coding . these encoding methods require no additional processing in the decoder . the experiments confirmed that tiling artifacts were greatly reduced and that the coding process was considerably accelerated .",
    "target": "jpeg <digit>;tiling artifacts;rate control;acceleration of coding process"
  },
  {
    "source": "laparoscopic myomectomy . <eos> the appearance of uterine myomas has been linked to infertility . it has been suggested that surgical management of myomas by laparoscopic myomectomy improves fertility rates in these group of patients . in this paper we initially describe specific aspects of the surgical technique of laparoscopic myomectomy including the set up , precise technique for hysteroromy , enucleation of the myoma , suturing of the uterus , and extraction of the myoma . we detail recent findings that demonstrate improved fertility rates in women undergoing laparoscopic myomectomy . we recommend that , when criteria for selection of patients is strictly adhered to and patients present with no other associated infertility , laparoscopic myomectomy be used to increase the implantation rate .",
    "target": "laparoscopic myomectomy;uterine myoma;fertility;laparotomy"
  },
  {
    "source": "theoretical study on the antioxidant properties of <digit> ' hydroxychalcones h atom vs. electron transfer mechanism . <eos> the free radical scavenging activity of six <digit> ' hydroxychalcones has been studied in gas phase and solvents using the density functional theory ( dft ) method . the three main working mechanisms , hydrogen atom transfer ( hat ) , stepwise electron transfer proton transfer ( et pt ) and sequential proton loss electron transfer ( splet ) have been considered . the o h bond dissociation enthalpy ( bde ) , ionization potential ( ip ) , proton affinity ( pa ) and electron transfer energy ( ete ) parameters have been computed in gas phase and solvents . the theoretical results confirmed the important role of the b ring in the antioxidant properties of hydroxychalcones . in addition , the calculated results matched well with experimental values . the results suggested that hat would be the most favorable mechanism for explaining the radical scavenging activity of hydroxychalcone in gas phase , whereas splet mechanism is thermodynamically preferred pathway in aqueous solution .",
    "target": "hydroxychalcones;radical scavenging;dft;hydrogen atom transfer;stepwise electron transfer proton transfer;sequential proton loss electron transfer"
  },
  {
    "source": "program analysis for event based distributed systems . <eos> designing distributed applications around the idiom of events has several benefits including extensibility and scalability . to improve conciseness , safety , and efficiency of corresponding programs , several authors have recently proposed programming languages or language extensions with support for event based programming . the presence of a dedicated programming language and compilation process offers avenues for program analyses to further improve simplicity , safety , and expressiveness of distributed event based software . this paper presents three program analyses specifically designed for event based programs immutability analysis avoids costly cloning of events in the presence of co located handlers for same events guard analysis allows for simple yet expressive subscriptions which can be further simplified and handled efficiently causality analysis determines causal dependencies among events which are related , allowing unrelated events to be transferred independently for efficiency . we convey the benefits of our approach by empirically evaluating their performance benefits .",
    "target": "program analysis;event;distributed;language;correlation"
  },
  {
    "source": "automatic determination of envelopes and other derived curves within a graphic environment . <eos> dynamic geometry programs provide environments where accurate construction of geometric configurations can be done . nevertheless , intrinsic limitations in their standard development technology mostly produce objects that are equationally unknown and so can not be further used in constructions . in this paper , we pursue the development of a geometric system that uses in the background the symbolic capabilities of two computer algebra systems , cocoa and mathematica . the cooperation between the geometric and symbolic modules of the software is illustrated by the computation of plane envelopes and other derived curves . these curves are described both graphically and analytically . since the equations of these curves are known , the system allows the construction of new elements depending on them . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .",
    "target": "envelopes;dynamic geometry;symbolic computing;symbolic numeric interface;groebner bases;caustics;pedals"
  },
  {
    "source": "a platform for okapi based contextual information retrieval . <eos> we present an extensible java based platform for contextual retrieval based on the probabilistic information retrieval model . modules for dual indexes , relevance feedback with blind or machine learning approaches and query expansion with context are integrated into the okapi system to deal with the contextual information . this platform allows easy extension to include other types of contextual information .",
    "target": "contextual information retrieval;probabilistic model"
  },
  {
    "source": "the ternary description language as a formalism for the parametric general systems theory part iii . <eos> this part is a continuation of the first and second parts of my article that were published in the international journal of general systems , vol . <digit> ( <digit> <digit> ) , pp. <digit> <digit> vol . <digit> ( <digit> ) , pp. <digit> <digit> . in part iii , we deal with the construction of the axiomatic system of the ternary description language ( tdl ) . axioms and rules of inference are formulated . on the basis of these axioms and rules some theorems of tdl are proved . several system theoretical laws , which concern the values of systems parameters , are proved as theorems of tdl . thus the deductive construction of general systems theory is made .",
    "target": "axioms;system theoretical laws;syntactical priority;synonymy;rules of substitution;rules of replacement;theorems of the tdl"
  },
  {
    "source": "definitions and approaches to model quality in model based software development a review of literature . <eos> more attention is paid to the quality of models along with the growing importance of modelling in software development . we performed a systematic review of studies discussing model quality published since <digit> to identify what model quality means and how it can be improved . from forty studies covered in the review , six model quality goals were identified i.e. , correctness , completeness , consistency , comprehensibility , confinement and changeability . we further present six practices proposed for developing high quality models together with examples of empirical evidence . the contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "modelling;model quality;systematic review;model driven development;uml"
  },
  {
    "source": "animal identification introduction and history . <eos> in the beginning of the <digit> research institutes in different countries developed the first electronic animal identification systems . these systems were tested on experimental farms . the first systems were all built with the conventional components and attached to a collar around the cows neck . in the 1980s however special integrated circuits were developed minimising the size of the transponders . now in the 1990s , official organisations are testing systems for identification and registration of all animals to control movements from birth to slaughterhouse . this will enable farm livestock to be traced at the outbreak of diseases and residues in slaughter animals to be followed up . injectable transponders , electronic eartags and rumenal bolusses are being used .",
    "target": "transponder;electronic eartag;electronic identification;ruminant bolus"
  },
  {
    "source": "a fractional variational iteration method for solving fractional nonlinear differential equations . <eos> recently , fractional differential equations have been investigated by employing the famous variational iteration method . however , all the previous works avoid the fractional order term and only handle it as a restricted variation . a fractional variational iteration method was first proposed in g.c. wu , e.w.m. lee , fractional variational iteration method and its application , phys . lett . a <digit> ( <digit> ) <digit> and gave a generalized lagrange multiplier . in this paper , two fractional differential equations are approximately solved with the fractional variational iteration method .",
    "target": "fractional variational iteration method;modified riemannliouville derivative;fractional corrected functional"
  },
  {
    "source": "three level privacy control for sensing based real world content digital diorama . <eos> digital diorama , the sensing based real world content , can be constructed by integrating real time information obtained from sensors monitoring the real world . in order to increase the benefits of viewers without violating the privacy of monitored persons , this paper proposes three level privacy control over the monitored persons based on their agreement to the usage of their information obtained from sensors privacy control with i ) no agreement , ii ) partial agreement . and iii ) mutual agreement . i ) presents only their positions to show where persons are . ii ) additionally presents the information which can be automatically obtained from sensors such as age and gender to show what kinds of persons are where without disclosing the visual appearances . iii ) presents their visual appearances based on their mutual agreement with specific viewers . our evaluation indicated that the representation simulating each privacy control presented information from sensors with acceptable privacy protection .",
    "target": "sensing based real world content;benefits of viewers;privacy protection"
  },
  {
    "source": "enhancing wireless video streaming using lightweight approximate authentication . <eos> in this paper we propose a novel lightweight approximate authentication algorithm that provides efficient protection for wireless video streaming where bit errors are frequent . the benefits of the proposed algorithm over other algorithms are fast execution , due to its simplicity , and small message authentication code size . the algorithm is capable of detecting even a small number of bit errors in relatively small packets that are used in video streaming . these features have never previously been available at the same time . another benefit of the approximate authentication is that it supports error resilient video decoding by dropping packets with too many bit errors , thus improving the perceived quality of the video stream . the performance of the algorithm is demonstrated via simulations and measurements",
    "target": "wireless video streaming;approximate authentication;error resilient video coding;security;qos"
  },
  {
    "source": "distributed federative qos resource management . <eos> in a distributed multimedia system qos resources have to be managed carefully to utilize the resource pool in a way that bottlenecks can be avoided . our key idea is to let the applications participate on the resource management . we propose a distributed architecture with a fine granulated , balanced resource management with explicit qos characteristics . the architecture is based on a distributed cooperative resource manager which combines both the adaption and reservation principle for guaranteeing qos . we have designed and implemented a prototype of our federative qos resource manager ( fqrm ) in the java environment .",
    "target": "qos resource management;distributed resources;cooperative resource sharing"
  },
  {
    "source": "algorithms for on line order batching in an order picking warehouse . <eos> in manual order picking systems , order pickers walk or ride through a distribution warehouse in order to collect items required by ( internal or external ) customers . order batching consists of combining these indivisible customer orders into picking orders . with respect to order batching , two problem types can be distinguished in off line ( static ) batching , all customer orders are known in advance in on line ( dynamic ) batching , customer orders become available dynamically over time . this paper considers an on line order batching problem in which the maximum completion time of the customer orders arriving within a certain time period has to be minimized . the author shows how heuristic approaches for off line order batching can be modified in order to deal with the on line situation . in a competitive analysis , lower and upper bounds for the competitive ratios of the proposed algorithms are presented . the proposed algorithms are evaluated in a series of extensive numerical experiments . it is demonstrated that the choice of an appropriate batching method can lead to a substantial reduction of the maximum completion time .",
    "target": "order batching;order picking;warehouse management;on line optimization"
  },
  {
    "source": "passive and active reduction techniques for on chip high frequency digital power supply noise . <eos> signal integrity has become a major problem in digital ic design . one cause is device scaling that results in a sharp reduction of supply voltage , creating stringent noise margin requirements to ensure functionality . this paper introduces both a novel on chip decoupling capacitance methodology and active noise cancellation ( anc ) structure . the decoupling methodology focuses on quantification and location . the anc structure , with an area of <digit> mu m x <digit> mu m , uses decoupling capacitance to sense noise and inject a proportional current into v ( ss ) as a method of reduction . a chip has been designed and fabricated using tsmc 's <digit> nm technology . measurements show that the decoupling methodology improved the average voltage headroom loss by <digit> % while the anc structure improved the average voltage headroom loss by <digit> % .",
    "target": "power supply noise;decoupling capacitance;active noise cancellation;on chip interconnect;power distribution"
  },
  {
    "source": "on modal mu calculus over finite graphs with small components or small tree width . <eos> this paper is a continuation and correction of a paper presented by the same authors at the conference gandalf <digit> . we consider the modal mu calculus and some fragments of it . for every positive integer k we consider the class scck of all finite graphs whose strongly connected components have size at most k , and the class twk of all finite graphs of tree width at most k. as upper bounds , we show that for every k , the temporal logic ctl collapses to alternation free mu calculus in scck and in tw1 , the winning condition for parity games of any index n belongs to the level delta ( <digit> ) of modal mu calculus . as lower bounds , we show that buchi automata are not closed under complement in tw2 and cobuchi nondeterministic and alternating automata differ in tw1 .",
    "target": "modal mu calculus;tree width;strongly connected component"
  },
  {
    "source": "product line selection and pricing analysis impact of genetic relaxations . <eos> a model for the product line selection and pricing problem ( plsp ) is presented and three solution procedures based on a genetic algorithm are developed to analyze the results based on consumer preference patterns . since the plsp model is nonlinear and integer , two of the solution procedures use genetic encoding to relax the np hard model . the relaxations result in linear integer and shortest path models for the fitness evaluation which are solved using branch and bound and labeling algorithms , respectively . performance of the quality of solutions generated by the procedures is evaluated for various problem sizes and customer preference structures . the results show that the genetic relaxations provide efficient and effective solution methodologies for the problem , when compared to the pure artificial intelligence technique of genetic search . the impact of the preference structure on the product line and the managerial implications of the solution characteristics generated by the genetic relaxations are also discussed . the models can be used to explicitly consider tradeoffs between marketing and operations concerns in designing a product line . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "product line;pricing;genetic algorithms;heuristics"
  },
  {
    "source": "a decomposed model predictive functional control approach to air vehicle pitch angle control . <eos> the requirements for the pitch angle control of an air vehicle are a very fast response with as few vibrations as possible . the vibrations can damage the equipment that is carried within the body of the vehicle . the main problem to deal with is the relatively fast and under damped dynamics of the vehicle and the slow actuators and sensors . we have solved the problem by using a predictive approach . the main idea of this approach is a process output prediction based on a decomposed process model . the decomposition enables the extension of the model based approach to processes with integrative behavior such as in the case of a rocket 's pitch angle control . the proposed approach is not only useful in this case but it gives us a framework to design the control for a wide range of processes . we compared the predictive design methodology with the classical compensator control approach , known from aerospace system control . the advantage of the new approach is the reduced vibrations during the transient response .",
    "target": "modelling;vibrations;compensation;decomposition methods;predictive control"
  },
  {
    "source": "the impact of electronic medical record systems on outpatient workflows a longitudinal evaluation of its workflow effects . <eos> the promise of the electronic medical record ( emr ) <digit> lies in its ability to reduce the costs of health care delivery and improve the overall quality of care a promise that is realized through major changes in workflows within the health care organization . yet little systematic information exists about the workflow effects of emrs . moreover , some of the research to date points to reduced satisfaction among physicians after implementation of the emr and increased time , i.e. , negative workflow effects . a better understanding of the impact of the emr on workflows is , hence , vital to understanding what the technology really does offer that is new and unique . ( i ) to empirically develop a physician centric conceptual model of the workflow effects of emrs ( ii ) to use the model to understand the antecedents to the physicians workflow expectation from the new emr ( iii ) to track physicians satisfaction overtime , <digit> months and <digit> months after implementation of the emr ( iv ) to explore the impact of technology learning curves on physicians reported satisfaction levels . the current research uses the mixed method technique of concept mapping to empirically develop the conceptual model of an emr 's workflow effects . the model is then used within a controlled study to track physician expectations from a new emr system as well as their assessments of the emr 's performance <digit> months and <digit> months after implementation . the research tracks the actual implementation of a new emr within the outpatient clinics of a large northeastern research hospital . the pre implementation survey netted <digit> physician responses post implementation time <digit> survey netted <digit> responses , and time <digit> survey netted <digit> physician responses . the implementation of the actual emr served as the intervention . since the study was conducted within the same setting and tracked a homogenous group of respondents , the overall study design ensured against extraneous influences on the results . outcome measures were derived empirically from the conceptual model . they included <digit> items that measured physician perceptions of the emr 's workflow effect on the following eight issues ( <digit> ) administration , ( <digit> ) efficiency in patient processing , ( <digit> ) basic clinical processes , ( <digit> ) documentation of patient encounter , ( <digit> ) economic challenges and reimbursement , ( <digit> ) technical issues , ( <digit> ) patient safety and care , and ( <digit> ) communication and confidentiality . the items were used to track expectations prior to implementation and they served as retrospective measures of satisfaction with the emr in post implementation time <digit> and time <digit> . the findings suggest that physicians conceptualize emrs as an incremental extension of older computerized provider order entries ( cpoes ) rather than as a new innovation . the emrs major functional advantages are seen to be very similar to , if not the same as , those of cpoes . technology learning curves play a statistically significant though minor role in shaping physician perceptions . the physicians expectations from the emr are based on their prior beliefs rather than on a rational evaluation of the emr 's fit , functionality , or performance . their decision regarding the usefulness of the emr is made very early , within the first few months of use of the emr . these early perceptions then remain stable and become the lens through which subsequent experience with the emr is interpreted . the findings suggest a need for communication based interventions aimed at explaining the value , fit , and usefulness of emrs to physicians early in the pre and immediate post emr implementation stages .",
    "target": "physicians adoption of emrs;diffusion of technology;health information technology;work flow effects;outpatient emr use"
  },
  {
    "source": "application of the lattice boltzmann method to flow in aneurysm with ring shaped stent obstacles . <eos> to resolve the characteristics of a highly complex flow , a lattice boltzmann method with an extrapolation boundary technique was used in aneurysms with and without transverse objects oil the upper wall , and results were compared with the non stented aneurysm . the extrapolation boundary concept allows the use of cartesian grids even when the boundaries do not conform to cartesian coordinates . to case the code development and facilitate the incorporation of new physics , a new scientific programming strategy based on object oriented concepts was developed . the reduced flow , smaller vorticity magnitude and wall shear stress , and smaller du dy near the dome of the aneurysm were observed when the proposed stent obstacles were used . the height of the stent obstacles was more effective to reduce the vorticity near the dome of the aneurysm than the width of the stent . the rectangular stent with <digit> % height of vessel radius was observed to be optimal and decreased the magnitude of the vorticity by <digit> % near the dome of the aneurysm . copyright ( c ) <digit> john wiley sons , ltd .",
    "target": "lattice boltzmann method;aneurysm;stent;computational fluid dynamics;object oriented program"
  },
  {
    "source": "simplification rules for the coherent probability assessment problem . <eos> in this paper we develop a procedure for checking the consistency ( coherence ) of a partial probability assessment . the general problem ( called cpa ) is np complete , hence , to have a reasonable application some heuristic is needed . our proposal differs from others because it is based on a skilful use of the logical relations present among the events . in other approaches the consistency problem is reduced directly to the satisfiability of a system of linear constraints . here , thanks to the characterization of particular configurations and to the elimination of variables , an instance of the problem is reduced to smaller instances . to obtain such results , we introduce a procedure based on rules resembling those given by davis putnam for the satisfiability of boolean formulas . at the end a particularized description of an actual implementation is given .",
    "target": "simplification rules;coherent probability assessment;probabilistic satisfiability"
  },
  {
    "source": "constructing special k dominating sets using variations on the greedy algorithm . <eos> this paper focuses on the efficient selection of a special type of subset of network nodes , which we call a k k spr set , for the purpose of coordinating the routing of messages through a network . such a set is a special k k hop connected k k dominating set that has an additional property that promotes the regular occurrence of routers in all directions . the distributed algorithms introduced here for obtaining a k k spr set require that each node broadcast at most three messages to its k k hop neighbors . these transmissions can be made asynchronously . the time required to send these messages and the sizes of the resulting sets are compared by means of data collected from simulations . the main contribution is the adaptation of some variations of the distributed greedy algorithms to the problem of generating a small k k spr set . these variations are much faster than the standard distributed greedy algorithm . yet , when used with a sensible choice for a certain parameter , our empirical evidence strongly suggests that the resulting set size will generally be very close to the set size for the standard greedy algorithms .",
    "target": "dominating set;routing;distributed greedy algorithm;ad hoc network"
  },
  {
    "source": "the convergence test of transformation performance of resource cities in china considering undesirable output . <eos> the main challenge for sustainable development of resource cities is to work out a feasible strategy for transformation processes . this paper introduces a new approach for analysis of transformation performance . using the environmental production technology and a malmquist resource performance index ( mrpi ) , we conduct sigma , absolute beta and conditional beta convergence tests for the transformation performance of <digit> resource cities in china . the results show that mrpi does not follow the same trend as economic strength of three chinese regions . in addition , the transformation performance results exhibit a convergence trend for the <digit> resource cities . crown copyright ( c ) <digit> published by elsevier ltd. all rights reserved .",
    "target": "convergence test;transformation performance;resource cities"
  },
  {
    "source": "ant colony optimization based clustering methodology . <eos> a novel aco based methodology ( aco c ) is proposed for spatial clustering . it works in data sets with no a priori information . it includes solution evaluation , neighborhood construction and data set reduction . it has a multi objective framework , and yields a set of non dominated solutions . experimental results show that aco c outperforms other competing approaches .",
    "target": "ant colony optimization;clustering;data set reduction;multiple objectives"
  },
  {
    "source": "breaching euclidean distance preserving data perturbation using few known inputs . <eos> we examine euclidean distance preserving data perturbation as a tool for privacy preserving data mining . such perturbations allow many important data mining algorithms ( e.g. hierarchical and k means clustering ) , with only minor modification , to be applied to the perturbed data and produce exactly the same results as if applied to the original data . however , the issue of how well the privacy of the original data is preserved needs careful study . we engage in this study by assuming the role of an attacker armed with a small set of known original data tuples ( inputs ) . little work has been done examining this kind of attack when the number of known original tuples is less than the number of data dimensions . we focus on this important case , develop and rigorously analyze an attack that utilizes any number of known original tuples . the approach allows the attacker to estimate the original data tuple associated with each perturbed tuple and calculate the probability that the estimation results in a privacy breach . on a real <digit> dimensional dataset , we show that the attacker , with <digit> known original tuples , can estimate an original unknown tuple with less than <digit> % error with probability exceeding 0.8 .",
    "target": "euclidean distance;data perturbation;privacy;data mining"
  },
  {
    "source": "consistent interactive augmentation of live camera images with correct near field illumination . <eos> inserting virtual objects in real camera images with correct lighting is an active area of research . current methods use a high dynamic range camera with a fish eye lens to capture the incoming illumination . the main problem with this approach is the limitation to distant illumination . therefore , the focus of our work is a real time description of both near and far field illumination for interactive movement of virtual objects in the camera image of a real room . the daylight , which is coming in through the windows , produces a spatially varying distribution of indirect light in the room therefore a near field description of incoming light is necessary . our approach is to measure the daylight from outside and to simulate the resulting indirect light in the room . to accomplish this , we develop a special dynamic form of the irradiance volume for real time updates of indirect light in the room and combine this with importance sampling and shadow maps for light from outside . this separation allows object movements with interactive frame rates ( <digit> <digit> fps ) . to verify the correctness of our approach , we compare images of synthetic objects with real objects .",
    "target": "global illumination;augmented image synthesis"
  },
  {
    "source": "dynamic programming based approximation algorithms for sequence alignment with constraints . <eos> given two sequences x and y , the classical dynamic programming solution to the local alignment problem searches for two subsequences i subset of or equal to x and j subset of or equal to y with maximum similarity score under a given scoring scheme . in several applications , variants of this problem arise with different objectives and with length constraints on the subsequences i and j. this constraint can be explicit , such as requiring i j greater than or equal to t , or j < t , or may be implicit such as in cyclic sequence comparison , or as in the maximization of length normalized scores , and driven by practical considerations . we present a survey of approximation algorithms for various alignment problems with constraints , and several new approximation algorithms . these approximations are in two distinct senses in one the constraints are satisfied but the score computed is within a prescribed tolerance of the optimum instead of the exact optimum . in another , the alignment returned is assured to have at least the optimum score with respect to the given constraints , but the length constraints are satisfied to within a prescribed tolerance from the required values . the algorithms proposed involve applications of techniques from fractional programming and dynamic programming .",
    "target": "dynamic programming;approximation algorithm;local alignment;cyclic sequence comparison;fractional programming;normalized local alignment;length restricted local alignment;ratio maximization"
  },
  {
    "source": "a model of multisecond timing behaviour under peak interval procedures . <eos> in this study , the authors developed a fundamental theory of interval timing behaviour , inspired by the learning to time ( let ) model and the scalar expectancy theory ( set ) model , and based on quantitative analyses of such timing behaviour . our experiments used the peak interval procedure with rats . the proposed model of timing behaviour comprises clocks , a regulator , a mixer , a response , and memory . using our model , we calculated the basic clock speeds indicated by the subjects behaviour under such peak procedures . in this model , the scalar property can be defined as a kind of transposition , which can then be measured quantitatively . the akaike information criterion ( aic ) values indicated that the current model fit the data slightly better than did the set model . our model may therefore provide a useful addition to set for the analysis of timing behaviour .",
    "target": "scalar expectancy theory;basic clock speed;peak procedure;scalar property;akaike information criterion;learning to time model"
  },
  {
    "source": "the support vector machine under test . <eos> support vector machines ( svms ) are rarely benchmarked against other classification or regression methods . we compare a popular svm implementation ( libsvm ) to <digit> classification methods and <digit> regression methodsall accessible through the software rby the means of standard performance measures ( classification error and mean squared error ) which are also analyzed by the means of bias variance decompositions . svms showed mostly good performances both on classification and regression tasks , but other methods proved to be very competitive .",
    "target": "support vector machines;benchmark;classification;regression;comparative study"
  },
  {
    "source": "using cascade method for table access on small devices . <eos> users increasingly expect access to web data from a wide range of devices , both wired and wireless . the goal of our research is to inform the design of applications that support data access by providing reasonably seamless migration of web data among internet compatible devices with minimal loss of effectiveness and efficiency . this study focuses on the tables of data on small mobile devices . in this paper we report on the results of a user study that compare effectiveness , efficiency and preference of two methods for the display and use of tables on small screens column row expansion and cascade , a cell based expansion method .",
    "target": "tables;small screen;handheld device;pda;auto transformation;focus context"
  },
  {
    "source": "practical use of polynomials over the reals in proofs of termination . <eos> nowadays , polynomial interpretations are an essential ingredient in the development of tools for proving termination . we have recently proven that polynomial interpretations over the reals are strictly better for proving polynomial termination of rewriting than those which only use integer coefficients . some essential aspects of their practical use , though , remain unexplored or underdeveloped . in this paper , we compare the two current frameworks for using polynomial intepretations over the reals and show that one of them is strictly better than the other , thus making a suitable choice for implementations . we also prove that the use of algebraic real co efficients in the interpretations suffice for termination proofs . we also discuss the use of algorithms and techniques from tarski 's first order logic of the real closed fields for implementing their use in proofs of termination . we argue that more standard constraint solving techniques are better suited for this . we propose an algorithm to solve the polynomial constraints which arise when specific finite subsets of rational ( or even algebraic real ) numbers are considered for giving value to the coefficients . we provide a preliminary experimental evaluation of the algorithm which has been implemented as part of the termination tool mu term .",
    "target": "termination;program analysis;term rewriting;polynomial orderings"
  },
  {
    "source": "detecting coherent energy . <eos> we apply the mathematics of cognitive radio to a single receiver to obtain a new coherent energy metric . this allows us to derive the time correlation law separating gaussian colored noise from coherent signal energy .",
    "target": "coherent energy;cognitive radio;colored noise;random matrix theory"
  },
  {
    "source": "optimal , quality aware scheduling of data consumption in mobile ad hoc networks . <eos> in this paper we study the delivery of quality contextual information in mobile ad hoc networks . we consider that information has a certain quality level that fades over time . mobile context aware applications receive and process disseminated information given that the corresponding quality is above the lowest level . the necessity for optimally scheduling information delivery arises from the dynamic nature of the network , e.g. , probabilistic spreading , caching , deferred delivery , and mobility of nodes . we propose two policies for optimal scheduling information delivery consumption based on the optimal stopping theory . the mobile nodes delay the reporting of information to mobile context aware applications in search for better quality . the proposed policies efficiently deal with the delivery of quality information in mobile ad hoc networks .",
    "target": "mobile ad hoc networks;optimal stopping theory;quality information delivery"
  },
  {
    "source": "partnering enhanced nlp with semantic analysis in support of information extraction . <eos> information extraction using natural language processing ( nlp ) tools focuses on extracting explicitly stated information from textual material . this includes named entity recognition ( ner ) , which produces entities and some of the relationships that may exist among them . intelligent analysis requires examining the entities in the context of the entire document . while some of the relationships among the recognized entities may be preserved during extraction , the overall context of a document may not be preserved . in order to perform intelligent analysis on the extracted information , we provide an ontology , which describes the domain of the extracted information , in addition to rules that govern the classification and interpretation of added elements . the ontology is at the core of an interactive system that assists analysts with the collection , extraction , organization , analysis and retrieval of information , with the topic of terrorism financing as a case study . user interaction provides valuable assistance in assigning meaning to extracted information . the system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference . this case study demonstrates the information extraction features as well as the inference power that is supported by the ontology .",
    "target": "nlp;semantic analysis;information extraction;natural language processing;intelligent analysis;ontology;modeling;owl"
  },
  {
    "source": "on stabilization of gradient based training strategies for computationally intelligent systems . <eos> this paper develops a novel training methodology for computationally intelligent systems utilizing gradient information in parameter updating . the devised scheme uses the first order dynamic model of the training procedure and applies the variable structure systems approach to control the training dynamics . this results in an optimal selection of the learning rate , which is continually updated as prescribed by the adopted strategy . the parameter update rule is then mixed with the conventional error backpropagation method in a weighted average . the paper presents an analysis of the imposed dynamics , which is the response of the training dynamics driven solely by the inputs designed by variable structure control approach . the analysis continues with the global stability proof of the mixed training methodology and the restrictions on the design parameters . the simulation studies presented are focused on the advantages of the proposed scheme with regards to the compensation of the adverse effects of the environmental disturbances and its capability to alleviate the inherently nonlinear behavior of the system under investigation . the performance of the scheme is compared with that of a conventional backpropagation , it is observed that the method presented is robust under noisy observations and time varying parameters due to the integration of gradient descent technique with variable structure systems methodology , in the application example studied , control of a two degrees of freedom direct drive robotic manipulator is considered . a standard fuzzy system is chosen as the controller in which the adaptation is carried out only on the defuzzifier parameters .",
    "target": "variable structure systems;gradient descent;fuzzy control;stable training"
  },
  {
    "source": "improving recruit distribution decisions in the us marine corps . <eos> the united states marine corps ( usmc ) accomplishes its mission to put the right marine in the right place at the right time with the right skills and quality of life in various ways . one of these is a recruit distribution modeling ( rdm ) and information system that assigns new recruits to entry level schools , thereby determining the entire career paths . this article proposes improvements to the existing marine corps decision processes and information systems for recruit distribution . the proposed system , recruit distribution decision support system ( rddss ) , provides intuitive navigation through a hierarchy of switchboards , and promotes data integrity by eliminating manual data entry for data already available in the system . it incorporates four objective measures for understanding the quality of proposed distributions , and allows the user to generate and compare multiple solutions based on the trade off between these objectives . it is a fully functional working prototype system that was installed into the usmc manpower environment , and demonstrated to provide several improvements over the current technology .",
    "target": "recruit distribution;decision support system;manpower modeling"
  },
  {
    "source": "boundary effects on the soil water characteristic curves obtained from lattice boltzmann simulations . <eos> pore scale simulations using a lattice boltzmann method ( lbm ) based numerical model were conducted to examine how the capillary pressure ( pc ) ( p c ) and saturation ( s ) evolve within a virtual porous medium subjected to drainage and imbibition cycles . the results show the presence of a sharp front ( interface separating the wetting and non wetting fluids ) across the cell during the test , which expectably moves up and down as the controlling non wetting fluid pressure at the upper boundary varies to simulate different pc p c levels over the drainage and imbibition cycle . this phenomenon , representing inhomogeneity at the simulated scale , is in conflict with the homogenization applied to the pressure cell for deriving the constitutive pcs p c s relationship . different boundary conditions , adopted to achieve more homogeneous states in the virtual soil , resulted in different pcs p c s curves . no unique relationship between pc p c and s , even with the interfacial area ( anw ) ( a nw ) included , could be found . this study shows dependence of the lbm predicted pcs p c s relation on the chosen boundary conditions . this effect should be taken into account in future numerical studies of multiphase flow within porous media .",
    "target": "lattice boltzmann methods;unsaturated soil physics"
  },
  {
    "source": "a generic sampling framework for improving anomaly detection in the next generation network . <eos> the heterogeneous nature of network traffic in next generation networks ( ngns ) may impose scalability issue to traffic monitoring applications . while this issue can be well addressed by existing sampling approaches , owing to their inherent ' lossy ' characteristic and data reduction principle , traditional sampling techniques suffer from incomplete traffic statistics , which can lead to inaccurate inferences of the network traffic . by focusing on two distinct traffic monitoring applications , namely , anomaly detection and traffic measurement , we highlight the possibility of addressing the accuracy of both applications without having to sacrifice one for the sake of the other . in light of this , we propose a generic sampling framework , which is capable of providing creditable network traffic statistics for accurate anomaly detection in the non , while at the same time preserves the principal purpose of sampling ( i.e. , to sample dominant traffic flows for accurate traffic measurement ) , and thus addressing the accuracy of both applications concurrently . with the emphasize on the accuracy of anomaly detection and the scalability of monitoring devices , the performance evaluation over real network traces demonstrates the superiority of the proposed framework over traditional sampling techniques . copyright ( c ) <digit> john wiley sons , ltd .",
    "target": "sampling framework;anomaly detection;next generation network;scalability;traffic measurement;accuracy"
  },
  {
    "source": "the ( k ) separator problem polyhedra , complexity and approximation results . <eos> given a vertex weighted undirected graph ( g ( v , e , w ) ) and a positive integer ( k ) , we consider the ( k ) separator problem it consists in finding a minimum weight subset of vertices whose removal leads to a graph where the size of each connected component is less than or equal to ( k ) . we show that this problem can be solved in polynomial time for some graph classes including bounded treewidth , ( m k_2 ) free , ( ( g_1 , g_2 , g_3 , p_6 ) ) free , interval filament , asteroidal triple free , weakly chordal , interval and circular arc graphs . polyhedral results with respect to the convex hull of the incidence vectors of ( k ) separators are reported . approximation algorithms are also presented .",
    "target": "polyhedra;approximation algorithms;graph partitioning;complexity theory;optimization"
  },
  {
    "source": "structural and electronic properties of z isomers of ( <digit> alpha > <digit> '' ,2 alpha > o > <digit> '' ) phenylflavans substituted with r h , oh and och3 calculated in aqueous solution with pcm solvation model . <eos> in the search for new antioxidants , flavan structures called our attention , as substructures of many important natural compounds , including catechins ( flavan <digit> ols ) , simple and dimeric proanthocyanidins , and condensed tannins . in this work the conformational space of the z isomers of ( <digit> alpha > <digit> '' , <digit> alpha > o > <digit> '' ) phenylflavans substituted with r h , oh and och3 was scanned in aqueous solution , simulating the solvent by the polarizable continuum model ( pcm ) . geometry optimizations were performed at b3lyp <digit> <digit> g level . electronic distributions were analyzed at a better calculation level , thus improving the basis set ( <digit> <digit> g ) . a topological study based on bader 's theory ( atoms in molecules ) and natural bond orbital ( nbo ) framework was performed . furthermore , molecular electrostatic potential maps ( meps ) were obtained and thoroughly analyzed . the stereochemistry was discussed , and the effect of the solvent was addressed . moreover , intrinsic properties were identified , focusing on factors that may be related to their antioxidant properties . hyperconjugative and inductive effects were described . the coordinated nbo aim analysis allowed us to rationalize the changes of meps in a polar solvent . to investigate the molecular and structural properties of these compounds in biological media , the polarizabilities and dipolar moments were predicted which were further used to enlighten stability and reactivity properties . all conformers were taken into account . relevant stereoelectronic aspects were described for understanding the stabilization and antioxidant function of these structures .",
    "target": "phenylflavans;antioxidants;atoms in molecules;aqueous solvent effect;molecular dipole moment;natural bond orbital analysis"
  },
  {
    "source": "re thinking metaphor , experience and aesthetic awareness . <eos> purpose the purpose of this paper is to explore current questions about metaphor , experience and aesthetic awareness that persist through the variations of critical approaches and projective research in architectural theory and practice . design methodology approach further considerations focus on the advanced technological possibilities which re invest the relations between principles of cybernetics and architecture . findings the current between art and architecture is more than ever manifested in fields related to the computer sciences and its conceptual background cybernetic sciences . originality value the paper re thinks the aesthetic value of architecture and architectural experience in this time of digital productivity .",
    "target": "metaphor;experience;aesthetics;architecture;technology;cybernetics"
  },
  {
    "source": "agent technologies for sensor networks . <eos> the development of agent technologies for sensor networks has received increasing research attention within both the sensor network and multi agent systems research communities . the international workshops on agent technologies for sensor networks ( atsn ) held in <digit> , <digit> and <digit> sought to bring these communities together , and this special issue of the computer journal presents extended versions of some of the papers that appeared at these workshops , along with new submissions specifically for this journal .",
    "target": "agent technologies;sensor networks;networks"
  },
  {
    "source": "optimal consensus of fuzzy opinions under group decision making environment . <eos> the gist of this paper is to propose a new method for aggregating individual fuzzy opinions into an optimal group consensus . by optimality , we mean the sum of weighted dissimilarity among aggregated consensus and individual opinions is minimized . we propose an iterative procedure for approximating the optimal consensus of expert opinions . finally , the importance of each expert is taken into consideration in the process of aggregation . ( c ) <digit> elsevier science b.v. all rights reserved .",
    "target": "fuzzy individual opinions;fuzzy opinions aggregation;group consensus opinion;fuzzy numbers;multi criteria decision making"
  },
  {
    "source": "multiscale bagging and its applications . <eos> we propose multiscale bagging as a modification of the bagging procedure . in ordinary bagging , the bootstrap resampling is used for generating bootstrap samples . we replace it with the multiscale bootstrap algorithm . in multiscale bagging , the sample size in of bootstrap samples may be altered from the sample size n of learning dataset . for assessing the output of a classifier , we compute bootstrap probability of class label the frequency of observing a specified class label in the outputs of classifiers learned from bootstrap samples . a scaling law of bootstrap probability with respect to sigma ( <digit> ) n m has been developed in connection with the geometrical theory . we consider two different ways for using multiscale bagging of classifiers . the first usage is to construct a confidence set of class labels , instead of a single label . the second usage is to find . inputs close to decision boundaries in the context of query by bagging for active learning . it turned out , interestingly , that an appropriate choice of m is m n , i.e. , sigma ( <digit> ) <digit> , for the first usage , and m infinity , i.e. , sigma ( <digit> ) <digit> , for the second usage .",
    "target": "bagging;active learning;confidence level;classification"
  },
  {
    "source": "estimation of lower and upper bounds on the power consumption from scheduled data flow graphs . <eos> in this paper , we present an approach for the calculation of lower and upper bounds on the power consumption of data path resources like functional units , registers , i o ports , and busses from scheduled data flow graphs executing a specified input data stream . the low power allocation and binding problem is formulated , first , it is shown that this problem without constraining the number of resources can be relaxed to the bipartite weighted matching problem which is solvable in o ( n ) ( <digit> ) . n is the number of arithmetic operations , variables , i o access or bus access operations which have to be bound to data path resources , in a second step we demonstrate that the relaxation can be efficiently extended by including lagrange multipliers in the problem formulation to handle a resource constraint , the estimated bounds take into account the effects of resource sharing . the technique can be used , for example , to prune the design space in high level synthesis for low power before the allocation and binding of the resources . the application of the technique on benchmarks with real application input data shows the tightness of the bounds .",
    "target": "low power;high level synthesis;bound estimation;power dissipation;power estimation;switching activity"
  },
  {
    "source": "the effects of perceived risk and technology type on users acceptance of technologies . <eos> previous studies on technology adoption disagree regarding the relative magnitude of the effects of perceived usefulness and perceived ease of use . however these studies did not consider moderating variables . we investigated four potential moderating variables perceived risk , technology type , user experience , and gender in users technology adoption . their moderating effects were tested in an empirical study of <digit> subjects . results showed that perceived risk , technology type , and gender were significant moderating variables . however the effects of user experience were marginal after the variance of errors was removed .",
    "target": "perceived risk;technology type;moderating variable;user experience;gender;technology acceptance;utaut"
  },
  {
    "source": "estimation of process parameter variations in a pre defined process window using a latin hypercube method . <eos> the aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre defined process window . to exemplify the developed methodology , the stochastic simulation technique is used for a sheet metal forming application . a sampling plan based on the latin hypercube sampling method for variation of design parameters is utilized , and the thickness reduction is specified as the response . moreover , the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value . the conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation . consequently , it gives significant insights into the usefulness of individual design parameters . it has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters . furthermore , the dominating design parameters can be predicated using sensitivity analysis , and this in its turn clarifies how the reliability criteria are met . finally , the developed software can be used as an additional module for set up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications .",
    "target": "sheet metal forming;stochastical analysis;sensitivity indicator;admissible process parameter variation;finite element method"
  },
  {
    "source": "architecture and applications of the fingermouse a smart stereo camera for wearable computing hci . <eos> in this paper we present a visual input hci system for wearable computers , the fingermouse . it is a fully integrated stereo camera and vision processing system , with a specifically designed asic performing stereo block matching at 5mpixel s ( e.g. qvga 320240at <digit> fps ) and a disparity range of <digit> , consuming 187mw ( 78mw in the asic ) . it is button sized ( 43mm18mm ) and can be worn on the body , capturing the users hand and processing in real time its coordinates as well as a <digit> bit image of the hand segmented from the background . alternatively , the system serves as a smart depth camera , delivering foreground segmentation and tracking , depth maps and standard images , with a processing latency smaller than 1ms . this paper describes the fingermouse functionality and its applications , and how the specific architecture outperforms other systems in size , latency and power consumption .",
    "target": "wearable computing;hci;foreground segmentation;stereo vision;mobile embedded vision;hand tracking"
  },
  {
    "source": "wireless sensor networking for rain fed farming decision support . <eos> wireless sensor networks ( wsns ) can be a valuable decision support tool for farmers . this motivated our deployment of a wsn system to support rain fed agriculture in india . we defined promising use cases and resolved technical challenges throughout a two year deployment of our common sense net system , which provided farmers with environment data . however , the direct use of this technology in the field did not foster the expected participation of the population . this made it difficult to develop the intended decision support system . based on this experience , we take the following position in this paper currently , the deployment of wsn technology in developing regions is more likely to be effective if it targets scientists and technical personnel as users , rather than the farmers themselves . we base this claim on the lessons learned from the common sense system deployment and the results of an extensive user experiment with agriculture scientists , which we describe in this paper .",
    "target": "wireless sensor network;agriculture;user experiment;developing country"
  },
  {
    "source": "on stable parametric finite element methods for the stefan problem and the mullinssekerka problem with applications to dendritic growth . <eos> we introduce a parametric finite element approximation for the stefan problem with the gibbsthomson law and kinetic undercooling , which mimics the underlying energy structure of the problem . the proposed method is also applicable to certain quasi stationary variants , such as the mullinssekerka problem . in addition , fully anisotropic energies are easily handled . the approximation has good mesh properties , leading to a well conditioned discretization , even in three space dimensions . several numerical computations , including for dendritic growth and for snow crystal growth , are presented .",
    "target": "parametric finite elements;stefan problem;mullinssekerka problem;dendritic growth;gibbsthomson law;kinetic undercooling;snow crystal growth;surface tension;anisotropy"
  },
  {
    "source": "binomial moments of the distance distribution bounds and applications . <eos> we study a combinatorial invariant of codes which counts the number of ordered pairs of codewords in all subcodes of restricted support in a code . this invariant can be expressed as a linear form of the components of the distance distribution of the code with binomial numbers as coefficients . for this reason we call it a binomial moment of the distance distribution . binomial moments appear in the proof of the macwilliams identities and in many other problems of combinatorial coding theory . we introduce a linear programming problem for bounding these linear forms from below . it turns out that some known codes ( <digit> error correcting perfect codes , golay codes , nordstrom robinson code , etc. ) yield optimal solutions of this problem , i.e. , have minimal possible binomial moments of the distance distribution . we derive several general feasible solutions of this problem , which give lower bounds on the binomial moments of codes with given parameters , and derive the corresponding asymptotic bounds . applications of these bounds include new lower bounds on the probability of undetected error for binary codes used over the binary symmetric channel with crossover probability p and optimality of many codes for error detection . asymptotic analysis of the bounds enables us to extend the range of code rates in which the upper bound on the undetected error exponent is tight .",
    "target": "binomial moments;distance distribution;linear programming;undetected error;extremal codes;rodemich theorem"
  },
  {
    "source": "characteristics of wap traffic . <eos> this paper considers the characteristics of wireless application protocol ( wap ) traffic . we start by constructing a wap traffic model by analysing the behaviour of users accessing public wap sites via a monitoring system . a wide range of different traffic scenarios were considered , but most of these scenarios resolve to one of two basic types . the paper then uses this traffic model to consider the effects of large quantities of wap traffic on the core network . one traffic characteristic which is of particular interest in network dimensioning is the degree of self similarity , so the paper looks at the characteristics of aggregated traffic with wap , web and packet speech components to estimate its self similarity . the results indicate that , while wap traffic alone does not exhibit a significant degree of self similarity , a combined load from various traffic sources retains almost the same degree of self similarity as the most self similar individual source .",
    "target": "wap;traffic modelling;self similarity;mobile data"
  },
  {
    "source": "multimodal interactions in typically and atypically developing children natural versus artificial environments . <eos> this review addresses the central role played by multimodal interactions in neurocognitive development . we first analyzed our studies of multimodal verbal and nonverbal cognition and emotional interactions within neuronal , that is , natural environments in typically developing children . we then tried to relate them to the topic of creating artificial environments using mobile toy robots to neurorehabilitate severely autistic children . by doing so , both neural natural and artificial environments are considered as the basis of neuronal organization and reorganization . the common thread underlying the thinking behind this approach revolves around the brains intrinsic properties neuroplasticity and the fact that the brain is neurodynamic . in our approach , neural organization and reorganization using natural or artificial environments aspires to bring computational perspectives into cognitive developmental neuroscience .",
    "target": "multimodal interactions;mobile toy robot;verbal nonverbal development;autism;free game play;positive emotion;neural mediator"
  },
  {
    "source": "theoretical demonstration of symmetric iv i v curves in asymmetric molecular junction of monothiolate alkane . <eos> a molecular junction of an asymmetric molecule generally demonstrates an asymmetric currentvoltage ( iv i v ) curve , due to the unequal voltage drops at the two moleculeelectrode contacts . however , for asymmetric s ( ch2 ) nch3 s ( ch <digit> ) n ch <digit> molecules , symmetric iv i v curves are always obtained in the experimental measurements . here , we investigate the electronic transport of the aus ( ch2 ) 7ch3au au s ( ch <digit> ) <digit> ch <digit> au molecular junction in order to reveal the mechanism of the symmetric iv i v curve with atk package , in which the density functional theory is combined with keldysh nonequilibrium green 's function method to calculate the electronic and transport properties of nanoscale systems . and the symmetric iv i v curve can be interpreted by the curved surface model , which reproduces curved surface of the top electrode in the experiment .",
    "target": "first principles;symmetry;iv i v curve iv i v iv i v i v;thiolalkane monolayer junction"
  },
  {
    "source": "fuzzy reliability analysis of repairable industrial systems using soft computing based hybridized techniques . <eos> the present study analyzes the fuzzy reliability of a repairable industrial system utilizing uncertain data . one traditional ( flt ) and two soft computing based hybridized techniques ( gablt and ngablt ) are used . some very important fuzzy reliability indices of a washing system in a paper plant have been computed . it is observed that gablt performs consistently well in comparison to other two techniques . the analysis may be helpful for improving the performance of the considered system .",
    "target": "flt technique;gablt technique;ngablt technique;nonlinear programming;genetic algorithm;artificial neural networks"
  },
  {
    "source": "demagnetization properties of ipm and spm motors used in the high demanding automotive application . <eos> purpose in order to reduce co2 emissions of new cars many hydraulic and mechanical systems like e.g. water pump , oil pump , power steering , clime compressor have been exchanged with pure electromechanical systems , which are driven only on request . this helps to reduce fuel consumption . this trend requires of utilization of modern brushless electric motors , which are controlled from power electronic control unit ecu . in today 's car can be found between <digit> to <digit> electric motors . many of them are still simple brush type with ferrite magnets . also in this area , drift in the direction of brushless motors can bee seen , because of higher efficiency , longer lifetime , lower noise , better emc and more controllable torque vs speed characteristic . there are different technological solutions , which can been used in the area of brushless motors in order to reduce size and cost of single component . one major factor of bldc ac motor is rear earth permanent magnet material used during production . a magnet material cost could be in the range from <digit> percent ( basis price <digit> ) up to <digit> percent ( basis price <digit> ) of total material motor cost , depends on actual rear earth material price level . in order to reduce magnet cost , the aim of this paper is to find the most robust motor design , which can be resistant against maximum temperature and phase current amplitude for the same magnet material properties , coercive force hcj . this behaviour is called demagnetization property . design methodology approach analysis was performed based on review of literature , own theoretical and practical research and experience in the area of electromechanical systems for automotive application . during motor analysis computer numerical simulation method , cad and experiment were used . findings as a result , comparison of different motors ' topologies with different properties of magnet materials is presented . the worked out methodology shows very good correlation between simulations and measurements . this work can be used in order to reduce test effort and reduce cost of design . practical implications the presented methodology reduces for new designs test effort and development cost and gives an implication of robust motor topology for demagnetization effects . originality value it is the first paper where demagnetization effects have been studied theoretically and in laboratory in order to find the most robust design , reduce magnet cost by reduction of dysprosium content and develop simulation procedure for analysis of demagnetizations behaviours of interior and surface permanent magnet .",
    "target": "automotive application;brushless motor;coercive force;dysprosium;interior permanent magnet ipm;low weight;mass production;cost effectiveness;neodymium;automotive components industry"
  },
  {
    "source": "a novel direct search approach for combined heat and power dispatch . <eos> a novel approach based on the direct search method ( dsm ) is proposed for the solution of combined heat and power ( chp ) dispatch problem . to deal with the mutual dependency of multiple demand and heatpower capacity of cogeneration units , the penalty functions should be considered in dsm to enforce the corresponding violated constraints from the infeasible region into the feasible region . many nonlinear characteristics of the generator can be handled properly in the direct search procedure . to increase the possibility of exploring the search space where the global optimal solution exists , another effective strategy based on a successive refinement search technique is also proposed to guarantee a possibly complete examination of the solution space . numerical experiments are included to demonstrate the proposed direct search approach can obtain a higher quality solution than many existing techniques .",
    "target": "combined heat and power dispatch;direct search method;cogeneration;economic dispatch"
  },
  {
    "source": "fourth and tenth order compact finite difference solutions of perturbed circular vortex flows . <eos> in this study , high order compact finite difference calculations are reported for 2d unsteady incompressible circular vortex flow in primitive variable formulation . the fourth order runge kutta temporal discretization is used together with fourth or tenth order compact spatial discretization . dependent on the perturbation initially imposed , the solutions display a tripole , triangular or square vortex . the comparison of the predictions with the detailed spectral calculations of kloosterziel and carnevale ( j. fluid mech . 1999 388 217 257 ) shows that the vorticity fields are very well captured . the spectral resolution of the present method was quantified from the decomposition of the vorticity distribution in its azimuthal components and compared with reported spectral results . using identical grid resolution to the reference results yields negligible differences in the main features of the flow . the perturbation amplitude and its first harmonic are virtually identical to the reference results for both fourth or tenth order spatial discretization , as theoretically expected but seldom a posteriori verified . the differences between the two spatial discretizations appear only for coarser grids , favouring the tenth order discretization . copyright ( c ) <digit> john wiley sons , ltd .",
    "target": "compact finite differences;circular vortex;high order;incompressible navier stokes"
  },
  {
    "source": "simulation based study of wireless rf interconnects for practical cmos implementation . <eos> an electromagnetic analysis for the practical implementation of on chip antennas to be used as wireless ic interconnects is presented . the undesired electromagnetic signal coupling between the on chip antennas and the metal interconnects is characterized under varying geometries and placement of the metal interconnects . the variations in the transmission gain between the antenna pair due to the typical complementary metal oxide semiconductor ( cmos ) manufacturing requirements are presented . using a <digit> d finite element method ( fem ) based full wave electromagnetic solver , it is shown that the antenna characteristics are significantly impacted by the presence of the essential epitaxial layer and the required minimum metal utilization . it is also shown in a 250nm cmos technology that there can be a significant electromagnetic signal coupling between the on chip transmitting antenna and the metal interconnects on a die ( 12.09 db for a 1.6 mm long , <digit> m wide interconnect at a distance of <digit> m from the antenna ) . design considerations are presented for the metal interconnects in the presence of on chip antennas in order to minimize the undesired electromagnetic signal coupling .",
    "target": "interconnects;electromagnetic;on chip antennas;vlsi"
  },
  {
    "source": "identification of tumor immune system via recurrent neural network . <eos> cancer immunotherapy is an emerging therapy for cancer disease treatment which stimulates immune systems to fight against tumor cells . in this paper , a back propagation neural network with some feedbacks from hidden layer is used as a method of identification for one validated mathematical model . since it is not possible to model complex system due to void of information and knowledge to model all complexity of complex system , identification methods are effective tools for modeling ill defined system . afterward , it is possible to perform control methods on the estimated model to reach the clinical goals . the simulation results have shown the correctness of the identification process .",
    "target": "identification;immune system;multi layer perceptron;artificial neural network"
  },
  {
    "source": "an efficient iterative algorithm for the approximation of the fast and slow dynamics of stiff systems . <eos> the relation between the iterative algorithms based on the computational singular perturbation ( csp ) and the invariance equation ( ie ) methods is examined . the success of the two methods is based on the appearance of fast and slow time scales in the dynamics of stiff systems . both methods can identify the low dimensional surface in the phase space ( slow invariant manifold , sim ) , where the state vector is attracted under the action of fast dynamics . it is shown that this equivalence of the two methods can be expressed by simple algebraic relations . csp can also construct the simplified non stiff system that models the slow dynamics of the state vector on the sim . an extended version of ie is presented which can also perform this task . this new ie version is shown to be exactly similar to a modified version of csp , which results in a very efficient algorithm , especially in cases where the sim dimension is small , so that significant model simplifications are possible .",
    "target": "invariant manifolds;model reduction;multiple time scales;asymptotic analysis;singular perturbation analysis"
  },
  {
    "source": "an adaptive comb filter with flexible notch gain . <eos> this paper proposes an adaptive comb filter with flexible notch gain . it can appropriately remove a periodic noise from an observed signal . the proposed adaptive comb filter uses a simple lms algorithm to update the notch gain coefficient for removing the noise and preserving a desired signal , simultaneously . simulation results show the effectiveness of the proposed comb filter .",
    "target": "comb filter;lms;adaptive algorithm"
  },
  {
    "source": "does the use of structured reporting improve usability a comparative evaluation of the usability of two approaches for findings reporting in a large scale telecardiology context . <eos> poor usability leads to a low adoption rate of telemedicine systems . mode of input , free text or structured report , influences usability . usability and user satisfaction are higher for structured report interfaces in telecardiology .",
    "target": "telecardiology;telemedicine;system usability scale;keystroke level model;heuristic usability evaluation;dicom sr"
  },
  {
    "source": "fixed points of correspondences defined on cone metric spaces . <eos> in the present note , we investigate the fixed points of correspondences defined on cone metric spaces satisfying a conditionally contractive condition .",
    "target": "fixed point;correspondence;cone metric space;banach lattice"
  },
  {
    "source": "personal content management system a semantic approach . <eos> the amount of multimedia resources that is created and needs to be managed is increasing considerably . additionally , a significant increase of metadata , either structured ( metadata fields of standardized metadata formats ) or unstructured ( free tagging or annotations ) is noticed . this increasing amount of data and metadata , combined with the substantial diversity in terms of used metadata fields and constructs , results in severe problems to manage and retrieve these multimedia resources . standardized metadata schemes can be used but the plethora of these schemes results in interoperability issues . in this paper , we propose a metadata model suited for personal content management systems . we create a layered metadata service that implements the presented model as an upper layer and combines different metadata schemes in the lower layers . semantic web technologies are used to define and link formal representations of these schemes . specifically , we create an ontology for the dig35 metadata standard and elaborate on how it is used within this metadata service . to illustrate the service , we present a representative use case scenario consisting of the upload , annotation , and retrieval of multimedia content within a personal content management system .",
    "target": "personal content management;metadata;annotation;interoperability;semantic web;ontology;multimedia standards;reasoning"
  },
  {
    "source": "a next generation multimedia call center for internet commerce imc . <eos> human assistance , as well as automated service , is necessary for providing more convenient services to customers on the internet based commerce system . call centers have been typically human based service systems . however , the services of existing public switched telephone network based call centers are not enough to meet the needs of customers on the internet . most of them have been designed without considering the interaction involved in shopping on the internet in our research , we design a call center named imc ( internet based multimedia call center ) that can be integrated with an internet shopping mall . it contains <digit> parts an internet multimedia dialogue system and a human agent assisting system . the internet multimedia dialogue system is an internet and multimedia version of the interactive voice response service of computer telephony integration based call centers because it provides access to the multimedia web page along with the recorded voice explanation through the internet . the human agent assisting system aims to select the most appropriate human agents in the call center and support them in providing high quality individualized information for each customer . imc is a real time , human embedded system that can provide high quality services cost effectively for internet commerce .",
    "target": "call center;internet commerce;internet shopping mall;computer telephony integration;human embedded system;electronic commerce"
  },
  {
    "source": "wind tunnel experiments of tracer dispersion downwind from a small scale physical model of a landfill . <eos> wind tunnel experiments have been carried out on a small scale physical model of a municipal waste landfill ( mwl ) in the criaciv ( research centre of building aerodynamics and wind engineering ) environmental wind tunnel in prato ( italy ) . the mwl model simulates a landfill whose surface is higher than the surrounding surface , applying a <digit> <digit> scaling factor . modelling an area source such as landfill is a difficult task for numerical models due to turbulence phenomena that modifies the flow near the source increasing ground level concentration ( glc ) . for the specific task , a new set up of the wind tunnel has been developed , with respect to previous studies carried out on line and point sources physical models . the tracer used in the experiments was ethylene , suitable for non buoyant plume conditions , typical for mwl emissions . a detailed result database has been obtained in terms of glc and concentration profiles as well as flow turbulence and velocity field characterisation .",
    "target": "wind tunnel;physical modelling;landfill;concentration profiles;experimental data"
  },
  {
    "source": "exclusively your 's dynamic individuate search by extending user profile . <eos> a universal search engine is unable to provide a personal touch to a user query . to overcome the deficiency of a universal search engine , vertical search engines are used , which return search results from a specific domain . an alternate option is to use a personalized search system . in our endeavor to provide personalized search results , the proposed system , exclusively your 's , observes a user browsing behavior and his actions . based on the observed user behavior , it dynamically constructs user profile which consists of some terms that are related to user 's interest . the constructed profile is later used for query expansion . the goal of research work in this paper is not to provide all the relevant results , but a few high quality personalized search results at the top of ranked list , which in other words means high precision . we performed experiments by personalizing google , yahoo , and naver ( widely used search engine in korea ) . the results show that using exclusively your 's , a search engine yields significant improvement . we also compared the user profile constructed by the proposed approach with other similar personalization approaches the results show a marginal increase in precision .",
    "target": "search engine;personalization;summarization;weighted index;anchor text;hyperlink"
  },
  {
    "source": "constrained diffusion limited aggregation in <digit> dimensions . <eos> diffusion limited aggregation ( dla ) has usually been studied in <digit> dimensions as a model of fractal growth processes such as river networks , plant branching , frost on glass , electro deposition , lightning , mineral deposits , and coral . here , the basic principles are extended into <digit> dimensions and used to create , among other things , believable models of root systems . an additional innovation is a means of constraining the growth of the 3d dla by a surface or containing it within a vessel .",
    "target": "diffusion;aggregation;dla;fractal;branching;brownian motion"
  },
  {
    "source": "skill specific spoken dialogs in a reading tutor that listens . <eos> project listen 's reading tutor listens to children read aloud . a controlled study indicates that the reading tutor helps children 's reading comprehension . however , the results for word attack ( decoding ) skills and word identification skills were not statistically better than in the control condition . our thesis therefore proposes to develop skill specific dialogs based on cognitive skill models and successful tutoring strategies . these dialogs will be dynamically assembled by the reading tutor and include text , speech , illustrations , and dialog parameters . we hypothesize that such dialogs will improve elementary students ' reading abilities .",
    "target": "spoken dialog;reading;children;intelligent tutoring systems;speech recognition"
  },
  {
    "source": "a difference expansion oriented data hiding scheme for restoring the original host images . <eos> this paper proposes a lossless data embedding scheme that exploits the difference expansion of the pixels to conceal large amount of message data in a digital image . the proposed scheme takes into consideration the correlation between the pixel and its surrounding pixels to determine the degree of the difference expansion for message data embedding . the performance has been evaluated in terms of image distortion , payload capacity , as well as embedding rate . the experimental results show that the scheme is capable of providing a great payload capacity , and the image quality of the embedded image is better than that of tians and celiks schemes for a gray level image . what is more , for a color image , the proposed scheme outperforms alattars scheme at low psnr . in addition , the proposed scheme can completely restore the original image after data extraction .",
    "target": "difference expansion;lossless data embedding;information hiding;reversible data hiding"
  },
  {
    "source": "bit parallel random number generation for discrete uniform distributions . <eos> when a die is cast , the outcome is one of the six sides , i.e. the outcome is discrete and uniformly distributed over the range r <digit> , <digit> , <digit> , <digit> , <digit> , <digit> . generating random numbers with such a distribution is very easy obtain a random number w epsilon w , the domain of the random numbers , and take ( w mod r ) <digit> . however , many uniform discrete distributions have a rather short range , e.g. , r <digit> in a dice game , and r <digit> for the walking directions of a <digit> dimensional nonreversal random walk . the number w is typically a machine word , i.e. log ( <digit> ) ( w ) approximate to <digit> in a <digit> bit computer , so generating a log ( <digit> ) ( r ) bit random number has consumed about <digit> random bits . when w much greater than r , it is wasteful and hence inefficient . this paper presents an efficient algorithm for generating random numbers for the distributions with r discrete uniform outcomes . the algorithm uses parallel bit wise operations on machine words . the performance results of the algorithm are presented . the statistical quality of the random numbers generated from this algorithm is also discussed . ( c ) <digit> elsevier science b.v. all rights reserved .",
    "target": "random number generators;discrete uniform distribution;random walks;monte carlo simulation"
  },
  {
    "source": "a metaobject protocol for clforjava . <eos> clforjava is a new implementation of common lisp that intertwines its architecture and operation with java . the authors describe a new architecture for a clos mop that supports transparent , bi directional access between lisp and java . the access requires no special techniques nor syntactic mechanisms on the part of the programmer being either java or lisp . the core of the new mop is a data structure that melds the fundamental structures of java instances ( n tuples ) and clos instances ( <digit> tuples ) in such a way that the respective object systems can interact without cumbersome translations . methods from their respective object systems can interact freely . we discuss certain aspects of the respective mops that prevent a complete integration and replacement of one system by the other .",
    "target": "lisp;java;clos;interoperation"
  },
  {
    "source": "a systematic evaluation of disk imaging in encase 6.8 and linen 6.1 . <eos> tools for disk imaging ( or more generally speaking , digital acquisition ) are a foundation for forensic examination of digital evidence . therefore it is crucial that such tools work as expected . the only way to determine whether this is the case or not is through systematic testing of each tool . in this paper we present such an evaluation of the disk imaging functions of encase 6.8 and linen 6.1 , conducted on behalf of the swedish national laboratory of forensic science . although both tools performed as expected under most circumstances , we identified cases where flaws that can lead to inaccurate and incomplete acquisition results in linen 6.1 were exposed . we have also identified limitations in the tool that were not evident from its documentation . in addition summarizing the test results , we present our testing methodology , which has novel elements that we think can benefit other evaluation projects .",
    "target": "encase;linen;acquisition of digital data;hard drive imaging;testing forensic tools;linux"
  },
  {
    "source": "parallel simulation of devs and cell devs models on windows based pc cluster systems . <eos> the growing popularity of networks of workstations ( now ) in scientific computation has drawn increasing interest from the m s community . this paper addresses the issue of parallel discrete event simulation of devs and cell devs models on a microsoft windows based cluster system comprising interconnected general purpose personal computers . we present the architecture and features of pcd win , a parallel simulator that takes advantage of the multi purpose graphical user interface of the deinompi middleware for construction of ad hoc pc clusters and configuration of simulation environment . this environment significantly reduces the learning curve for general users and the cost of the simulation platform . pcd win has been developed using a modular approach that promotes code reuse and allows for easy switching to other middleware technologies . the portability of the simulator is enhanced with multi platform programming and compilation techniques . moreover , it leaves open the possibility of further extensions such as web based distributed simulation and database based model construction by leveraging the native support of microsoft visual studio . the experiments demonstrate the capability of the new simulator , making it an ideal m s toolkit for tapping the computational power of general purpose desktop computers .",
    "target": "parallel simulation;devs;cell devs;cluster systems;discrete event simulation"
  },
  {
    "source": "a computable version of the daniell stone theorem on integration and linear functionals . <eos> for every measure mu , the integral i f bar right arrow integral f d mu is a linear functional on the set of real measurable functions . by the daniell stone theorem , for every abstract integral lambda f > r on a stone vector lattice f of real functions f omega > r there is a measure mu such that integral f d mu lambda ( f ) for all f is an element of f. in this paper we prove a computable version of this theorem . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "computability;daniell stone theorem;computable analysis;measure theory"
  },
  {
    "source": "factors influencing intention to use e government services among citizens in malaysia . <eos> this study is an exploratory study on the e government in malaysia . with the liberalization and globalization , internet has been used as a medium of transaction in almost all aspects of human living . this study investigates the factors that influencing the intention to use e government service among malaysians . this study integrates constructs from the models of technology acceptance model ( tam ) , diffusion of innovation ( doi ) which been moderated by culture factor and trust model with five dimensions . the study was conducted by surveying a broad diversity of citizens in malaysia community . a structured questionnaire was used to collect data from <digit> respondents but only <digit> of the respondents with complete answers participating in the study . the result of the analysis showed that trust , perceived usefulness , perceived relative advantage and perceived image , respectively , has a direct positive significant relationship towards intention to use e government service and perceived complexity has a significant negative relationship towards intention to use e government service . while perceived strength of online privacy and perceived strength of non repudiation have a positive impact on a citizen 's trust to use e government service . however , the uncertainty avoidance ( moderating factor ) used in the study has no significant effect on the relationship between the innovation factors ( complexity , relative advantage and image ) and intention to use e government service . finally in comparing the explanatory power of the entire intention based model ( tam , doi and trust ) with the studied model , it has been found that the doi model has a better explanatory power .",
    "target": "e government;innovation;adoptions;malaysian services"
  },
  {
    "source": "reducing the energy dissipation of the issue queue by exploiting narrow immediate operands . <eos> in contemporary superscalar microprocessors , issue queue is a considerable energy dissipating component due its complex scheduling logic . in addition to the energy dissipated for scheduling activities , read and write lines of the issue queue entries are also high energy consuming pieces of the issue queue . when these lines are used for reading and writing unnecessary information bits , such as the immediate operand part of an instruction that does not use the immediate field or the insignificant higher order bits of an immediate operand that are in fact not needed , significant amount of energy is wasted . in this paper , we propose two techniques to reduce the energy dissipation of the issue queue by exploiting the immediate operand files of the stored instructions firstly by storing immediate operands in separate immediate operand files rather than storing them inside the issue queue entries and secondly by issue queue partitioning based on widths of immediate operands of instructions . we present our performance results and energy savings using a cycle accurate simulator and testing the design with spec2k benchmarks and <digit> nm cmos ( umc ) technology .",
    "target": "issue queue;immediate operands;encoding;energy consumption;low power"
  },
  {
    "source": "study of speed dependent packet error rate for wireless sensor on rotating mechanical structures . <eos> wireless sensors on rotating mechanical structures have rich and fast changing multipath that can not be easily predicted by conventional regression approaches in time for effective transmission coding or power control , resulting in deteriorated transmission quality . this study aims to study the speed dependent packet error rate ( per ) of wireless sensor radios on rotating mechanical structures . a series of rotating ieee 802.15.4 sensor radio transmission experiments and vector network analyzer measurements have been conducted to derive and validate a predictive per model for a fast rotating sensor radio channel based on channel impulse response measurements . the proposed predictive per model , including power attenuation , bit error rate ( ber ) and per sub models , captures the channel property of rotating sensors based on the received signal strength and the radio receiving sensitivity . the per model has accurately predicted the per profile of sensors on a rotating machine tool spindle as well as a rotating plate of a prototype rotation system . the analysis provides an in depth understanding of how multipath propagation causes the fast power variation and the resulting speed dependent per for wireless sensors on rotating mechanical structures .",
    "target": "packet error rate;wireless sensor;rotating mechanical structure;transmission performance"
  },
  {
    "source": "crosstalk in vlsi interconnections . <eos> we address the problem of crosstalk computation and reduction using circuit and layout techniques in this paper , we provide easily computable expressions for crosstalk amplitude and pulse width in resistive , capacitively coupled lines , the expressions hold for nets with arbitrary number of pins and of arbitrary topology under any specified input excitation . experimental results show that the average error is about <digit> % and the maximum error is less than <digit> % . the expressions are used to motivate circuit techniques , such as transistor sizing , and layout techniques , such as wire ordering and wire width optimization to reduce crosstalk .",
    "target": "coupled noise;signal integrity;timing optimization"
  },
  {
    "source": "a hybrid genetic algorithm for the energy efficient virtual machine placement problem in data centers . <eos> server consolidation using virtualization technology has become an important technology to improve the energy efficiency of data centers . virtual machine placement is the key in the server consolidation technology . in the past few years , many approaches to the virtual machine placement have been proposed . however , existing virtual machine placement approaches consider the energy consumption by physical machines only , but do not consider the energy consumption in communication network , in a data center . however , the energy consumption in the communication network in a data center is not trivial , and therefore should be considered in the virtual machine placement . in our preliminary research , we have proposed a genetic algorithm for a new virtual machine placement problem that considers the energy consumption in both physical machines and the communication network in a data center . aiming at improving the performance and efficiency of the genetic algorithm , this paper presents a hybrid genetic algorithm for the energy efficient virtual machine placement problem . experimental results show that the hybrid genetic algorithm significantly outperforms the original genetic algorithm , and that the hybrid genetic algorithm is scalable .",
    "target": "hybrid genetic algorithm;virtual machine placement;data center;server consolidation;cloud computing"
  },
  {
    "source": "sensitivity of tapered optical fiber surface plasmon resonance sensors . <eos> the effect of tapered profiles on the sensitivity of spr sensor is studied . it is observed that as the taper ratio decreases the sensitivity of proposed sensor for each profile increases up to certain taper ratio where the plasmonic condition is satisfied . in all considered cases , the maximum sensitivity is obtained for sinusoidal tapered profile .",
    "target": "sensitivity;surface plasmon resonance;tapered profiles;taper ratio;sinusoidal tapered profile"
  },
  {
    "source": "an image contrast enhancement method based on genetic algorithm . <eos> contrast enhancement plays a fundamental role in image video processing . histogram equalization ( he ) is one of the most commonly used methods for image contrast enhancement . however , he and most other contrast enhancement methods may produce un natural looking images and the images obtained by these methods are not desirable in applications such as consumer electronic products where brightness preservation is necessary to avoid annoying artifacts . to solve such problems , we proposed an efficient contrast enhancement method based on genetic algorithm in this paper . the proposed method uses a simple and novel chromosome representation together with corresponding operators . experimental results showed that this method makes natural looking images especially when the dynamic range of input image is high . also , it has been shown by simulation results that the proposed genetic method had better results than related ones in terms of contrast and detail enhancement and the resulted images were suitable for consumer electronic products .",
    "target": "contrast enhancement;genetic algorithm;natural looking images"
  },
  {
    "source": "limits of a conjecture on a leakage resilient cryptosystem . <eos> we introduce the hidden shares number problem , a variant of the hidden number problem . we give a leakage resilience bound for elgamal cryptosystem with stateful decryption . we have implemented our attack and give some details about our implementation .",
    "target": "hidden number problem;elgamal;cryptography;leakage resilient cryptography;lattice based attacks"
  },
  {
    "source": "robust reconstruction of low resolution document images by exploiting repetitive character behaviour . <eos> in this paper , we present a new approach for reconstructing low resolution document images . unlike other conventional reconstruction methods , the unknown pixel values are not estimated based on their local surrounding neighbourhood , but on the whole image . in particular , we exploit the multiple occurrence of characters in the scanned document . in order to take advantage of this repetitive behaviour , we divide the image into character segments and match similar character segments to filter relevant information before the reconstruction . a great advantage of our proposed approach over conventional approaches is that we have more information at our disposal , which leads to a better reconstruction of the high resolution ( hr ) image . experimental results confirm the effectiveness of our proposed method , which is expressed in a better optical character recognition ( ocr ) accuracy and visual superiority to other traditional interpolation and restoration methods .",
    "target": "repetition;character segmentation;ocr;interpolation;restoration;bimodal distribution"
  },
  {
    "source": "complexity of inflammatory responses in endothelial cells and vascular smooth muscle cells determined by microarray analysis . <eos> to better understand the molecular basis of vascular cell system behavior in inflammation , we used gene expression microarrays to analyze the expression of 7,075 genes and their response to il <digit> and tnf in cultures of coronary artery endothelium and smooth muscle derived from a single coronary artery . the most noticeable difference between the cell types was the considerably greater magnitude and complexity of the transcriptional response in the endothelial cells . two hundred and nine genes were regulated in the endothelium and only <digit> in vascular smooth muscle . among the <digit> regulated genes in the endothelium , <digit> have not been previously associated with endothelial cell activation and many implicate the endothelium in unconventional roles . for example , the induced genes include several that have only been associated with leukocyte function ( e.g. , il <digit> receptor , ebi <digit> receptor ) and others related to antiviral and antibacterial defense ( e.g. , oligoadenylate synthetase , lmp7 , toll like receptor <digit> , complement component <digit> ) . in addition , <digit> genes likely to participate in signal transduction ( eg . il <digit> receptor , stk2 kinase , staf50 , anp receptor , vip receptor , rac3 , ifp35 ) were regulated providing evidence that a major effect of tnf and il <digit> is to alter the potential of the endothelial cell to respond to various other external stimuli .",
    "target": "inflammation;tnf;interleukin <digit> beta;gene expression profiling;gene expression regulation"
  },
  {
    "source": "partial x ray photoelectron spectroscopy to constructing neural network model of plasma etching surface . <eos> a new model to control plasma processes was constructed by combining a backpropagation neural network ( bpnn ) with x ray photoelectron spectroscopy ( xps ) . this technique was evaluated with the data collected during the etching of silicon carbide films at nf3 inductively coupled plasma . the etching characteristics modeled were the etch rate and surface roughness measured by scanning electron microscope and atomic force microscopy , respectively . for systematic modeling , the etching was characterized by means of <digit> full factorial experiment plus one center point . the bpnn was trained by the training data composed of xps spectra corresponding to five major peaks . prediction performance of trained bpnn model was tested with a test data set , not belonging to the training data . in modeling surface roughness , pure xps model yielded an improvement of about <digit> % over pca xps ( <digit> % data variance ) model . for the etch rate data , the improvement was more than <digit> % irrespective of the data variances . these results indicate that non reduced xps spectra are more effective in constructing a prediction model . xps models can be utilized to diagnose or control plasma processes .",
    "target": "x ray photoelectron spectroscopy;neural network;model;plasma etching;surface roughness;atomic force microscopy"
  },
  {
    "source": "aggregate profit based caching replacement algorithms for streaming media transcoding proxy systems . <eos> this work derives a generalized video object profit function from the extended weighted transcoding graph to calculate the individual cache profit of certain versions of a video object , and the aggregate profit from caching multiple versions of the same video object . this proposed function takes into account the popularity of certain versions of an object , the transcoding delay among versions , and the average duration of access of each version . based on the profit function , cache replacement algorithms are proposed to reduce the startup delay and network traffic by efficiently caching video objects with the most profits . two kinds of simulations were conducted to evaluate the performance of the proposed algorithms . these simulations exploit partial viewing traces and complete viewing traces , separately . the results demonstrate that the proposed algorithms outperform the competing algorithms by <digit> % <digit> % in delay saving ratio and <digit> % <digit> % in byte hit ratio .",
    "target": "computer networks;multimedia communication;multimedia streaming;prefix caching;proxy caching;streaming media distribution"
  },
  {
    "source": "refining and reasoning about nonfunctional requirements . <eos> nonfunctional requirements ( nfr ) must be addressed early in the software development cycle to avoid the cost of revisiting those requirements or re factoring at the later stages of the development cycle . methods and frameworks that identify and incorporate nfr at each stage of development cycle reduce this cost . the methodology used in this work for refining and reasoning about nfr is based on the nfr framework . this work identifies four nfr types and provides the methodology for developing domain specific nfr by using techniques for converting the requirements into design artifacts per nfr type . the contribution is four nfr types functionally restrictive , additive restrictive , policy restrictive , and architecture restrictive and the software engineering process that provides specific refinements that result in unique architectural and design artifacts . by applying the same functional requirement focus to the different nfr domains it enhances the development process and promotes software quality attributes such as composability , maintainability , evolvability , and traceability .",
    "target": "nonfunctional requirements;software engineering;aspect oriented programming"
  },
  {
    "source": "model updated image guidance initial clinical experiences with gravity induced brain deformation . <eos> image guided neurosurgery relies on accurate registration of the patient , the preoperative image series , and the surgical instruments in the same coordinate space . recent clinical reports have documented the magnitude of gravity induced brain deformation in the operating room and suggest these levels of tissue motion may compromise the integrity of such systems , we are investigating a model based strategy which exploits the wealth of readily available preoperative information in conjunction with intraoperatively acquired data to construct and drive a three dimensional ( <digit> d ) computational model which estimates volumetric displacements in order to update the neuronavigational image set . using model calculations , the preoperative image database can be deformed to generate a more accurate representation of the surgical focus during an operation , in this paper , we present a preliminary study of four patients that experienced substantial brain deformation from gravity and correlate cortical shift measurements with model predictions , additionally , me illustrate our image deforming algorithm and demonstrate that preoperative image resolution is maintained . results over the four cases show that the brain shifted , on average , 5.7 mm in the direction of gravity and that model predictions could reduce this misregistration error to an average of 1.2 mm .",
    "target": "image guidance;brain shift;brain deformation model;consolidation;finite element model;porous media"
  },
  {
    "source": "communication structure and collective actions in social media . <eos> in this paper i present results a study of different types of social media communication and networking channels that allow for collective action ( ca ) twitter , jaiku qaiku , ning and facebook . my preliminary findings indicate , that the visual outlining and the structure of communication create different kinds of collectivity and collective actions . a status stream is effective for simple and fast repetitive mass actions and for individual mass broadcasting , while channels and threads are needed as a backchannel for more complicated , coordinated and iterative tasks and support a sense of community . when planning collective action on the internet , ranging from citizen participation to marketing campaigns , it is essential to note that different social media tools support different forms of collective action and feelings of collectiveness .",
    "target": "collective action;social media;backchannel;crowdsourcing;collective intelligence;real time web;micro channel"
  },
  {
    "source": "better reporting of randomized trials in biomedical journal and conference abstracts . <eos> well reported research published in conference and journal abstracts is important as individuals reading these reports often base their initial assessment of a study based on information reported in the abstract . however , there is growing concern about the reliability and quality of information published in these reports . this article provides an overview of research evidence underpinning the need for better reporting of abstracts reported in conference proceedings and abstracts of journal articles with a particular focus in the area of health care . where available we highlight evidence which refers specifically to abstracts reporting randomized trials . we seek to identify current initiatives aimed at improving the reporting of these reports and recommend that an extension of the consort statement ( consolidated standards of reporting trials ) , consort for abstracts , be developed . this checklist would include a list of essential items to be reported in any conference or journal abstract reporting the results of a randomized trial .",
    "target": "conference proceedings;checklists;randomized controlled trial;methodological quality;structured abstracts"
  },
  {
    "source": "continuous testing in eclipse . <eos> continuous testing uses excess cycles on a developer 's workstation to continuously run regression tests in the background , providing rapid feedback about test failures as code is edited . it reduces the time and energy required to keep code well tested , and it prevents regression errors from persisting uncaught for long periods of time .",
    "target": "continuous testing;testing;development environments"
  },
  {
    "source": "bayesian analysis of the logit model and comparison of two metropolishastings strategies . <eos> we examine some markov chain monte carlo ( mcmc ) methods for a generalized non linear regression model , the logit model . it is first shown that mcmc algorithms may be used since the posterior is proper under the choice of non informative priors . then two non standard mcmc methods are compared a metropolishastings algorithm with a bivariate normal proposal resulting from an approximation , and a metropolishastings algorithm with an adaptive proposal . the results presented here are illustrated by simulations , and show the good behavior of both methods , and superior performances of the method with an adaptive proposal in terms of convergence to the stationary distribution and exploration of the posterior distribution surface .",
    "target": "markov chain monte carlo;metropolishastings algorithm;bayesian statistic;adaptive algorithm;stationarity;convergence assessment"
  },
  {
    "source": "aggregate features and adaboost for music classification . <eos> we present an algorithm that predicts musical genre and artist from an audio waveform . our method uses the ensemble learner adaboost to select from a set of audio features that have been extracted from segmented audio and then aggregated . our classifier proved to be the most effective method for genre classification at the recent mirex <digit> international contests in music information extraction , and the second best method for recognizing artists . this paper describes our method in detail , from feature extraction to song classification , and presents an evaluation of our method on three genre databases and two artist recognition databases . furthermore , we present evidence collected from a variety of popular features and classifiers that the technique of classifying features aggregated over segments of audio is better than classifying either entire songs or individual short timescale features .",
    "target": "genre classification;mirex;artist recognition;audio feature aggregation;multiclass adaboost"
  },
  {
    "source": "weak limits and their calculation in analog signal theory . <eos> purpose this paper aims to improve the mathematical justification of certain analog signal theory concepts and offer a rigorous framework for it . design methodology approach the framework relies on functional analysis , namely theory of distributions and the concept of weak limit . its notation is adjusted to resemble the notation usually used in engineering signal theory . it can be used to prove in a rigorous manner already established results in signal theory , but also to establish new ones . findings examples have shown the lack of rigour caused by using ordinary calculus in proving fundamental signal theoretic results . on that basis , concepts of limit , fourier transform and derivative are revisited in the spirit of functional analysis . a new useful formula for weak limit computation is proved . originality value functional analysis is efficiently used in signal theory in a manner approachable by engineers . an original and efficient formula for weak limit computation is presented and proved .",
    "target": "weak limit;analog signal theory;functional analysis;fourier transforms;schwartz distribution"
  },
  {
    "source": "the social information infrastructure . <eos> the division of social , behavioral , and economic research of the national science foundation has explored aggressively the potential involvement of the social sciences in the national information infrastructure . we invision the nii as a global network of computer communications , which will evolve out of the internet , linking all social scientists to massive digital libraries and to myriad smaller distributed data sources containing information of every imaginable sort . five workshops have charted applications of high performance computing in the social and behavioral sciences cognitive science , computational geography computational economics , artificial social intelligence , and electronic networks . a survey of seer programs revealed that many are helping to create the information infrastructure , and substantial investment in six '' flagship '' digital library projects will develop the systems necessary for the nii of the 21st century .",
    "target": "information infrastructure;internet;digital library;cognitive science;computational geography;computational economics;artificial intelligence"
  },
  {
    "source": "web based public participation geographical information systems an aid to local environmental decision making . <eos> current research examining the potential of the world wide web as a means of increasing public participation in local environmental decision making in the uk is discussed . the paper considers traditional methods of public participation and argues that new internet based technologies have the potential to widen participation in the uk planning system . evidence is provided of the potential and actual benefits of online spatial decision support systems in the uk through a real environmental decision support problem in a village in northern england . the paper identifies key themes developing in this area of web based geographical information systems ( gis ) and provides a case study example of an online public participation gis from inception to the final phase in a public participation process . it is shown that in certain uk planning problems and policy formulation processes , participatory online systems are a useful means of informing and engaging the public and can potentially bring the public closer to a participatory planning system .",
    "target": "web;public participation;gis;planning for real"
  },
  {
    "source": "medical image analysis for cancer management in natural computing framework . <eos> natural computing , through its repertoire of nature inspired strategies , is playing a major role in the development of intelligent decision making systems . the objective is to provide flexible , application oriented solutions to current medical image analysis problems . it encompasses fuzzy sets , neural networks , genetic algorithms , rough sets , swarm intelligence , and a host of other paradigms , mimicking biological and physical processes from nature . radiographic imaging modalities , like computed tomography ( ct ) , positron emission tomography ( pet ) , and magnetic resonance imaging ( mri ) , help in providing improved diagnosis , prognosis and treatment planning for cancer . this survey highlights the role of natural computing , in efficiently analyzing radiographic medical images , for improved tumor management . we also provide a categorization of the segmentation , feature extraction and selection methods , based on different natural computing technologies , with reference to the application involving malignancy of the brain , breast , prostate , skin , lung , and liver .",
    "target": "segmentation;quantitative imaging;feature selection;radiomics;evolutionary algorithms;biomedical imaging"
  },
  {
    "source": "parallelisation of the lagrangian model in a mixed eulerian lagrangian cfd algorithm . <eos> this manuscript presents an algorithm implemented in a commercial computational fluid dynamics ( cfd ) code for parallelisation of the lagrangian particle tracking model in a mixed eulerian lagrangian cfd algorithm . the algorithm is based on the domain decomposition parallelisation strategy and asynchronous message passing protocol . the methodology is tested on two industrial cfd test cases and the parallelisation results are presented . further , it is discussed how the parallel efficiency of the runs can be improved by adopting the domain decomposition scattering technique . ( c ) <digit> elsevier inc. all rights reserved .",
    "target": "parallel eulerian lagrangian cfd;parallel particle tracking;parallelisation of the spray model"
  },
  {
    "source": "sequential and parallel triangulating algorithms for elimination game and new insights on minimum degree . <eos> elimination game is a well known algorithm that simulates gaussian elimination of matrices on graphs , and it computes a triangulation of the input graph . the number of fill edges in the computed triangulation is highly dependent on the order in which elimination game processes the vertices , and in general the produced triangulations are neither minimum nor minimal . in order to obtain a triangulation which is close to minimum , the minimum degree heuristic is widely used in practice , but until now little was known on the theoretical mechanisms involved . in this paper we show some interesting properties of elimination came in particular that it is able to compute a partial minimal triangulation of the input graph regardless of the order in which the vertices are processed . this results in a new algorithm to compute minimal triangulations that are sandwiched between the input graph and the triangulation resulting from elimination came . one of the strengths of the new approach is that it is easily parallelizable , and thus we are able to present the first parallel algorithm to compute such sandwiched minimal triangulations . in addition , the insight that we gain through elimination game is used to partly explain the good behavior of the minimum degree algorithm . we also give a new algorithm for producing minimal triangulations that is able to use the minimum degree idea to a wider extent . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "minimum degree;minimal triangulation;chordal graphs;parallel and sequential algorithms"
  },
  {
    "source": "quality evaluation of e government digital services . <eos> in this paper we present a quality estimation model for digital e government services suitable for quality evaluation , monitoring , discovery , selection and composition .",
    "target": "e government;quality model;quality of service"
  },
  {
    "source": "throughput improvement of incremental redundancy ldpc coded mimo v blast system . <eos> in this paper , we present ensembles of incremental redundancy low density parity check ( ir ldpc ) codes to improve the throughput performance of hybrid forward error correction ( fec ) automatic repeat request ( arq ) schemes in a vertical bell lab layered space time ( v blast ) system . these ensembles are designed to have good error rate performance at short block lengths , which result in higher throughput performance . the throughput simulations in various fading conditions show that these ensembles outperform a conventional random punctured ensemble by <digit> db eb n0 at a throughput region of 0.8 . to reduce the traffic of feedback channels , we consider using an adaptive code selection algorithm . in these adaptive hybrid fec arq schemes , the number of negative acknowledgement signals for retransmission is greatly reduced at operating snr ranges without any significant throughput loss .",
    "target": "ldpc codes;hybrid fec arq scheme;mimo system"
  },
  {
    "source": "benefits of averaging lateration estimates obtained using overlapped subgroups of sensor data . <eos> in this paper , we suggest averaging lateration estimates obtained using overlapped subgroups of distance measurements as opposed to obtaining a single lateration estimate from all of the measurements directly if a redundant number of measurements are available . least squares based closed form equations are used in the lateration . in the case of gaussian measurement noise the performances are similar in general and for some subgroup sizes marginal gains are attained . averaging laterations method becomes especially beneficial if the lateration estimates are classified as useful or not in the presence of outlier measurements whose distributions are modeled by a mixture of gaussians ( mog ) pdf . a new modified trimmed mean robust averager helps to regain the performance loss caused by the outliers . if the measurement noise is gaussian , large subgroup sizes are preferable . on the contrary , in robust averaging small subgroup sizes are more effective for eliminating measurements highly contaminated with mog noise . the effect of high variance noise was almost totally eliminated when robust averaging of estimates is applied to qr decomposition based location estimator . the performance of this estimator is just <digit> cm worse in root mean square error compared to the cramrrao lower bound ( crlb ) on the variance both for gaussian and mog noise cases . theoretical crlbs in the case of mog noise are derived both for time of arrival and time difference of arrival measurement data .",
    "target": "averaging;lateration;robust averaging;least squares time of arrival location estimator;least squares time difference of arrival location estimator;robust estimator"
  },
  {
    "source": "em based iterative receiver for coded mimo systems in unknown spatially correlated noise . <eos> we present iterative channel estimation and decoding schemes for multi input multi output ( mimo ) rayleigh block fading channels in spatially correlated noise . an expectation maximization ( em ) algorithm is utilized to find the maximum likelihood ( ml ) estimates of the channel and spatial noise covariance matrices , and to compute soft information of coded symbols which is sent to an error control decoder . the extrinsic information produced by the decoder is then used to refine channel estimation . several iterations are performed between the above channel estimation and decoding steps . we derive modified cramer rao bound ( mcrb ) for the unknown channel and noise parameters , and show that the proposed em based channel estimation scheme achieves the mcrb at medium and high snrs . for a bit error rate of <digit> ( <digit> ) and long frame length , there is negligible performance difference between the proposed scheme and the ideal coherent detector that utilizes the true channel and noise covariance matrices . copyright ( c ) <digit> john wiley sons , ltd .",
    "target": "mimo;channel estimation;expectation maximization;turbo code;crb;ber"
  },
  {
    "source": "parameter exploration in science and engineering using many task computing . <eos> robust scientific methods require the exploration of the parameter space of a system ( some of which can be run in parallel on distributed resources ) , and may involve complete state space exploration , experimental design , or numerical optimization techniques . many task computing ( mtc ) provides a framework for performing robust design , because it supports the execution of a large number of otherwise independent processes . further , scientific workflow engines facilitate the specification and execution of complex software pipelines , such as those found in real science and engineering design problems . however , most existing workflow engines do not support a wide range of experimentation techniques , nor do they support a large number of independent tasks . in this paper , we discuss nimrod k a set of add in components and a new run time machine for a general workflow engine , kepler . nimrod k provides an execution architecture based on the tagged dataflow concepts , developed in 1980s for highly parallel machines . this is embodied in a new kepler director that supports many task computing by orchestrating execution of tasks on on clusters , grids , and clouds . further , nimrod k provides a set of actors that facilitate the various modes of parameter exploration discussed above . we demonstrate the power of nimrod k to solve real problems in cardiac science .",
    "target": "parameter exploration;many task computing;scientific workflows;kepler"
  },
  {
    "source": "group context based adaptations for recommendation . <eos> in groupware or community based applications the user interface is usually static or tailored to the individual user 's needs . newer developments try to adapt the user interface automatically in regard to user contexts . even though these techniques are proven useful , there exists no contextadaptive system taking the current context of a group or community in regard . in this paper , we briefly discuss the problems of defining context and present our understanding of context as a subset of the current information state . we provide an exemplary scenario to present different approaches how to compute group contexts based on semantic models and user contexts , and the consequences for the adaptation goals in the interface or through changes at system functionalities or tools . we additionally discuss the problems occurring at evaluating adaptations and the value of group context for collaborative work .",
    "target": "group context;group context based adaptation;context;user context;semantic models;content recommendation"
  },
  {
    "source": "issues of trust and control on agent autonomy . <eos> the relationship between trust and control is quite relevant both for the very notion of trust and for modelling and implementing trust control relations with autonomous systems . we claim that control is antagonistic of the strict form of trust ' trust in y ' but also that it completes and complements it for arriving to a global trust . in other words , putting control and guaranties is trust building it produces a sufficient trust , when trust in y 's autonomous willingness and competence would not be enough . we also argue that control requires new forms of trust trust in the control itself or in the controller , trust in y as for being monitored and controlled , trust in possible authorities , etc. finally , we show that paradoxically control could not be antagonistic of strict trust in y , but it can even create , increase it by making y more willing or more effective . in conclusion , depending on the circumstances , control makes y more reliable or less reliable control can either decrease or increase trust . a good theory of trust can not be complete without a theory of control .",
    "target": "trust;control;autonomous agents;cooperation;adjustable autonomy"
  },
  {
    "source": "youubi open software for ubiquitous learning . <eos> we propose a reference architecture for u learning environments . we propose a method development and validation of u learning environments . we present a u learning environment that combines playful aspects with learning strategies .",
    "target": "ubiquitous learning;ubiquitous computing;software engineering;design interactive;gamification"
  },
  {
    "source": "single symbol ml decodable distributed stbcs for cooperative networks . <eos> in this correspondence , the distributed orthogonal space time block codes ( dostbcs ) , which achieve the single symbol maximum likelihood ( ml ) decodability and full diversity order , are first considered . however , systematic construction of the dostbcs is very hard , since the noise covariance matrix is not diagonal in general . thus , some special dostbcs , which have diagonal noise covariance matrices at the destination terminal , are investigated . these codes are referred to as the row monomial dostbcs . an upper bound of the data rate of the row monomial dostbc is derived and it is approximately twice higher than that of the repetition based cooperative strategy . furthermore , systematic construction methods of the row monomial dostbcs achieving the upper bound of the data rate are developed when the number of relays and or the number of information bearing symbols are even .",
    "target": "cooperative networks;diversity;distributed space time block codes;single symbol maximum likelihood decoding"
  },
  {
    "source": "ontology based data mining approach implemented for sport marketing . <eos> since sport marketing is a commercial activity , precise customer and marketing segmentation must be investigated frequently and it would help to know the sport market after a specific customer profile , segmentation , or pattern come with marketing activities has found . such knowledge would not only help sport firms , but would also contribute to the broader field of sport customer behavior and marketing . this paper proposes using the apriori algorithm of association rules , and clustering analysis based on an ontology based data mining approach , for mining customer knowledge from the database . knowledge extracted from data mining results is illustrated as knowledge patterns , rules , and maps in order to propose suggestions and solutions to the case firm , taiwan adidas , for possible product promotion and sport marketing .",
    "target": "ontology;data mining;sport marketing;apriori algorithm;clustering analysis;endorser;media"
  },
  {
    "source": "layered acting for character animation . <eos> we introduce an acting based animation system for creating and editing character animation at interactive speeds . our system requires minimal training , typically under an hour , and is well suited for rapidly prototyping and creating expressive motion . a real time motion capture framework records the user 's motions for simultaneous analysis and playback on a large screen . the animator 's real world , expressive motions are mapped into the character 's virtual world . visual feedback maintains a tight coupling between the animator and character . complex motion is created by layering multiple passes of acting . we also introduce a novel motion editing technique , which derives implicit relationships between the animator and character . the animator mimics some aspect of the character motion , and the system infers the association between features of the animator 's motion and those of the character . the animator modifies the mimic by acting again , and the system maps the changes onto the character . we demonstrate our system with several examples and present the results from informal user studies with expert and novice animators .",
    "target": "layer;character animation;character;animation;systems;edit;interaction;minimal;training;prototype;express;motion;real time;motion capture;framework;records;user;analysis;maps;virtual world;visualization;feedback;couples;complexity;motion editing;relationships;aspect;association;feature;change;demonstrate;examples;informal;user studies;novice;statistical analysis;3d user interfaces;motion transformation"
  },
  {
    "source": "an extensible architecture based framework for coordination languages . <eos> the dynamic and heterogeneous nature of distributed systems makes the development of distributed applications a difficult task . various tools , such as middleware systems , component systems , and coordination languages , offer support the application developer at different levels . there are several coordination systems that integrate such tools into a complete environment to build applications from heterogeneous components . to achieve extensibility they usually have a layered architecture an application is first mapped to a middle layer and then to a target system . but this approach hides the specific features of a target system from the developer , as they are not represented in the middle layer , and often induces additional run time overhead . in this paper , we introduce the extensible coordination framework ecf that allows developers to build efficient distributed applications which exploit the specific features of the target systems . support for target systems and application domains are encapsulated by extension modules . modules can be built on top of other modules to support refined functionality .",
    "target": "coordination language;distributed systems;component technology;developer framework;software architectures"
  },
  {
    "source": "novel approaches to the measurement of arterial blood flow from dynamic digital x ray images . <eos> we have developed two new algorithms for the measurement of blood flow from dynamic x ray angiographic images . both algorithms aim to improve on existing techniques . first , a model based ( mb ) algorithm is used to constrain the concentration distance curve matching approach . second , a weighted optical flow algorithm ( op ) is used to improve on point based optical flow methods by averaging velocity estimates along a vessel with weighting based on the magnitude of the spatial derivative . the op algorithm was validated using a computer simulation of pulsatile blood flow . both the op and the mb algorithms were validated using a physiological blood flow circuit . dynamic biplane digital x ray images were acquired following injection of iodine contrast medium into a variety of simulated arterial vessels . the image data were analyzed using our integrated angiographic analysis software sara to give blood how waveforms using the nib and op algorithms . these waveforms were compared to flow measured using an electromagnetic flow meter ( emf ) . in total <digit> instantaneous measurements of flow were made and compared to the emf recordings . it was found that the new algorithms showed low measurement bias and narrow limits of agreement and also outperformed the concentration distance curve matching algorithm ( org ) and a modification of this algorithm ( pa ) in all studies .",
    "target": "blood flow measurement;x ray angiography;x ray measurements"
  },
  {
    "source": "his monitor an approach to assess the quality of information processing in hospitals . <eos> hospital information systems ( his ) are a substantial quality and cost factor for hospitals . systematic monitoring of his quality is an important task however , this task is often seen to be insufficiently supported . to support systematic his monitoring , we developed his monitor , comprising about <digit> questions , focusing on how a hospital information system does efficiently support clinical and administrative tasks . the structure of his monitor consists of a matrix , crossing his quality criteria on one axis with a list of process steps within patient care on the other axis . his monitor was developed based on several pretests and was now tested in a larger feasibility study with <digit> participants . his monitor intends to describe strengths and weaknesses of information processing in a hospital . results of the feasibility study show that his monitor was able to highlight certain his problems such as insufficiently supported cross departmental communication , legibility of drug orders and other paper based documents , and overall time needed for documentation . we discuss feasibility of his monitor and the reliability and validity of the results . further refinement and more formal validation of his monitor are planned .",
    "target": "quality;hospital information systems;healthcare information systems;information management;evaluation;questionnaire"
  },
  {
    "source": "recursive channel estimation based on finite parameter model using reduced complexity maximum likelihood equalizer for ofdm over doubly selective channels . <eos> to take intercarrier interference ( ici ) attributed to time variations of the channel into consideration , the time and frequency selective ( doubly selective ) channel is parameterized by a finite parameter model . by capitalizing on the finite parameter model to approximate the doubly selective channel , a kalman filter is developed for channel estimation . the ici suppressing , reduced complexity viterbi type maximum likelihood ( rml ) equalizer is incorporated into the kalman filter for recursive channel tracking and equalization to improve the system performance . an enhancement in the channel tracking ability is validated by theoretical analysis , and a significant improvement in ber performance using the channel estimates obtained by the recursive channel estimation method is verified by monte carlo simulations .",
    "target": "polynomial model;oversampled basis expansion model;recursive kalman;reduced complexity ml equalizer"
  },
  {
    "source": "a web service agent based decision support system for securities exception management . <eos> with rising trading volumes and increasing risks in securities transactions , the securities industry is making an effort to shorten the trade lifecycle and minimize transaction risks . while attempting to achieve this , exception management is crucial to pass trade information within the trade lifecycle in a timely and accurate fashion . for a competitive solution to exception management , a web service agent based decision support system is developed in this paper . agent technology is applied to deal with the dynamic , complex , and distributed processes in exception management web services techniques are proposed for more scalability and interoperability in network based business environment . by integrating agent technology with web services to make use of the advantages from both , this approach leads to more intelligence , flexibility and collaboration in business exception management . the effectiveness of this approach is evaluated through a use case and demonstration feedback .",
    "target": "web services;decision support systems;exception management;intelligent agents;securities trading"
  },
  {
    "source": "measuring energy consumption using eml ( energy measurement library ) . <eos> energy consumption and efficiency is a main issue in high performance computing systems in order to reach exascale computing . researchers in the field are focusing their effort in reducing the first and increasing the latter while there is no current standard for energy measurement . current energy measurement tools are specific and architectural dependent and this has to be addressed . by creating a standard tool , it is possible to generate independence between the experiments and the hardware , and thus , researchers effort can be focused in energy , by maximizing the portability of the code used for experimentation with the multiple architectures we have access nowadays . we present the energy measurement library ( eml ) library , a software library that eases the access to the energy measurement tools and can be easily extended to add new measurement systems . using eml , it is viable to obtain architectural and algorithmic parameters that affect energy consumption and efficiency . the use of this library is tested in the field of the analytic modeling of the energy consumed by parallel programs .",
    "target": "eml;energy measurement library;energy aware computing;energy efficiency"
  },
  {
    "source": "methylation sensitive representational difference analysis and its application to cancer research . <eos> methylation sensitive representational difference analysis ( ms rda ) was previously established to detect differences in the methylation status of two genomes . this method uses the digestion of genomic dna with a methylation sensitive restriction enzyme , hpaii , and pcr to prepare hpaii amplicons , followed by rda . an hpaii amplicon prepared using betaine and reverse electrophoresis was enriched 3.6 fold ( compared with the hpaii amplicon prepared by the original method ) with dna fragments originating from cpg islands ( cgis ) . as for the specificity of ms rda , it was shown that dna fragments that are unmethylated in the tester and almost completely methylated in the driver are efficiently isolated . this indicated that genes that are in biallelic methylation or in monoallelic methylation with loss of the other allele are efficiently isolated . further , by use of two additional methylation sensitive six base recognition restriction enzymes , sacii and nari , more dna fragments were isolated from cgis in the <digit> regions of genes . after analysis of human lung , gastric , and breast cancers , <digit> genes were seen to be silenced and additional genes seen to show decreased expression in association with methylation of genomic regions outside cgis in the <digit> regions of genes . ms rda is effective in identifying silenced genes in various cancers .",
    "target": "cancer;cpg island;dna methylation;gene silencing;genome scanning"
  },
  {
    "source": "simple ptass for families of graphs excluding a minor . <eos> we show that very simple algorithms based on local search are polynomial time approximation schemes for maximum independent set , minimum vertex cover and minimum dominating set , when the input graphs have a fixed forbidden minor .",
    "target": "polynomial time approximation scheme;maximum independent set;minimum vertex cover;minimum dominating set;forbidden minor;separator;local optimization"
  },
  {
    "source": "a modification of the index of liou and wang for ranking fuzzy number . <eos> different methods have been proposed for ranking fuzzy numbers . these include methods based on distances , centroid point , coefficient of variation , and weighted mean value . however , there is still no method that can always give a satisfactory result to every situation some are counterintuitive and not discriminating . this paper presents an approach for ranking fuzzy numbers with integral value that is an extension of the index of liou and wang . this method , that is independent of the type of membership function used , can rank more than two fuzzy numbers simultaneously . this ranking method use an index of optimism to reflect the decision maker 's optimistic attitude , but rather it also contains an index of modality that represents the neutrality of the decision maker . the approach is illustrated with numerical examples .",
    "target": "ranking fuzzy numbers;integral value;index of optimism;index of modality"
  },
  {
    "source": "a mobility based load control scheme in hierarchical mobile ipv6 networks . <eos> by introducing a mobility anchor point ( map ) , hierarchical mobile ipv6 ( hmipv6 ) reduces the signaling overhead and handoff latency associated with mobile ipv6 . in this paper , we propose a mobility based load control ( mlc ) scheme , which mitigates the burden of the map in fully distributed and adaptive manners . the mlc scheme combines two algorithms a threshold based admission control algorithm and a session to mobility ratio ( smr ) based replacement algorithm . the threshold based admission control algorithm gives higher priority to ongoing mobile nodes ( mns ) than new mns , by blocking new mns when the number of mns being serviced by the map is greater than a predetermined threshold . on the other hand , the smr based replacement algorithm achieves efficient map load distribution by considering mns traffic and mobility patterns . we analyze the mlc scheme using the continuous time markov chain in terms of the new mn blocking probability , ongoing mn dropping probability , and binding update cost . also , the map processing latency is evaluated based on the m g <digit> queueing model . analytical and simulation results demonstrate that the mlc scheme outperforms other schemes and thus it is a viable solution for scalable hmipv6 networks .",
    "target": "mobility based load control;hierarchical mobile ipv6;mobility anchor point;admission control algorithm;session to mobility ratio"
  },
  {
    "source": "randomized diffusion for indivisible loads . <eos> presentation of new algorithm for balancing discrete loads . algorithm is very natural and avoids nodes having negative loads . quality measure is the gap between maximum and minimum load , called discrepancy . upper bounds on discrepancy after algorithm has run as long as its continuous counterpart .",
    "target": "diffusion;distributed computing;load balancing;randomized algorithms;random walk"
  },
  {
    "source": "automatic image segmentation by dynamic region merging . <eos> this paper addresses the automatic image segmentation problem in a region merging style . with an initially oversegmented image , in which many regions ( or superpixels ) with homogeneous color are detected , an image segmentation is performed by iteratively merging the regions according to a statistical test . there are two essential issues in a region merging algorithm order of merging and the stopping criterion . in the proposed algorithm , these two issues are solved by a novel predicate , which is defined by the sequential probability ratio test and the minimal cost criterion . starting from an oversegmented image , neighboring regions are progressively merged if there is an evidence for merging according to this predicate . we show that the merging order follows the principle of dynamic programming . this formulates the image segmentation as an inference problem , where the final segmentation is established based on the observed image . we also prove that the produced segmentation satisfies certain global properties . in addition , a faster algorithm is developed to accelerate the region merging process , which maintains a nearest neighbor graph in each iteration . experiments on real natural images are conducted to demonstrate the performance of the proposed dynamic region merging algorithm .",
    "target": "image segmentation;region merging;dynamic programming;wald 's sequential probability ratio test"
  },
  {
    "source": "boundary properties of the inconsistency of pairwise comparisons in group decisions . <eos> we study the inconsistency of pairwise comparisons in group decision making . we study the effect of the aggregation of pairwise comparisons on their consistency . we derive general results and provide a complete study for well known inconsistency indices . we start a discussion on the meaning of inconsistency of aggregated preferences .",
    "target": "boundary properties;group decision making;inconsistency indices;pairwise comparison matrix;analytic hierarchy process"
  },
  {
    "source": "sensitivity of optimal prices to system parameters in a steady state service facility . <eos> we consider the problem of maximizing the long run average reward in a service facility with dynamic pricing . we investigate sensitivity of optimal pricing policies to the parameters of the service facility which is modelled as an m m s k m m s k queueing system . arrival process to the facility is poisson with arrival rate a decreasing function of the price currently being charged by the facility . we prove structural results on the optimal pricing policies when the parameters in the facility change . namely , we show that optimal prices decrease when the capacity of the facility or the number of servers in the facility increase . under a reasonable assumption , we also show that optimal prices increase as the overall demand for the service provided by the facility increases or when the service rate of the facility decreases . we illustrate how these structural results simplify the required computational effort while finding the optimal policy .",
    "target": "pricing;queueing;stochastic programming;markov decision processes;robustness and sensitivity analysis"
  },
  {
    "source": "a self organizing p2p system with multi dimensional structure . <eos> this paper presents and analyzes self can , a self organizing p2p system that , while relying on the multi dimensional structured organization of peers provided by can , exploits the operations of ant based mobile agents to sort the resource keys and distribute them to peers . the benefits of the self organization approach are remarkable , starting from increased flexibility and robustness , to better load balancing characteristics . most notably , peer indexes and resource keys can be defined on different and independent spaces , which overcomes the main limitation of standard structured p2p systems , i.e. , the necessity of assigning each key to a peer having a specified index . this decoupling opens the possibility of giving a semantic meaning to resource keys and enables the efficient execution of multi dimensional range queries , which are essential in some types of distributed systems , for example in grids .",
    "target": "self organizing;bio inspired;peer to peer;key value storage;resource discovery"
  },
  {
    "source": "self organized traffic control . <eos> in this paper we propose and present preliminary results on the migration of traffic lights as roadside based infrastructures to in vehicle virtual signs supported only by vehicle to vehicle communications . we design a virtual traffic light protocol that can dynamically optimize the flow of traffic in road intersections without requiring any roadside infrastructure . elected vehicles act as temporary road junction infrastructures and broadcast traffic light messages that are shown to drivers through in vehicle displays . this approach renders signalized control of intersections truly ubiquitous , which significantly increases the overall traffic flow . we pro vide compelling evidence that our proposal is a scalable and cost effective solution to urban traffic control .",
    "target": "self organized traffic;traffic lights;v2v communication"
  },
  {
    "source": "search strategies on a new health information retrieval system . <eos> purpose the goals of this study are to evaluate the merits of a newly developed health information retrieval system to investigate users ' search strategies when using the new search system and to study the relationships between users ' search strategies and their prior topic knowledge . design methodology approach the paper developed a new health information retrieval system called meshmed . a term browser and a tree browser are included in the new system in addition to the traditional search box . the term browser allows a user to search medical subject heading ( mesh ) terms using natural language . the tree browser presents a hierarchical tree structure of related mesh terms . a user study with <digit> participants was conducted to evaluate the benefits of meshmed . findings the paper found that meshmed provides a user with more choices to select an appropriate searching component and form more effective search strategies . based on the time a participant spent using different meshmed components , the paper identified three different search styles the traditional style , the novel style , and the balanced style , which falls in between . meshmed was particularly helpful for users with low topic knowledge . originality value a new health information retrieval system ( meshmed ) was designed and developed ( and is currently available at http 129.89.43.129 meshmed ) . this is the first study to explore users ' search strategies on such a system . the study results can inform the design of future clinical oriented health information retrieval systems .",
    "target": "information retrieval;knowledge management;hospitals;information systems"
  },
  {
    "source": "anfis approach to the scour depth prediction at a bridge abutment . <eos> an accurate estimation of the maximum possible scour depth at bridge abutments is of paramount importance in decision making for the safe abutment foundation depth and also for the degree of scour counter measure to be implemented against excessive scouring . despite analysis of innumerable prototype and hydraulic model studies in the past , the scour depth prediction at the bridge abutments has remained inconclusive . this paper presents an alternative to the conventional regression model ( rm ) in the form of an adaptive network based fuzzy inference system ( anfis ) modelling . the performance of anfis over rm and artificial neural networks ( anns ) is assessed here . it was found that the anfis model performed best among of these methods . the causative variables in raw form result in a more accurate prediction of the scour depth than that of their grouped form .",
    "target": "bridge abutments;neural network;anfis and regression analysis;local scour"
  },
  {
    "source": "an improved strength pareto evolutionary algorithm <digit> with application to the optimization of distributed generations . <eos> this paper presents an improved strength pareto evolutionary algorithm <digit> ( ispea2 ) , which introduces a penalty factor in objective function constraints , uses adaptive crossover and a mutation operator in the evolutionary process , and combines simulated annealing iterative process over spea2 . the testing result of ispea2 by authoritative testing functions meets the requirement of petro optimum fronts . the case study result shows that the proposed algorithm provides a rapid convergence in obtaining pareto optimal solutions during the calculation process of evolution . based on the fuzzy set theory , ispea2 is able to solve the multi objective problems in the ieee <digit> bus system , and its validity and practicality are demonstrated by the utilization on dgs economic dispatch and optimal operation in the field of power industry .",
    "target": "improved strength pareto evolutionary algorithm;distributed generation;simulated annealing;distribution power system;coordinative optimization"
  },
  {
    "source": "a distributed instrument for performance analysis of real time ethernet networks . <eos> ethernet technology is widely used in real time industrial automation . thanks to real time ethernet ( rte ) protocols , defined in iec61784 <digit> standard , new top performance automation solutions can be created . such systems may have communication cycle time down to tens of mu s and cycle jitter less than <digit> mu s , making network testing and debugging very critical . existing network and protocol analyzers can perform detailed local analysis , but characterization of high performance rte systems requires measurement of transmission delays and these instruments can not be adequately synchronized among them to realize a distributed measurement network . this paper introduces a new low cost distributed measurement instrument to measure timing characteristics of rte nodes ( end to end delays , synchronization , etc. ) . the proposed instrument has multiple fpga based probes that allow for simultaneous synchronized logging on different place of the target rte network . a pc based monitor station stores all the data , ready for further elaboration . architecture details are discussed , a prototype has been realized , and some experimental results are presented . for instance , synchronization accuracy between probes is below <digit> ns .",
    "target": "performance analysis;real time;ethernet network;fieldbus;network synchronization"
  },
  {
    "source": "documentation standards for beginning students . <eos> the importance of writing programs that are readable has finally gained preeminence in the struggle with such competing and contradictory goals as cuteness and optimization of code . as a result , a much greater stress on documentation standards is found in computer science education these days . industry and government standards for documentation are being more widely adhered to and certain points of agreement have emerged . some excellent books have been written that cover the subject ( van tassel , <digit> ledgard , <digit> kernighan plauger , <digit> ) however it is safe to say that both the exhaustive treatment of the subject in such publications and the extremely high standards proposed probably preclude wholesale adoption by instructors of beginning level programming courses . what is proposed here is a set of common sense , scaled down documentation standards for the student in a first programming course in , say , fortran , pl i , algol , or basic . the following represents an amalgam of documentation requirements achieved as a result of teaching introductory programming to college students for nine years . the actual sources have been the literature , colleagues , and last but not least , experience . they are not intended to represent an only or best approach the author has recently encountered other efforts in this direction that must surely be as reasonable and effective . it does represent one educator 's approach it is sufficiently scaled down so that one might reasonably expect to use it as a standard for beginning students and it may be most useful as a contributor of components to be integrated into a more effective set of standards . the basics of documentation and readable programming include comments , meaningful variable names , labelled output , flowcharts , and clear program flow . the major components of and basic rules for each of these categories will be presented in the context of the needs and limitations of the beginning student .",
    "target": "documentation;standardization;student;writing;program;goals;code;stress;computer science education;industrial;governance;point;public;adopt;requirements;teaching;introductory programming;experience;author;direct;effect;use;component;integrability;variability;flow;rules;context"
  },
  {
    "source": "differential space time modulation for ds cdma systems . <eos> differential space time modulation ( dstm ) schemes were recently proposed to fully exploit the transmit and receive antenna diversities without the need for channel state information . dstm is attractive in fast flat fading channels since accurate channel estimation is difficult to achieve . in this paper . we propose a new modulation scheme to improve the performance of ds cdma systems in fast time dispersive fading channels . this scheme is referred to as the differential space time modulation for ds cdma ( dst cdma ) systems . the new modulation and demodulation schemes are especially studied for the fast fading down link transmission in ds cdma systems employing multiple transmit antennas and one receive antenna . we present three demodulation schemes . referred to as the differential space time rake ( dstr ) receiver , differential space time deterministic ( dstd ) receiver , and differential space time deterministic de prefix ( dstdd ) receiver , respectively . the dstd receiver exploits the known information of the spreading sequences and their delayed paths deterministically besides the rake type combination consequently , it can outperform the dstr receiver . which employs the rake type combination only , especially for moderate to high snr . the dstdd receiver avoids the effect of intersymbol interference and hence can offer better performance than the dstd receiver . copyright ( c ) <digit> john wiley sons , ltd .",
    "target": "ds cdma;wireless communications;space time coding;smart antennas;spread spectrum;rake receiver"
  },
  {
    "source": "on the normalization of interval and fuzzy weights . <eos> the normalization of interval and fuzzy weights is often necessary in multiple criteria decision analysis ( mcda ) under uncertainty , especially in analytic hierarchy process ( ahp ) with interval or fuzzy judgements . the existing normalization methods based on interval arithmetic and fuzzy arithmetic are found flawed and need to be revised . this paper presents the correct normalization methods for interval and fuzzy weights and offers relevant theorems in support of them . numerical examples are examined to show the correctness of the proposed normalization methods and their differences from those existing normalization methods . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "normalization;fuzzy weights;interval weights;multiple criteria decision making"
  },
  {
    "source": "a queue with semi markovian batch plus poisson arrivals with application to the mpeg frame sequence . <eos> we consider a queueing system with a single server having a mixture of a semi markov process ( smp ) and a poisson process as the arrival process , where each smp arrival contains a batch of customers . the service times are exponentially distributed . we derive the distributions of the queue length of both smp and poisson customers when the sojourn time distributions of the smp have rational laplace stieltjes transforms . we prove that the number of unknown constants contained in the generating function for the queue length distribution equals the number of zeros of the denominator of this generating function in the case where the sojourn times of the smp follow exponential distributions . the linear independence of the equations generated by those zeros is discussed for the same case with additional assumption . the necessary and sufficient condition for the stability of the system is also analyzed . the distributions of the waiting times of both smp and poisson customers are derived . the results are applied to the case in which the smp arrivals correspond to the exact sequence of motion picture experts group ( mpeg ) frames . poisson arrivals are regarded as interfering traffic . in the numerical examples , the mean and variance of the waiting time of the atm cells generated from the mpeg frames of real video data are evaluated .",
    "target": "queue;mpeg;semi markov process;waiting time;batch arrival;group of pictures"
  },
  {
    "source": "fractal dimension as a descriptor of urban growth dynamics . <eos> the objective of this paper is to examine the development of the urban form of the city of olomouc since the 1920s in terms of fractal dimension , and to link the observation with two other descriptors of shape area and perimeter . the fractal dimension of built up areas and fractal dimension of the boundary of the city are calculated employing the box counting method the possibilities of their interpretation and usage in urban planning are discussed . the process of urban growth is observed with respect to its fractality and perspectives of this approach are discussed . an interesting dependence between area and its fractal dimension is derived .",
    "target": "fractal dimension;urban growth;box counting method;area perimeter relation"
  },
  {
    "source": "adaptive critics for dynamic optimization . <eos> a novel action dependent adaptive critic design ( acd ) is developed for dynamic optimization . the proposed combination of a particle swarm optimization based actor and a neural network critic is demonstrated through dynamic sleep scheduling of wireless sensor motes for wildlife monitoring . the objective of the sleep scheduler is to dynamically adapt the sleep duration to nodes battery capacity and movement pattern of animals in its environment in order to obtain snapshots of the animal on its trajectory uniformly . simulation results show that the sleep time of the node determined by the actor critic yields superior quality of sensory data acquisition and enhanced node longevity .",
    "target": "adaptive critic design;sleep scheduling;wildlife monitoring;energy efficiency;wireless sensor networks"
  },
  {
    "source": "linearity and recursion in a typed lambda calculus . <eos> we show that the full pcf language can be encoded in l _ rec , a syntactically linear calculus extended with numbers , pairs , and an unbounded recursor that preserves the syntactic linearity of the calculus . we give call by name and call by value evaluation strategies and discuss implementation techniques for l _ rec , exploiting its linearity .",
    "target": "recursion;pcf;linear lambda calculus"
  },
  {
    "source": "power aware page allocation . <eos> one of the major challenges of post pc computing is the need to reduce energy consumption , thereby extending the lifetime of the batteries that power these mobile devices . memory is a particularly important target for efforts to improve energy efficiency . memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes . in this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features . in particular , we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies . we perform experiments using two complementary simulation environments a trace driven simulator with workload traces that are representative of mobile computing and an execution driven simulator with a detailed processor memory model and a more memory intensive set of benchmarks ( spec2000 ) . our results make a compelling case for a cooperative hardware software approach for exploiting power aware memory , with down to as little as <digit> % of the energy delay for the best static policy and <digit> % to <digit> % of the energy delay for a traditional full power memory .",
    "target": "power;power aware;paging;allocation;challenge;computation;energy;energy consumption;lifetime;batteries;mobile device;memorialized;energy efficiency;technologies;power management;feature;paper;exploration;interaction;placement;dynamic;hardware;policy;exploit;informal;operating system;strategies;experience;simulation;environments;traces;workload;mobile computing;processor;memory model;benchmark;case;cooperation;software;delay"
  },
  {
    "source": "change rules for hierarchical beliefs . <eos> the paper builds a belief hierarchy as a framework common to all uncertainty measures expressing that an actor is ambiguous about his uncertain beliefs . the belief hierarchy is further interpreted by distinguishing physical and psychical worlds , associated to objective and subjective probabilities . various rules of transformation of a belief hierarchy are introduced , especially changing subjective beliefs into objective ones . these principles are applied in order to relate different contexts of belief change , revising , updating and even focusing . the numerous belief change rules already proposed in the literature receive epistemic justifications by associating them to specific belief hierarchies and change contexts . as a result , it is shown that the resiliency of probability judgments may have some limits and be reconciled with the possibility of learning from factual messages . ( c ) <digit> published by elsevier inc .",
    "target": "hierarchical belief;subjective probability;belief change;revising;updating;focusing;objective probability"
  },
  {
    "source": "a characterization of the two commodity network design problem . <eos> we study the uncapacitated version of the two commodity network design problem . we characterize optimal solutions and show that when flow costs are zero there is an optimal solution with at most one shared path . using this characterization , we solve the problem on a transformed graph with o ( n ) nodes and o ( m ) arcs based on a shortest path algorithm . next , we describe a linear programming reformulation of the problem using o ( m ) variables and o ( n ) constraints and show that it always has an integer optimal solution . we also interpret the dual constraints and variables as generalizations of the are constraints and node potentials for the shortest path problem . we show that the polyhedron described by the constraints of the reformulation always has an integer optimal solution for a more general two commodity problem with flow costs and an additional condition on the cost function . ( c ) <digit> john wiley sons , inc .",
    "target": "two commodity;network design;integer optima"
  },
  {
    "source": "on the minimum number of negations leading to super polynomial savings . <eos> we show that an explicit sequence of monotone functions f ( n ) <digit> , <digit> ( n ) > <digit> , <digit> ( m ) ( m less than or equal to n ) can be computed by boolean circuits with polynomial ( in n ) number of and , or and not gates , but every such circuit must use at least log n o ( log log n ) not gates . this is almost optimal because results of markov j. acm <digit> ( <digit> ) <digit> and fisher lecture notes in comput . sci. , vol . <digit> , springer , <digit> , p. <digit> imply that , with only small increase of the total number of gates , any circuit in n variables can be simulated by a circuit with at most log ( n <digit> ) not gates . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "computational complexity;negation limited circuits"
  },
  {
    "source": "differential log domain wave active filters . <eos> in this paper , the design of differential log domain wave filters is outlined which is based on the log domain wave technique . the differential configuration is achieved by introducing the differential log domain wave equivalent . the resulting filters inherit the good characteristics of the wave active filters , are very modular and easy to design . the method is demonstrated by a design example and its validity is verified through simulations results .",
    "target": "wave active filters;analogue filters;differential filters;log domain filters"
  },
  {
    "source": "a hybrid method based on linear programming and tabu search for routing of logging trucks . <eos> in this paper , we consider an operational routing problem to decide the daily routes of logging trucks in forestry . this industrial problem is difficult and includes aspects such as pickup and delivery with split pickups , multiple products , time windows , several time periods , multiple depots , driver changes and a heterogeneous truck fleet . in addition , the problem size is large and the solution time limited . we describe a two phase solution approach which transforms the problem into a standard vehicle routing problem with time windows . in the first phase , we solve an lp problem in order to find a destination of flow from supply points to demand points . based on this solution , we create transport nodes which each defines the origin ( s ) and destination for a full truckload . in phase two , we make use of a standard tabu search method to combine these transport nodes , which can be considered to be customers in vehicle routing problems , into actual routes . the tabu search method is extended to consider some new features . the solution approach is tested on a set of industrial cases from major forest companies in sweden .",
    "target": "linear programming;tabu search;routing;forestry;or in practice"
  },
  {
    "source": "the definition of assembly line balancing difficulty and evaluation of balance solution quality . <eos> assembly line balancing is a classic ill structured problem where total enumeration is infeasible and optimal solutions uncertain for industrial problems . a quantitative approach to classifying problem difficulty and solution quality is therefore important . two existing measures of difficulty , order strength and west ratio are compared to a new compound expression of difficulty , project index . project index is based on individual assessment of precedence ( precedence index ) and task time ( task time index ) . the current working definition of project index is given . early criteria for judging assembly lines use balance delay and smoothness index , both are flawed as criteria . line and balance efficiency are developed as more appropriate . project index , line and balance efficiency will be illustrated for a published test case examined by the a line balancing package . the potential for a learning approach , selecting models to suit problems using the measures of difficulty , will form part of the conclusions within this paper .",
    "target": "assembly line;balancing;evaluation"
  },
  {
    "source": "a class oriented feature selection approach for multi class imbalanced network traffic datasets based on local and global metrics fusion . <eos> feature selection is often used as a pre processing step for machine learning based network traffic classification . many feature selection techniques have been developed to find an optimal subset of relevant features and to improve overall classification accuracy . but such techniques ignore the class imbalance problem encountered in network traffic classification . the selected feature subset may bias towards the traffic class that occupies the majority of traffic flows on the internet . to address this issue , this paper proposes a new approach , called class oriented feature selection ( cofs ) , to identify a relevant feature subset for every class . it combines the proposed local metric and the existing global metric to yield a potentially optimal feature subset for each class , and then removes the redundant features in each feature subset based on the weighted symmetric uncertainty . additionally , to enhance the generalization on network traffic data , an ensemble learning based scheme is presented with cofs to overcome the negative impacts of the data drift on a traffic classifier . experiments on real world network traffic data show that cofs outperforms existing feature selection techniques in most cases . moreover , our approach achieves > <digit> % flow accuracy and > <digit> % byte accuracy on average .",
    "target": "feature selection;network traffic;local metrics;data drift;multi class imbalance"
  },
  {
    "source": "identifying and quantifying structural nonlinearities in engineering applications from measured frequency response functions . <eos> engineering structures seldom behave linearly and , as a result , linearity checks are common practice in the testing of critical structures exposed to dynamic loading to define the boundary of validity of the linear regime . however , in large scale industrial applications , there is no general methodology for dynamicists to extract nonlinear parameters from measured vibration data so that these can be then included in the associated numerical models . in this paper , a simple method based on the information contained in the frequency response function ( frf ) properties of a structure is studied . this technique falls within the category of single degree of freedom ( sdof ) modal analysis methods . the principle upon which it is based is effectively a linearisation whereby it is assumed that at given amplitude of displacement response the system responds at the same frequency as the excitation and that stiffness and damping are constants . in so doing , by extracting this information at different amplitudes of vibration response , it is possible to estimate the amplitude dependent natural frequency and modal loss factor . because of its mathematical simplicity and practical implementation during standard vibration testing , this method is particularly suitable for practical applications . in this paper , the method is illustrated and new analyses are carried out to validate its performance on numerical simulations before applying it to data measured on a complex aerospace test structure as well as a full scale helicopter .",
    "target": "nonlinear identification;nonlinear modal testing;nonlinear modal analysis;experimental;nonlinear modal analysis"
  },
  {
    "source": "on the formal specification and verification of multi agent systems . <eos> this article describes first steps towards the formal specification and verification of multiagent systems , through the use of temporal belief logics . the article first describes concurrent metatem , a multi agent programming language , and then develops a logic that may be used to reason about concurrent metatem systems . the utility of this logic for specifying and verifying concurrent metatem systems is demonstrated through a number of examples . the article concludes with a brief discussion on the wider implications of the work , and in particular on the use of similar logics for reasoning about multi agent systems in general .",
    "target": "formal specification and verification;multi agent systems"
  },
  {
    "source": "a contribution to multimedia document modeling and querying . <eos> metadata on multimedia documents may help to describe their content and make their processing easier , for example by identifying events in temporal media , as well as carrying descriptive information for the overall resource . metadata is essentially static and may be associated with , or embedded in , the multimedia contents . the aim of this paper is to present a proposal for multimedia documents annotation , based on modeling and unifying features elicited from content and structure mining . our approach relies on the availability of annotated metadata representing segment content and structure as well as segment transcripts . temporal and spatial operators are also taken into account when annotating documents . any feature is identified into a descriptor called meta document . these meta documents are the basis of querying by adapted query languages .",
    "target": "querying;metadata;annotation;spatiotemporal operators"
  },
  {
    "source": "a comparison of pressure and tilt input techniques for cursor control . <eos> three experiments were conducted in this study to investigate the human ability to control pen pressure and pen tilt input , by coupling this control with cursor position , angle and scale . comparisons between pen pressure input and pen tilt input have been made in the three experiments . experimental results show that decreasing pressure input resulted in very poor performance and was not a good input technique for any of the three experiments . in experiment <digit> coupling to cursor position , the tilt input technique performed relatively better than the increasing pressure input technique in terms of time . even though the tilt technique had a slightly higher error rate . in experiment <digit> coupling to cursor angle , the tilt input performed a little better than the increasing pressure input in terms of time , but the gap between them is not so apparent as experiment <digit> . in experiment <digit> coupling to cursor scale , tilt input performed a little better than increasing pressure input in terms of adjustment time . based on the results of our experiments , we have inferred several design implications and guidelines .",
    "target": "tilt input;pressure input;target selection tasks;pen based interfaces"
  },
  {
    "source": "a framework for analyzing the cognitive complexity of computer assisted clinical ordering . <eos> computer assisted provider order entry is a technology that is designed to expedite medical ordering and to reduce the frequency of preventable errors . this paper presents a multifaceted cognitive methodology for the characterization of cognitive demands of a medical information system . our investigation was informed by the distributed resources ( dr ) model , a novel approach designed to describe the dimensions of user interfaces that introduce unnecessary cognitive complexity . this method evaluates the relative distribution of external ( system ) and internal ( user ) representations embodied in system interaction . we conducted an expert walkthrough evaluation of a commercial order entry system , followed by a simulated clinical ordering task performed by seven clinicians . the dr model was employed to explain variation in user performance and to characterize the relationship of resource distribution and ordering errors . the analysis revealed that the configuration of resources in this ordering application placed unnecessarily heavy cognitive demands on the user , especially on those who lacked a robust conceptual model of the system . the resources model also provided some insight into clinicians interactive strategies and patterns of associated errors . implications for user training and interface design based on the principles of humancomputer interaction in the medical domain are discussed .",
    "target": "provider order entry;information systems;medical errors;cognitive evaluation;distributed cognition"
  },
  {
    "source": "crack propagation analysis in composite materials by using moving mesh and multiscale techniques . <eos> a novel multiscale method for crack propagation analysis in composites is proposed . an adaptive model refinement is used during crack propagation to improve efficiency . competition between different damage mechanisms is handled during crack simulation . matrix cracking is modeled by a novel optimization strategy based on moving meshes . the proposed approach is validated by original comparisons with existing methods .",
    "target": "crack propagation;composite materials;moving mesh;concurrent multiscale methods;micromechanics;interface debonding"
  },
  {
    "source": "motivations in virtual health communities and their relationship to community , connectedness and stress . <eos> this study explores the relationships between motivations for joining virtual health communities , online behaviors , and psycho social outcomes . a sample of <digit> women from two virtual health communities focusing on infertility completed survey measures assessing motivations , posting and receiving support , connectedness , community , and stress . our results indicate that socio emotional support motivations for joining the community were associated with posting support within the virtual community , while informational motivations were related to receiving support . further , receiving support was associated with greater sense of virtual community as well as more general feelings of connectedness , which was related to less stress . implications for virtual health community research are discussed .",
    "target": "motivations;virtual health communities;connectedness;stress;infertility;sense of virtual community"
  },
  {
    "source": "enterprise resource planning implementation procedures and critical success factors . <eos> enterprise resource planning ( erp ) systems are highly complex information systems . the implementation of these systems is a difficult and high cost proposition that places tremendous demands on corporate time and resources . many erp implementations have been classified as failures because they did not achieve predetermined corporate goals . this article identifies success factors , software selection steps , and implementation procedures critical to a successful implementation . a case study of a largely successful erp implementation is presented and discussed in terms of these key factors .",
    "target": "enterprise resource planning;implementation procedures;critical success factors;business process reengineering;project management"
  },
  {
    "source": "dflowz a free program to evaluate the area potentially inundated by a debris flow . <eos> debris flow inundated area can be estimated using scaling relationships . we provide a free , open source program to evaluate debris flow hazard . the model considers the uncertainties in scaling relationships and input data . a graphical user interface facilitate the process of susceptibility mapping .",
    "target": "debris flow;scaling relationships;flooding;hazard assessment"
  },
  {
    "source": "a novel approach to identify optimal access point and capacity of multiple dgs in a small , medium and large scale radial distribution systems . <eos> distributed generation ( dg ) sources are predicated to play major role in distribution systems due to the demand growth for electrical energy . location and sizing of dg sources found to be important on the system losses and voltage stability in a distribution network . in this paper an efficient technique is presented for optimal placement and sizing of dgs in a large scale radial distribution system . the main objective is to minimize network power losses and to improve the voltage stability . a detailed performance analysis is carried out on <digit> bus , <digit> bus and <digit> bus large scale radial distribution systems to demonstrate the effectiveness of the proposed technique . performing multiple power flow analysis on <digit> bus system , the effect of dg sources on the most sensitive buses to voltage collapse is also carried out .",
    "target": "large scale radial distribution system;distributed generation;voltage;simulated annealing;stability index"
  },
  {
    "source": "robust discrete control of nonlinear processes application to chemical reactors . <eos> trajectory tracking or rejecting persistent disturbances with digital controllers in nonlinear processes is a class of problems where classical control methods breakdown since it is very difficult to describe the dynamic behavior over the entire trajectory . in this paper , a model based robust control scheme is proposed as a potential solution approach for these systems . the proposed control algorithm is a robust error feedback controller that allows us to track predetermined operation profiles while attenuating the disturbances and maintaining the stability conditions of the nonlinear processes . various numerical simulation examples demonstrate the effectiveness of this robust scheme . two examples deal with effective trajectory tracking in chemical reactors over a wide range of operating conditions . the third example analyses the attenuation of periodic load in a biological reactor . all examples illustrate the ability of the robust control scheme to provide good control in the face of parameter uncertainties and load disturbances . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "robust discrete control;tracking control;reactors control"
  },
  {
    "source": "wavelet based statistical approach for speckle reduction in medical ultrasound images . <eos> a novel speckle reduction method is introduced , based on soft thresholding of the wavelet coefficients of a logarithmically transformed medical ultrasound image . the method is based on the generalised gaussian distributed ( ggd ) modelling of sub band coefficients . the method used was a variant of the recently published bayesshrink method by chang and vetterli , derived in the bayesian framework for denoising natural images . it was scale adaptive , because the parameters required for estimating the threshold depend on scale and sub band data . the threshold was computed by ksigma ( <digit> ) sigma ( x ) , where sigma and sigma ( x ) were the standard deviation of the noise and the sub band data of the noise free image , respectively , and k was a scale parameter . experimental results showed that the proposed method outperformed the median filter and the homomorphic wiener filter by <digit> % in terms of the coefficient of correlation and <digit> % in terms of the edge preservation parameter . the numerical values of these quantitative parameters indicated the good feature preservation performance of the algorithm , as desired for better diagnosis in medical image processing .",
    "target": "speckle reduction;soft thresholding;bayesshrink;median filter;wiener filter;discrete wavelet transform"
  },
  {
    "source": "computer based imaging and interventional mri applications for neurosurgery . <eos> advances in computer technology and the development of open mri systems definitely enhanced intraoperative image guidance in neurosurgery . based upon the integration of previously acquired and processed 3d information and the corresponding anatomy of the patient , this requires computerized image processing methods ( segmentation , registration , and display ) and fast image integration techniques . open mr systems equipped with instrument tracking systems , provide an interactive environment in which biopsies and minimally invasive interventions or open surgeries can be performed . enhanced by the integration of multimodal imaging these techniques significantly improve the available treatment options and can change the prognosis for patients with surgically treatable diseases .",
    "target": "interventional mri;neurosurgery;image guidance;minimally invasive therapy;surgical planning"
  },
  {
    "source": "dynamic response of a pile embedded in a porous medium subjected to plane sh waves . <eos> in this paper , the frequency domain dynamic response of a pile embedded in a porous medium subjected to sh seismic waves is investigated . the surrounding porous medium of the pile is described by biots poro elastic theory , while the pile embedded in the porous medium is treated as a beam and described by a beam vibration theory . using the hankel transformation method , the fundamental solution for a half space porous medium subjected to a horizontal circular patch load is established . according to the fictitious pile methodology , the second kind of fredholm integral equation for the pile is established in terms of the obtained fundamental solution and free wave field . the solution of the integral equation yields the dynamic response of the pile to plane sh waves . numerical results indicate that the parameters of the porous medium , the pile and incident waves have considerable influences on the dynamic response of the pile and the porous medium .",
    "target": "pile;sh waves;fredholm integral equation;biots theory;porous media"
  },
  {
    "source": "towards computational models of animal cognition , an introduction for computer scientists . <eos> the last few years of the twentieth century witnessed the emerging convergence of biology and computer science and this trend has been accelerating since then . the study of animal behavior or behavior biology has been one of the major contributors for this convergence . behavior is fascinating because it is the response of an organism to internal and external signals and it is controlled by complex interactions among nerves , the sensory and the motor systems . to some extent , behavior is similar to the output ( or response ) of a computer system or a network node if we consider an animal brain as a computer node . this paper is the first in a two part series in which i review the state of the art research in behavior biology inspired computing and communication , with the first part focusing on animal cognition and the second part on animal communication ( ma , <digit> ) . the present article also assumes the task of presenting a general introduction on behavior biology literature , which sets a foundation for synthesizing both parts of the series but the synthesis will be performed in the second part of the series . i sets three objectives in this cognition part ( i ) to present a brief overview on the literature of behavior biology for computer scientists ( ii ) to summarize the state of the art studies in several cognitive aspects of animal behavior focusing on emerging research in cognitive ecology , social learning and innovation , as well as animal logics ( iii ) to review some important existing studies inspired by animal behavior and further present a perspective on the future research . these cognition related topics offer insights for research fields such as machine learning , human computer interactions ( hci ) , brain computer interfaces ( bcis ) , evolutionary computing , pervasive computing , etc. in perspective , i suggest that the interaction between behavioral biology and computer science should be bidirectional , and a new subject , behavioral informatics , or more general computational behavior biology , should be developed by the cooperative efforts between biologists and computer scientists .",
    "target": "animal cognition;cognitive ecology;social learning;behavioral informatics;computational behavior biology;bioinspired computing and communication"
  },
  {
    "source": "crossing boundaries in facebook students framing of language learning activities as extended spaces . <eos> young peoples interaction online is rapidly increasing , which enables new spaces for communication the impact on learning , however , is not yet acknowledged in education . the aim of this exploratory case study is to scrutinize how students frame their interaction in social networking sites ( sns ) in school practices and what that implies for educational language teaching and learning practices . analytically , the study departs from a sociocultural perspective on learning , and adopts conceptual distinctions of frame analysis . the results based on ethnographic data from a facebook group in english learning classes , with <digit> students aged between <digit> and <digit> from colombia , finland , sweden and taiwan indicate that there is a possibility for boundary crossing , which could generate extended spaces for collaborative language learning activities in educational contexts where students combine their school subject of learning language and their communicative use of language in their everyday life . such extended spaces are , however , difficult to maintain and have to be recurrently negotiated . to take advantage of young peoples various dynamic communicative uses of language in their everyday life in social media , the implementation of such media for educational purposes has to be deliberately , collaboratively and dynamically negotiated by educators and students to form a new language learning space with its own potentials and constraints .",
    "target": "facebook;framing;language learning activities;extended spaces;sns;boundary crossing;computer supported collaborative learning"
  },
  {
    "source": "a study of the stabilizing process of unstable structures by dynamic relaxation method . <eos> in this paper , the stabilizing process of unstable structures is studied by dynamic relaxation method . the process of applying the internal force to unstable structures is called as stabilizing process of unstable structure . the initial behavior of unstable structures such as cables , pneumatic structures or cable domes , are unstable state because of no initial internal stiffness . the dynamic relaxation method is the energy minimization technique that searches the static equilibrium state by simple vector iteration method . because the dynamic relaxation method does not need to assemble the tangential stiffness matrix of structure during each iteration of the stabilizing process , the computational effort and cpu run time can be reduced . the finite difference integration technique is used to integrate the dynamic equilibrium equation for static equilibrium state . some numerical examples are presented to confirm the efficiency and applicability of dynamic relaxation method .",
    "target": "stabilizing process;unstable structures;dynamic relaxation method"
  },
  {
    "source": "a novel parallelization approach for hierarchical clustering . <eos> identification of groups of genes that manifest similar expression patters is a key step in the analysis of gene expression data . hierarchical clustering is developed for that purpose . a fundamental problem with the previous implementations of this clustering method is its limitation to handle large data sets within a reasonable time and memory resources . in this paper , we present a parallel approach for solving this problem . implementation of the parallel algorithm is illustrated on data from high dimensional microarray experiments related to the gene expression in cancerous disease and arabidopsis seedling growth . they show considerable reduction in computational time and inter node communication overhead , especially for large data sets .",
    "target": "parallelization;clustering;gene expression"
  },
  {
    "source": "surjective multidimensional cellular automata are non wandering a combinatorial proof . <eos> a combinatorial proof that surjective d dimensional ca are non wandering is given . this answers an old open question stated in blanchard and tisseur ( <digit> ) <digit> . moreover , an explicit upper bound for the return time is given . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "multidimensional cellular automata;combinatorial problems;symbolic dynamics;discrete dynamical systems"
  },
  {
    "source": "event based application of ws security policy on soap messages . <eos> ws security and ws security policy are the common standards for ensuring integrity and confidentiality for web service messages . on the one hand they allow very flexible definition of security requirements . on the other hand they lead to complex security administration and low performance message processing . in this paper , we present our solution for a security gateway , which uses complete event based xml and ws security processing to create policy conforming soap messages . the evaluation of our implementation shows that the event based approach leads to a much better performance than tree based ws security implementations . further , we discuss some problematical issues of ws security policy processing , such as determination of digital identities .",
    "target": "ws security;web services;event based processing"
  },
  {
    "source": "a scheduling problem with three competing agents . <eos> scheduling with multiple agents has become a popular topic in recent years . however , most of the research focused on problems with two competing agents . in this article , we consider a single machine scheduling problem with three agents . the objective is to minimize the total weighted completion time of jobs from the first agent given that the maximum completion time of jobs from the second agent does not exceed an upper bound and the maintenance activity from the third agent must be performed within a specified period of time . a lower bound based on job division and several propositions are developed for the branch and bound algorithm , and a genetic algorithm with a local search is constructed to obtain near optimal solutions . in addition , computational experiments are conducted to test the performance of the algorithms .",
    "target": "scheduling;multiple agent;single machine;total weighted completion time;maintenance activity;makespan"
  },
  {
    "source": "tsa tree seed algorithm for continuous optimization . <eos> this paper presents a new optimizer to solve continuous optimization problems . new optimizer is proposed by considering relations between trees and their seeds . the new optimizer is applied to solve <digit> benchmark functions . the results of the proposed method are compared with state of arts methods . the proposed method is a useful optimizer for continuous optimization .",
    "target": "heuristic search;tree and seed;numeric optimization;multilevel thresholding"
  },
  {
    "source": "solving fuzzy multidimensional multiple choice knapsack problems the multi start partial bound enumeration method versus the efficient epsilon constraint method . <eos> in this paper a new fuzzy multidimensional multiple choice knapsack problem ( mmkp ) is proposed . in the proposed fuzzy mmkp , each item may belong to several groups according to a predefined fuzzy membership value . the total profit and the total cost of the knapsack problem are considered as two conflicting objectives . a mathematical approach and a heuristic algorithm are proposed to solve the fuzzy mmkp . one method is an improved version of a well known exact multi objective mathematical programming technique , called the efficient constraint method . the second method is a heuristic algorithm called multi start partial bound enumeration ( pbe ) . both methods are used to comparatively generate a set of non dominated solutions for the fuzzy mmkp . the performance of the two methods is statistically compared with respect to a set of simulated benchmark cases using different diversity and accuracy metrics .",
    "target": "fuzzy multidimensional multiple choice knapsack problem;multi start partial bound enumeration method;efficient epsilon constraint method"
  },
  {
    "source": "a trace transformation technique for communication refinement . <eos> models of computation like kahn and dataflow process networks provide convenient means for modeling signal processing applications . this is partly due to the abstract primitives that these models offer for communication between concurrent processes . however , when mapping an application model onto an architecture , these primitives need to be mapped onto architecture level communication primitives . we present a trace transformation technique that supports a system architect in performing this communication refinement . we discuss the implementation of this technique in a tool for architecture exploration named spade and present examples .",
    "target": "traces;communication;refine;model;computation;dataflow;process;network;signal processing;applications;abstraction;concurrency;map;architecture;systems;implementation;tool;exploration;examples"
  },
  {
    "source": "a regularity condition of the information matrix of a multilayer perceptron network . <eos> the fisher information matrix of a multi layer perceptron network can be singular at certain parameters , and in such cases many statistical techniques based on asymptotic theory can not be applied properly . in this paper , we prove rigorously that the fisher information matrix of a three layer perceptron network is positive definite if and only if the network is irreducible that is , if there is no hidden unit that makes no contribution to the output and there is no pair of hidden units that could be collapsed to a single unit without altering the input output map . this implies that a network that has a singular fisher information matrix can be reduced to a network with a positive definite fisher information matrix by eliminating redundant hidden units . copyright <digit> elsevier science ltd",
    "target": "information matrix;multilayer perceptron;irreducibility;parametric estimation;minimality;sigmoidal function"
  },
  {
    "source": "how hci design influences web security decisions . <eos> even though security protocols are designed to make computer communication secure , it is widely known that there is potential for security breakdowns at the human machine interface . this paper reports on a diary study conducted in order to investigate what people identify as security decisions that they make while using the web . the study aimed to uncover how security is perceived in the individual 's context of use . from this data , themes were drawn , with a focus on addressing security goals such as confidentiality and authentication . this study is the first study investigating users ' web usage focusing on their self documented perceptions of security and the security choices they made in their own environment .",
    "target": "hci;design;security;diary study;participation;retail;phishing;online;trust"
  },
  {
    "source": "the complexity of finding arc disjoint branching flows . <eos> the concept of arc disjoint flows in networks was recently introduced in bang jensen and bessy ( <digit> ) . this is a very general framework within which many well known and important problems can be formulated . in particular , the existence of arc disjoint branching flows , that is , flows which send one unit of flow from a given source s s to all other vertices , generalizes the concept of arc disjoint out branchings ( spanning out trees ) in a digraph . a pair of out branchings b s , <digit> , b s , <digit> from a root s s in a digraph d ( v , a ) d ( v , a ) on n n vertices corresponds to arc disjoint branching flows x1 , x2 x <digit> , x <digit> ( the arcs carrying flow in xi x i are those used in b s , i , i 1,2 i <digit> , <digit> ) in the network that we obtain from d d by giving all arcs capacity n <digit> n <digit> . it is then a natural question to ask how much we can lower the capacities on the arcs and still have , say , two arc disjoint branching flows from the given root s s . we prove that for every fixed integer k <digit> k <digit> it is an np complete problem to decide whether a network n ( v , a , u ) n ( v , a , u ) where uij k u i j k for every arc ij i j has two arc disjoint branching flows rooted at s s . a polynomial problem to decide whether a network n ( v , a , u ) n ( v , a , u ) on n n vertices and uij n k u i j n k for every arc ij i j has two arc disjoint branching flows rooted at s s .",
    "target": "disjoint branchings;branching flow;np complete;polynomial algorithm"
  },
  {
    "source": "double ended queues with impatience . <eos> the effect of impatient behaviour is studied primarily in the context of double ended queues where each demands service from the other , typically taxis and passengers . related models , single queue , and double , with a variety of mechanisms are considered . impatience is to be understood in a wider context than simply becoming tired of waiting it can arise because the customer , for some reason , runs out of time ( inventory and organ transplantation ) , or because an alternative service becomes available ( communication applications ) . the emphasis in this paper is theoretical but a brief numerical assessment of operational consequences is given . the double ended ( or synchronization ) queue is a model for a variety of service demanding providing systems . in an orderly taxi rank at a railway station or airport , on one side a queue is formed by the arrival of stream of passengers who wait for taxis to their destinations while on the other side a queue of taxis waiting for passengers . obviously , the two queues can never coexist . the concept of impatience enters when a taxi or passenger leaves the queue before receiving service . this concept of reneging is widely applicable . in health care , for example , organs are stored for transplantation for needful patients . both the organs and the demands for them have limited lifetime . a similar scenario applies to perishable inventory systems . in a similar manner , the real time communication networks admit impatient behaviour . a typical example is a processor shared queue in data networks with random time out periods or deadlines . the paper sets out the basics in a variety of theoretical model settings with the common feature of exponential arrival , service and impatience mechanisms . a brief discussion based on numerical calculation is given of some operational features of the models but the thrust is on the theoretical techniques needed to make meaningful operational assessments .",
    "target": "60k25"
  },
  {
    "source": "su <digit> ridge waveguide with holographic grating embedded in nanoimprinted groove . <eos> fabrication of su <digit> slab and ridge waveguides with holographic grating for dfb laser , effectively utilizing nanoimprint technology ( nil ) , is presented . rhodamine 6g doped su <digit> slab and ridge waveguides were embedded in grooves defined by nil in uv curable resin . utilization of nil made it easier to form such a three dimensional micro structure consisting of ridge stripe and fine corrugation grating . te polarized 587nm laser and te polarized 594nm light emissions were observed from the slab and ridge waveguides , respectively , when the waveguides were irradiated by 532nm pulsed nd yag laser .",
    "target": "su <digit>;waveguide;holographic grating;nanoimprint;polymer optics;polymer dfb laser"
  },
  {
    "source": "optical method for characterization of nanoplates in lyosol . <eos> in the recent years , two dimensional nanoparticles ( the nanoplates ) have become a subject of intense scientific researches and industrial applications . the liquid exfoliation of layered crystals becomes the most simple and efficient manufacturing method of graphene and layered compounds nanosheets . the primary product of the liquid exfoliation is a lyosol a colloidal suspension of desired nanoparticles in a fluid . the shape of the produced nanoparticles may be determined with the use of an electron microscopy ( sem ) . in the case of spherical particles the concentration and the particles size can be assessed by the light absorption and the dynamic laser light scattering ( dls ) measurements respectively . we propose an optical method for fast and comprehensive characterization of shape and size of nanoplates in a lyosol . the system makes use of an optical goniometer . it allows to measure angular distribution of intensity of the light scattered on the lyosol sample . the distribution of the intensity ( indicatrix ) depends on the shape and size of the nanoparticles . we developed a method of theoretical indicatrix calculation of the nanoplates with the use of amsterdam discrete dipole approximation method . the experimentally measured indicatrix is compared with the theoretical calculations ( standardized indicatrices ) . we observed that the nanoplate has a dual character of light scattering . it can scatter light as a small or big particle according to the direction of the incident light beam . we have applied the proposed method of characterization for a water suspension of graphene nanosheets . results of the measurements are in good agreement with particles dimensions assessed with the sem images and do not fit with the graphene dimensions estimated with the dls method . it leads to a conclusion that the two dimensional particles size estimation with the dls is highly inaccurate .",
    "target": "nanoplates;graphene;discrete dipole approximation;light scattering measurement"
  },
  {
    "source": "what you see is what you code a live algorithm development and visualization environment for novice learners . <eos> pedagogical algorithm visualization ( av ) systems produce graphical representations that aim to assist learners in understanding the dynamic behavior of computer algorithms . in order to foster active learning , computer science educators have developed av systems that empower learners to construct their own visualizations of algorithms under study . notably , these systems support a similar development model in which coding an algorithm is temporally distinct from viewing and interacting with the resulting visualization . given that they are known to have problems both with formulating syntactically correct code , and with understanding how code executes , novice learners would appear likely to benefit from a more live development model that narrows the gap between coding an algorithm and viewing its visualization . in order to explore this possibility , we have implemented what you see is what you code , an algorithm development and visualization model geared toward novices first learning to program under the imperative paradigm . in the model , the line of algorithm code currently being edited is reevaluated on every edit , leading to immediate syntactic feedback , along with immediate semantic feedback in the form of an av . analysis of usability and field studies involving introductory computer science students suggests that the immediacy of the model 's feedback can help novices to quickly identify and correct programming errors , and ultimately to develop semantically correct code .",
    "target": "algorithm visualization;field studies;novice programming environments;live programming environments;usability studies"
  },
  {
    "source": "inver2dbasea program to compute basement depths of density interfaces above which the density contrast varies with depth . <eos> a computer program to invert the gravity anomalies of density interfaces above which the density contrast varies with depth , is presented . the sedimentbasement interface is approximated by an n sided polygon . a function subprogram gr2dpol and two subroutine subprograms zor2dpol and simeq support the main program . subroutine zor2dpol calculates the initial depth estimates of a density interface at all anomaly points on the principal profile using the infinite slab approximation . function subprogram gr2dpol computes the theoretical gravity anomaly of a density interface at each anomaly point on the profile and returns to the main program . subroutine simeq is used to solve n incremental depth parts of the vertices of a density interface . partial derivatives are calculated by a simple finite difference method . normal equations are constructed and solved by marquardt 's algorithm . the proposed inversion scheme is independent on the equal station spacing criteria . the validity of the algorithm is demonstrated by calculating the basement depths of the tucson basin , southern arizona .",
    "target": "gravity anomaly;sedimentbasement interface;n sided polygon;inversion;variable density contrast"
  },
  {
    "source": "graph based construction of textured large field of view mosaics for bladder cancer diagnosis . <eos> large field of view panoramic images greatly facilitate bladder cancer diagnosis and follow up . such 2d mosaics can be obtained by registering the images of a video sequence acquired during cystoscopic examinations . the scientific challenge in the registration process lies in the strong inter and intra patient texture variability of the images , from which primitives can not be robustly extracted . state of the art registration methods are not at the same time robust and accurate , especially for image pairs with a small amount of overlap ( less than <digit> % ) or strong perspective transformations . moreover , no previous contribution to cystoscopy mosaicing presents panoramic images created from multiple overlapping sequences ( e.g. zigzags or loop trajectories ) . we show how such overlapping sections can be automatically detected and present a novel registration algorithm that robustly superimposes non consecutive image pairs , which are related by stronger perspective transformations and share less overlap than consecutive images ( less than <digit> % ) . globally coherent panoramic images are constructed using a non linear optimization and a novel contrast enhancing stitching method . results on both phantom and patient data are obtained using constant algorithm parameters , which demonstrate the robustness of the proposed method . while the methods presented in this contribution are specifically designed for cystoscopy mosaicing , they can also be applied to more general mosaicing problems . we demonstrate this on a traditional stitching application , where a set of pictures of a building are stitched into a seamless , globally coherent panoramic image .",
    "target": "bladder cancer;image mosaicing;seamless panoramic stitching;image registration;endoscopy;graph cuts;higher order terms;non linear refinement"
  },
  {
    "source": "an heuristic set for evaluation in information visualization . <eos> evaluation is a key research challenge within the international information visualization ( infovis ) community , and heuristic evaluation is one recognized method . various sets of heuristics have been proposed but there remains no consensus as to which heuristics are most useful for addressing aspects specific to the complex interactive visual displays used in modern infovis systems . this paper presents a first effort to empirically determine a new set of such general heuristics tailored for heuristic evaluation of common and important usability problems in infovis techniques . participants in the study rated how well a total of <digit> heuristics from <digit> earlier published heuristic sets could explain a collection of <digit> usability problems derived from earlier infovis evaluations . the results were used to synthesize <digit> heuristics that , as a set , provided the highest explanatory coverage . the paper also stresses the challenges for future research to validate and further improve upon this set .",
    "target": "heuristics;information visualization;heuristic evaluation"
  },
  {
    "source": "a dependable infrastructure for cooperative web services coordination . <eos> a current trend in the web services community is to define coordination mechanisms to execute collaborative tasks involving multiple organizations . following this tendency , in this paper the authors present a dependable ( i.e. , intrusion tolerant ) infrastructure for cooperative web services coordination that is based on the tuple space coordination model . this infrastructure provides decoupled communication and implements several security mechanisms that allow dependable coordination even in presence of malicious components . this work also investigates the costs related to the use of this infrastructure and possible web service applications that can benefit from it .",
    "target": "dependability;web services coordination;coordination mechanisms;tuple spaces;web service applications"
  },
  {
    "source": "performance evaluation of an ieee 802.15.4 sensor network with a star topology . <eos> one class of applications envisaged for the ieee 802.15.4 lr wpan ( low data rate wireless personal area network ) standard is wireless sensor networks for monitoring and control applications . in this paper we provide an analytical performance model for a network in which the sensors are at the tips of a star topology , and the sensors need to transmit their measurements to the hub node so that certain objectives for packet delay and packet discard are met . we first carry out a saturation throughput analysis of the system i.e. , it is assumed that each sensor has an infinite backlog of packets and the throughput of the system is sought . after a careful analysis of the csma ca mac that is employed in the standard , and after making a certain decoupling approximation , we identify an embedded markov renewal process , whose analysis yields a fixed point equation , from whose solution the saturation throughput can be calculated . we validate our model against ns2 simulations ( using an ieee 802.15.4 module developed by zheng <digit> ) . we find that with the default back off parameters the saturation throughput decreases sharply with increasing number of nodes . we use our analytical model to study the problem and we propose alternative back off parameters that prevent the drop in throughput . we then show how the saturation analysis can be used to obtain an analytical model for the finite arrival rate case . this finite load model captures very well the qualitative behavior of the system , and also provides a good approximation to the packet discard probability , and the throughput . for the default parameters , the finite load throughput is found to first increase and then decrease with increasing load . we find that for typical performance objectives ( mean delay and packet discard ) the packet discard probability would constrain the system capacity . finally , we show how to derive a node lifetime analysis using various rates and probabilities obtained from our performance analysis model .",
    "target": "lr wpans;wireless sensor networks;performance analysis"
  },
  {
    "source": "navigation models for a flexible , multi mode vr navigation framework . <eos> navigation is a key issue for virtual reality ( vr ) applications because it forms an integral part of the feeling of presence , which should be conveyed by vr applications . this paper presents several vr navigation modes which are useful for orientation and interaction in virtual environments ( ves ) . due to the existence of different kinds of applications several navigation modes are required .",
    "target": "navigation models;virtual reality navigation;navigation techniques"
  },
  {
    "source": "multiscale simulations application to the heat transfer simulation of sliding solids . <eos> molecular dynamics is a powerful tool allowing the simulation of matter behaviour at the atomic scale . due to computation time , it is clearly not possible to use molecular dynamics to simulate a forming process . however , atomistic simulations can be used to study and understand the physical phenomena that occur during matter deformation . as an example , heat transfer between the contacting solids in forming processes is one of the important physics phenomena that have to be taken into account in order to do realistic simulations . a multiscale analysis of heat transfer is presented . this analysis leads to two kinds of models a macroscopic model which can be used for the simulation of the process itself and a microscopic model that is used to determine the parameters of the macroscopic model . in this microscopic model , the friction heat generation phenomena has to be described quite accurately . friction heat is mainly due to plastic and elastic deformation and adhesion . thus , to understand the underlying friction heat generation phenomena , atomistic simulations using molecular dynamics are carried out . it is shown that friction heat is the transformation of mechanical work given to the system at the macroscopic scale into potential energy during elastic deformation . this potential energy which is stored in the system is finally transformed into atomic kinetic energy ( friction heat ) during plastic transformation .",
    "target": "heat transfer;molecular dynamics;atomic scale;friction;sliding contact"
  },
  {
    "source": "l p nested symmetric distributions . <eos> in this paper , we introduce a new family of probability densities called l p nested symmetric distributions . the common property , shared by all members of the new class , is the same functional form rho ( x ) ( rho ) over tilde ( f ( x ) ) , where f is a nested cascade of l p norms parallel to x parallel to ( p ) ( sigma vertical bar x ( i ) vertical bar ( p ) ) ( <digit> p ) . l p nested symmetric distributions thereby are a special case of nu spherical distributions for which f is only required to be positively homogeneous of degree one . while both , nu spherical and l p nested symmetric distributions , contain many widely used families of probability models such as the gaussian , spherically and elliptically symmetric distributions , l p spherically symmetric distributions , and certain types of independent component analysis ( ica ) and independent subspace analysis ( isa ) models , nu spherical distributions are usually computationally intractable . here we demonstrate that l p nested symmetric distributions are still computationally feasible by deriving an analytic expression for its normalization constant , gradients for maximum likelihood estimation , analytic expressions for certain types of marginals , as well as an exact and efficient sampling algorithm . we discuss the tight links of l p nested symmetric distributions to well known machine learning methods such as ica , isa and mixed norm regularizers , and introduce the nested radial factorization algorithm ( nrf ) , which is a form of non linear ica that transforms any linearly mixed , non factorial l p nested symmetric source into statistically independent signals . as a corollary , we also introduce the uniform distribution on the l p nested unit sphere .",
    "target": "symmetric distribution;nu spherical distributions;independent subspace analysis;nested radial factorization;parametric density model;non linear independent component analysis;robust bayesian inference;mixed norm density model;uniform distributions on mixed norm spheres"
  },
  {
    "source": "data delivery in fragmented wireless sensor networks using mobile agents . <eos> due to the wide range of applications in sensors and wireless sensor networks ( wsn ) , research in this area has recently received increasing attention . wsns rely on network connectivity to deliver data to a base station through multihop communication . however , connectivity may not be always achievable for a number of reasons . in this paper , we study the problem of data delivery in disconnected wsns . a special class of disconnected sensor networks called fragmented wireless sensor networks ( fwsn ) is considered . a fwsn consists of several groups of connected sensors that we call fragments . to achieve connectivity between these fragments , mobile agents move in the network and act as data relays between fragments , in order to eventually deliver data to the base station . the main contribution of this paper is the modeling of the movement of these mobile relay nodes as a closed queueing network to obtain steady state results of the distribution of the mobile relays in the network . building on these results , we derive the distributions of the fragment to fragment , and fragment to sink delays . comparing these analytical results to results from the tossim simulator , it is shown that this model accurately captures the system behavior , and can be used to predict data delivery delays .",
    "target": "wireless sensor networks;mobile agents;closed queueing networks"
  },
  {
    "source": "implementation of conditional simulation by successive residuals . <eos> conditional simulation of ergodic and stationary gaussian random fields using successive residuals is a new approach used to overcome the size limitations of the lu decomposition algorithm as well as provide fast updating of existing simulated realizations with new data . this paper discusses two different implementations of this approach . the implementations differ in the use of the new information available in the first implementation new information is partially used to generate updated realizations however , in the second implementation , the realizations are updated using all the new information available . the implementations are validated using the walker lake data set , and compared through a case study at a stockwork gold deposit .",
    "target": "successive residuals;lu decomposition;generalised sequential gaussian simulation"
  },
  {
    "source": "completion of overdetermined parabolic pdes . <eos> in this paper we apply methods of commutative algebra to analysis of systems of pdes . more precisely , we show that systems which are parabolic in a generalized sense are equivalent to certain completed systems which are parabolic in the standard sense . we also propose a constructive method for getting this completion , and grobner basis methods , via symbol modules of the systems , play a central role in practical computations . moreover , we can easily construct systems which are not parabolic in the generalized sense but nevertheless become parabolic when completed . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "completion;overdetermined system;partial differential equation;parabolic system;graded module;free resolution"
  },
  {
    "source": "steady state analysis of a discrete time batch arrival queue with working vacations . <eos> this paper analyzes a discrete time batch arrival queue with working vacations . in a geo x g <digit> system , the server works at a lower speed during the vacation period which becomes a lower speed operation period . this model is more appropriate for the communication systems with the transmit units arrived in batches . we formulate the system as an embedded markov chain at the departure epoch and by the m g <digit> type matrix analytic approach , we derive the probability generating function ( pgf ) of the stationary queue length . then , we obtain the distribution for the number of the customers at the busy period initiation epoch , and use the stochastic decomposition technique to present another equivalent pgf of the queue length . we also develop a variety of stationary performance measures for this system . some special models and numerical results are presented . finally , a real world example in an ethernet passive optical network ( epon ) is provided .",
    "target": "batch arrival;working vacations;m g <digit> type matrix;stochastic decomposition;ethernet passive optical network;discrete time queue"
  },
  {
    "source": "bimql an open query language for building information models . <eos> in this paper we present the on going development of a framework for a domain specific , open query language for building information models . the proposed query language is intended for selecting , updating and deleting of data stored in industry foundation classes models . even though some partial solutions already have been suggested , none of them are open source , domain specific , platform independent and implemented at the same time . this paper provides an overview of existing approaches , conceptual sketches of the language in development and documents the current state of implementation as a prototype plugin developed for the open source model server platform bimserver.org . we report on the execution of example test cases to show the general feasibility of the approach chosen .",
    "target": "bimql;query language;domain specific language;ifc;bim;building information model server"
  },
  {
    "source": "a prototype for an agent based secure electronic marketplace including reputation tracking mechanisms . <eos> software agents will play a crucial role in the coming digital economy , but their reliability and honesty can not be guaranteed by technical security mechanisms , such as encryption of messages and digital signing of documents , and entities that can sanction fraudulent behavior in open networks are still in a rudimentary stage . this article describes a reputation mechanism that records previous cooperation behavior of participants in agent based markets and conveys this information to other software agents , thereby influencing the future behavior of participants . the mechanism has been prototypically implemented in the avalanche multiagent system . the deployment of this reputation mechanism will help to exclude fraudulent software agents from market participation .",
    "target": "electronic marketplaces;reputation tracking;software agents;agent mediated electronic commerce"
  },
  {
    "source": "an advanced hydro mechanical constitutive model for unsaturated soils with different initial densities . <eos> this paper presents an advanced constitutive model for unsaturated soils , using bishops effective stress ( ) and the effective degree of saturation ( se ) as two fundamental constitutive variables in the proposed constitutive model . a sub loading surface and a unified hardening parameter ( h ) are introduced into the se modelling framework to interpret the effects of initial density on coupled hydro mechanical behaviour of compacted soils . compared with existing models in the literature , the main advantage of the proposed model that it is capable of modelling hydro mechanical behaviour of unsaturated soils compacted to different initial densities , such as the dependence of loadingcollapse volume on initial void ratio and density effect on the shearing induced saturation change . the proposed model requires <digit> material parameters , all of which can be calibrated through conventional laboratory tests . numerical studies are conducted to assess the performance of the model for a hypothetical soil under two typical hydro mechanical loading scenarios . the proposed advanced unsaturated soil model is then validated against a number of experimental results for both isotropic and triaxial conditions reported in the literature .",
    "target": "unsaturated soils;initial densities;bishops effective stress;degree of saturation;compacted soils;hydro mechanical interaction"
  },
  {
    "source": "understanding the value of software engineering technologies . <eos> when ai search methods are applied to software process models , then appropriate technologies can be discovered for a software project . we show that those recommendations are greatly affected by the business context of its use . for example , the automatic defect reduction tools explored by the ase community are only relevant to a subset of software projects , and only according to certain value criteria . therefore , when arguing for the value of a particular technology , that argument should include a description of the value function of the target user community .",
    "target": "artificial intelligence;software economics"
  },
  {
    "source": "the unfinished history of usage rights for spectrum . <eos> the key task in the next stage of spectrum management is to adapt regulation to the prospect of widespread sharing , on a much more sophisticated basis than sharing is used today . there is a role for the regulator to take steps to expand the area of choice within which public and private sector users can operate . this is best done in general by enhancing the flexibility of usage rights , which itself is best achieved by enhancing the freedom to trade them in the dimensions of time , space , level of interference and priority of access , by subdividing , re aggregating , etc. however , there are considerable transactions cost impediments to trading where unlicensed users are involved . this creates a role for the regulator pro actively to investigate different allocations , to make provisions for the most promising to occur and to incorporate both in refarming exercises and in primary assignments based on auctions configurations of usage rights , which might favour promising avenues of shared spectrum use .",
    "target": "usage rights;spectrum;unlicensed;refarming;surs"
  },
  {
    "source": "a discrete scheme of laplacebeltrami operator and its convergence over quadrilateral meshes . <eos> laplacebeltrami operator and its discretization play a central role in the fields of image processing , computer graphics , computer aided geometric design and so on . in this paper , a discrete scheme for laplacebeltrami operator over quadrilateral meshes is constructed based on a bilinear interpolation of the quadrilateral . convergence results for the proposed discrete scheme are established under some conditions . numerical results which justify the theoretical analysis are also given .",
    "target": "discretization;laplacebeltrami operator;convergence;quadrilateral meshes;bilinear interpolation;mean curvature"
  },
  {
    "source": "energy and time efficient algorithm for cloud offloading using dynamic profiling . <eos> with the advent of computationally intensive application for mobile devices there is need of time and energy efficient component offloading algorithm which involves execution of resource intensive components of an application on remote machine . traditional solution includes offloading of entire application ( no partition ) , offloading predetermined components ( static partition ) or making offloading decision at runtime for each component ( <digit> ilp ) . our proposed solution of dynamic profiling uses depth first search ( topological sorting ) to calculate the offloading point at runtime . the subsequent nodes are offloaded with high probability . experimental result demonstrates that proposed algorithm is better than <digit> ilp in time domain while outperforming no partitioning and static partitioning in energy domain .",
    "target": "depth first search;topological sorting;mobile computing"
  },
  {
    "source": "adaptive critic design based robust neural network control for nonlinear distributed parameter systems with unknown dynamics . <eos> in this paper , an adaptive critic design ( acd ) based robust on line neural network control design is developed for a class of parabolic partial differential equation ( pde ) systems with unknown nonlinear dynamics . first , the galerkin method is applied to the parabolic pde system to derive a finite dimensional slow one and an infinite dimensional stable fast subsystem . the obtained slow system is an ordinary differential equation ( ode ) system with unknown nonlinearities , which accurately describes the dynamics of the slow modes of the pde system . then , a novel acd based robust optimal control scheme is proposed for the resulting nonlinear slow system with unknown dynamics . an action neural network ( nn ) is employed to approximate all the derived unknown nonlinear terms and a robust control term is further developed to attenuate the nn reconstruction errors and disturbances . especially , by developing novel critic signals and lyapunov function candidate , together with the adaptive bounding technique , no a prior knowledge for the bounds of the disturbance term , the nn ideal weights of action nn and critic nn and the nn reconstruction errors is required . finally , simulation results demonstrate the effectiveness of the proposed robust optimal control scheme .",
    "target": "adaptive critic designs;neural networks;partial differential equation;adaptive dynamic programming;learning control;uniformly ultimate boundedness"
  },
  {
    "source": "web services with generic simulation models for discrete event simulation . <eos> today the internet and the world wide web ( www ) are on the cusp of a paradigm shift . up to now most actions in the www are sorts of human computer interaction , but the introduction of the extensible markup language ( xml ) changed the perception . the internet will be seen as a great space of information and with the use of xml and following technologies like web services , grid computing and semantic web the difference between human machine interaction and machine machine interaction vanishes . this work investigates the usefulness of xml in the simulation domain and uses web service technology to build the simasp framework for discrete event simulation ( des ) . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .",
    "target": "web service;discrete event simulation;xml;application service providing"
  },
  {
    "source": "the impact of sample reduction on pca based feature extraction for supervised learning . <eos> the curse of dimensionality is pertinent to many learning algorithms , and it denotes the drastic raise of computational complexity and classification error in high dimensions . in this paper , different feature extraction ( fe ) techniques are analyzed as means of dimensionality reduction , and constructive induction with respect to the performance of nave bayes classifier . when a data set contains a large number of instances , some sampling approach is applied to address the computational complexity of fe and classification processes . the main goal of this paper is to show the impact of sample reduction on the process of fe for supervised learning . in our study we analyzed the conventional pca and two eigenvector based approaches that take into account class information . the first class conditional approach is parametric and optimizes the ratio of between class variance to the within class variance of the transformed data . the second approach is a nonparametric modification of the first one based on the local calculation of the between class covariance matrix . the experiments are conducted on ten uci data sets , using four different strategies to select samples ( <digit> ) random sampling , ( <digit> ) stratified random sampling , ( <digit> ) kd tree based selective sampling , and ( <digit> ) stratified sampling with kd tree based selection . our experiments show that if the sample size for fe model construction is small then it is important to take into account both class information and data distribution . further , for supervised learning the nonparametric fe approach needs much less instances to produce a new representation space that result in the same or higher classification accuracy than the other fe approaches .",
    "target": "sample reduction;feature extraction;supervised learning"
  },
  {
    "source": "a topology preserving level set method for geometric deformable models . <eos> active contour and surface models , also known as deformable models , are powerful image segmentation techniques . geometric deformable models implemented using level set methods have advantages over parametric models due to their intrinsic behavior , parameterization independence , and ease of implementation . however , a long claimed advantage of geometric deformable models the ability to automatically handle topology changes turns out to be a liability in applications where the object to be segmented has a known topology that must be preserved . in this paper , we present a new class of geometric deformable models designed using a novel topology preserving level set method , which achieves topology preservation by applying the simple point concept from digital topology . these new models maintain the other advantages of standard geometric deformable models including subpixel accuracy and production of nonintersecting curves or surfaces . moreover , since the topology preserving constraint is enforced efficiently through local computations , the resulting algorithm incurs only nominal computational overhead over standard geometric deformable models . several experiments on simulated and real data are provided to demonstrate the performance of this new deformable model algorithm .",
    "target": "topology preservation;level set method;geometric deformable model;active contours;simple points;digital topology;topological constraint"
  },
  {
    "source": "elastic geodesic paths in shape space of parameterized surfaces . <eos> this paper presents a novel riemannian framework for shape analysis of parameterized surfaces . in particular , it provides efficient algorithms for computing geodesic paths which , in turn , are important for comparing , matching , and deforming surfaces . the novelty of this framework is that geodesics are invariant to the parameterizations of surfaces and other shape preserving transformations of surfaces . the basic idea is to formulate a space of embedded surfaces ( surfaces seen as embeddings of a unit sphere in r <digit> ) and impose a riemannian metric on it in such a way that the reparameterization group acts on this space by isometries . under this framework , we solve two optimization problems . one , given any two surfaces at arbitrary rotations and parameterizations , we use a path straightening approach to find a geodesic path between them under the chosen metric . second , by modifying a technique presented in <digit> , we solve for the optimal rotation and parameterization ( registration ) between surfaces . their combined solution provides an efficient mechanism for computing geodesic paths in shape spaces of parameterized surfaces . we illustrate these ideas using examples from shape analysis of anatomical structures and other general surfaces .",
    "target": "geodesics;shape analysis;path straightening;riemannian distance;parameterization invariance"
  },
  {
    "source": "on the coupling of the homotopy perturbation method and laplace transformation . <eos> in this paper , a laplace homotopy perturbation method is employed for solving one dimensional non homogeneous partial differential equations with a variable coefficient . this method is a combination of the laplace transform and the homotopy perturbation method ( lhpm ) . lhpm presents an accurate methodology to solve non homogeneous partial differential equations with a variable coefficient . the aim of using the laplace transform is to overcome the deficiency that is mainly caused by unsatisfied conditions in other semi analytical methods such as hpm , vim , and adm. the approximate solutions obtained by means of lhpm in a wide range of the problem 's domain were compared with those results obtained from the actual solutions , the homotopy perturbation method ( hpm ) and the finite element method . the comparison shows a precise agreement between the results , and introduces this new method as an applicable one which it needs fewer computations and is much easier and more convenient than others , so it can be widely used in engineering too . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "homotopy perturbation method;laplace homotopy perturbation method;non homogeneous partial differential equation"
  },
  {
    "source": "central limit theorems for super ornstein uhlenbeck processes . <eos> suppose that x x t t <digit> is a supercritical super ornstein uhlenbeck process , that is , a superprocess with an ornstein uhlenbeck process on ( mathbb r d ) corresponding to ( l frac <digit> <digit> sigma <digit> delta b x cdot nabla ) as its underlying spatial motion and with branching mechanism ( ) <digit> ( <digit> , ) ( e x <digit> x ) n ( dx ) , where ( <digit> ) > <digit> , <digit> , and n is a measure on ( <digit> , ) such that ( <digit> , ) x <digit> n ( dx ) < . let ( mathbb p _ mu ) be the law of x with initial measure . then the process w t e t x t is a positive ( mathbb p _ mu ) martingale . therefore there is w such that w t w , ( mathbb p _ mu ) a.s. as t . in this paper we establish some spatial central limit theorems for x.",
    "target": "central limit theorem;super ornstein uhlenbeck process;ornstein uhlenbeck process;superprocess;backbone decomposition;branching process;branching ornstein uhlenbeck process;60j80;60g57;60j45"
  },
  {
    "source": "e learning recommender system for a group of learners based on the unified learner profile approach . <eos> in the age of information explosion , e learning recommender systems ( el_rss ) have emerged as effective information filtering techniques that attempt to provide the most appropriate learning resources for learners while using e learning systems . these learners are differentiated on the basis of their learning styles , goals , knowledge levels and others . several attempts have been made in the past to design el_rss to recommend resources to individuals however , an investigation of recommendations to a group of learners in e learning is still in its infancy . in this paper , we focus on the problem of recommending resources to a group of learners rather than to an individual . the major challenge in group recommendation is how to merge the individual preferences of different learners that form a group and extract a pseudo unified learner profile ( ulp ) that closely reflects the preferences of all learners . firstly , we propose a profile merging scheme for the ulp by utilizing learning styles , knowledge levels and ratings of learners in a group . thereafter , a collaborative approach is proposed based on the ulp for effective group recommendations . experimental results are presented to demonstrate the effectiveness of the proposed group recommendation strategy for e learning .",
    "target": "e learning;recommender systems;learning styles;knowledge levels;group learning"
  },
  {
    "source": "sms based human hosted interactive tv in finland . <eos> interactive tv entertainment has brought to life a new kind of tv game show host culture in finland . a qualitative study of sms to tv human hosted interactive tv games , specifically , tv mobile games and call quizzes , was conducted by recording sample interactive tv programs and corresponding discussion forums on the internet and analyzing the content . the role of the human host in these programs was analyzed and discussed to answer these questions why is this interactive entertainment popular what different dimensions can be found how could this field be used more effectively and what are the aspects developers should pay attention to while designing itv entertainment this research is important beyond finland since finland tends to pioneer interactive entertainment that later spreads out to other countries",
    "target": "tv mobile games;tv quizzes;ethnography"
  },
  {
    "source": "artificial channel aided lmmse estimation for timefrequency selective channels in ofdm context . <eos> this paper proposes a linear minimum mean square error based ( lmmse ) channel estimation method , which allows avoiding the necessary knowledge of the channel covariance matrix or its estimation . to do so , a perfectly tunable filter acting like an artificial channel is added at the receiver side . we show that an lmmse estimation of the sum of this artificial channel and the physical channel only needs the covariance matrix of the artificial channel , and the channel estimation is finally obtained by subtracting the frequency coefficients of the added filter . we call this method artificial channel aided lmmse ( aca lmmse ) . theoretical developments and simulations prove that its performance is close to theoretical lmmse , and we show that this method reduces the computational complexity , compared to usual lmmse , due to the covariance matrix used for aca lmmse is computed only once throughout the transmission duration . we put the conditions on the artificial channel parameters to get the expected mask effect . simulations display the performance of the proposed method , in terms of mmse and bit error rate ( ber ) . indeed , the difference of ber between our method and the theoretical lmmse is less than 2db .",
    "target": "ofdm;channel estimation;mean square error methods;digital radio mondiale"
  },
  {
    "source": "prediction of automobile tire cornering force characteristics by finite element modeling and analysis . <eos> in this study , a detailed finite element model of a radial automobile tire is constructed for the prediction of cornering force characteristics during the design stage . the nonlinear stressstrain relationship of rubber as well as a linear elastic approximation , reinforcement , large displacements , and frictional ground contact are modeled . validity of various simplifications is checked . the cornering force characteristics obtained by the finite element tire model are verified on the experimental setup constructed for this purpose .",
    "target": "cornering force characteristics;pneumatic tires;nonlinear finite element analysis;tire testing"
  },
  {
    "source": "design and implementation of a mppt circuit for a solar uav . <eos> this paper presents a maximum power point tracking ( mppt ) circuit for an unmanned air vehicle . the design of the mppt is proposed utilizing a boost converter topology . the power of the photovoltaic cells is monitored by a closed loop microcontroller based control system , and the pwm signal of the boost converter continuously adjusted to extract maximum power . the mppt is used to charge the lithium ion polymer battery and feed the electrical load of the unmanned aircraft .",
    "target": "mppt;uav;photovoltaic cells"
  },
  {
    "source": "technical assessment and evaluation of environmental models and software letter to the editor . <eos> this letter details the collective views of a number of independent researchers on the technical assessment and evaluation of environmental models and software . the purpose is to stimulate debate and initiate action that leads to an improved quality of model development and evaluation , so increasing the capacity for models to have positive outcomes from their use . as such , we emphasize the relationship between the model evaluation process and credibility with stakeholders ( including funding agencies ) with a view to ensure continued support for modelling efforts . many journals , including em s , publish the results of environmental modelling studies and must judge the work and the submitted papers based solely on the material that the authors have chosen to present and on how they present it . there is considerable variation in how this is done with the consequent risk of considerable variation in the quality and usefulness of the resulting publication . part of the problem is that the review process is reactive , responding to the submitted manuscript . in this letter , we attempt to be proactive and give guidelines for researchers , authors and reviewers as to what constitutes best practice in presenting environmental modelling results . this is a unique contribution to the organisation and practice of model based research and the communication of its results that will benefit the entire environmental modelling community . for a start , our view is that the community of environmental modellers should have a common vision of minimum standards that an environmental model must meet . a common vision of what a good model should be is expressed in various guidelines on good modelling practice . the guidelines prompt modellers to codify their practice and to be more rigorous in their model testing . our statement within this letter deals with another aspect of the issue it prompts professional journals to codify the peer review process . introducing a more formalized approach to peer review may discourage reviewers from accepting invitations to review given the additional time and labour requirements . the burden of proving model credibility is thus shifted to the authors . here we discuss how to reduce this burden by selecting realistic evaluation criteria and conclude by advocating the use of standardized evaluation tools as this is a key issue that needs to be tackled .",
    "target": "model evaluation;model credibility;software verification;environmental assessment"
  },
  {
    "source": "on the wadge reducibility of k partitions . <eos> we establish some results on the wadge degrees and on the boolean hierarchy of k partitions of some spaces , where k is a natural number . the main attention is paid to the baire space , baire domain and their close relatives . for the case of delta ( <digit> ) ( <digit> ) measurable k partitions the structures of wadge degrees are characterized completely . for many degree structures , undecidability of the first order theories is shown , for any k > <digit> . ( c ) <digit> elsevier inc. all rights reserved .",
    "target": "wadge reducibility;k partition;baire space;baire domain;discrete weak semilattice;forest;homomorphic preorder"
  },
  {
    "source": "simulation of axonal excitability using a spreadsheet template created in microsoft excel . <eos> the objective of this present study was to implement an established simulation protocol ( a.m. brown , a methodology for simulating biological systems using microsoft excel , comp . methods prog . biomed . <digit> ( <digit> ) <digit> ) to model axonal excitability . the simulation protocol involves the use of in cell formulas directly typed into a spreadsheet and does not require any programming skills or use of the macro language . once the initial spreadsheet template has been set up the simulations described in this paper can be executed with a few simple keystrokes . the model axon contained voltage gated ion channels that were modeled using hodgkin huxley style kinetics . the basic properties of axonal excitability modeled were ( <digit> ) threshold of action potential firing , demonstrating that not only are the stimulus amplitude and duration critical in the generation of an action potential , but also the resting membrane potential ( <digit> ) refractoriness , the phenomenon of reduced excitability immediately following an action potential . the difference between the absolute refractory period , when no amount of stimulus will elicit an action potential , and relative refractory period , when an action potential may be generated by applying increased stimulus , was demonstrated with regard to the underlying state of the na and k channels ( <digit> ) temporal summation , a process by which two sub threshold stimuli can unite to elicit an action potential was shown to be due to conductance changes outlasting the first stimulus and summing with the second stimulus induced conductance changes to drive the membrane potential past threshold ( <digit> ) anode break excitation , where membrane hyperpolarization was shown to produce an action potential by removing na channel inactivation that is present at resting membrane potential . the simulations described in this paper provide insights into mechanisms of axonal excitation that can be carried out by following an easily understood protocol .",
    "target": "simulation;axon;spreadsheet;microsoft excel;modeling;ion channel;hodgkin huxley"
  },
  {
    "source": "trabecular bone remodelling under pathological conditions based on biochemical and mechanical processes involved in bmu activity . <eos> in adulthood , bone tissue is continuously renewed by processes governed by basic multicellular units composed of osteocytes , osteoclasts and osteoblasts , which are subjected to local mechanical loads . osteocytes are known to be integrated mechanosensors that regulate the activation of the osteoclasts and osteoblasts involved in bone resorption and apposition processes , respectively . after collagen tissue apposition , a process of collagen mineralisation takes place , gradually increasing the effective stiffness of bone . this study presents a new model based on physicochemical parameters involved in spongy bone remodelling under pathological conditions . our model simulates the transient evolution of both geometry and effective young 's modulus of the trabeculae , also taking turnover into account . various loads were applied on a trabecula in order to determine the evolution of bone volume fraction under pathological conditions . a parametric study performed on the model showed that one key parameter here is the kinetic constant of hydroxyapatite crystallisation . we subsequently tested our model on a pathological case approaching osteoporosis , involving a decrease in the number of viable osteocytes present in bone . the model converges to a lower value ( <digit> % ) for bone volume fraction than with a normal quantity of osteocytes . this useful tool offers new perspectives for predicting bone remodelling deficits on a local scale in patients with pathological conditions such as osteoporosis and in bedridden patients , as well as for astronauts subjected to weightlessness in space .",
    "target": "osteocytes;osteoclast;osteoblast;mineralisation;bone pathology"
  },
  {
    "source": "robust and imperceptible dual watermarking for telemedicine applications . <eos> in this paper , the effects of different error correction codes on the robustness and imperceptibility of discrete wavelet transform and singular value decomposition based dual watermarking scheme is investigated . text and image watermarks are embedded into cover radiological image for their potential application in secure and compact medical data transmission . four different error correcting codes such as hamming , the bose , ray chaudhuri , hocquenghem ( bch ) , the reedsolomon and hybrid error correcting ( bch and repetition code ) codes are considered for encoding of text watermark in order to achieve additional robustness for sensitive text data such as patient identification code . performance of the proposed algorithm is evaluated against number of signal processing attacks by varying the strength of watermarking and covers image modalities . the experimental results demonstrate that this algorithm provides better robustness without affecting the quality of watermarked image.this algorithm combines the advantages and removes the disadvantages of the two transform techniques . out of the three error correcting codes tested , it has been found that reedsolomon shows the best performance . further , a hybrid model of two of the error correcting codes ( bch and repetition code ) is concatenated and implemented . it is found that the hybrid code achieves better results in terms of robustness . this paper provides a detailed analysis of the obtained experimental results .",
    "target": "error correcting codes;discrete wavelet transforms;singular value decomposition;image watermarking;steganography"
  },
  {
    "source": "robust estimation of dimension reduction space . <eos> most dimension reduction methods based on nonparametric smoothing are highly sensitive to outliers and to data coming from heavy tailed distributions . two recently proposed methods , minimum average variance estimation and outer product of gradients , can be and are made robust in such a way that preserves all advantages of the original approach . their extension based on the local one step m estimators is sufficiently robust to outliers and data from heavy tailed distributions , it is relatively easy to implement , and surprisingly , it performs as well as the original methods when applied to normally distributed data .",
    "target": "dimension reduction;l and m estimation;nonparametric regression"
  },
  {
    "source": "complexity of deciding sense of direction . <eos> in this paper we prove that deciding whether a distributed system ( represented as a colored digraph with n nodes ) has weak sense of direction is in ac ( <digit> ) ( using n ( <digit> ) processors ) . moreover , we show that deciding sense of direction is in p. our algorithms can also be used to decide in ac ( <digit> ) whether a colored graph is a cayley color graph .",
    "target": "sense of direction;distributed systems;computational complexity;cayley graphs"
  },
  {
    "source": "probabilistic fuzzy image fusion approach for radar through wall sensing . <eos> this paper addresses the problem of combining multiple radar images of the same scene to produce a more informative composite image . the proposed approach for probabilistic fuzzy logic based image fusion automatically forms fuzzy membership functions using the gaussian rayleigh mixture distribution . it fuses the input pixel values directly without requiring fuzzification and defuzzification , thereby removing the subjective nature of the existing fuzzy logic methods . in this paper , the proposed approach is applied to through the wall radar imaging in urban sensing and evaluated on real multi view and polarimetric data . experimental results show that the proposed approach yields improved image contrast and enhances target detection .",
    "target": "image fusion;fuzzy logic;through the wall radar imaging"
  },
  {
    "source": "inductive time space lower bounds for sat and related problems . <eos> we improve upon indirect diagonalization arguments for lower bounds on explicit problems within the polynomial hierarchy . our contributions are summarized as follows . <digit> . we present a technique that uniformly improves upon most known nonlinear time lower bounds for nondeterminism and alternating computation , on both subpolynomial ( n ( o ( <digit> ) ) ) space rams and sequential one tape machines with random access to the input . we obtain improved lower bounds for boolean satisfiability ( sat ) , as well as all np complete problems that have efficient reductions from sat , and sigma ( k ) sat , for constant k > <digit> . for example , sat can not be solved by random access machines using n ( root <digit> ) time and subpolynomial space . <digit> . we show how indirect diagonalization leads to time space lower bounds for computation with bounded nondeterminism . for both the random access and multitape turing machine models , we prove that for all k > <digit> , there is a constant c ( k ) > <digit> such that linear time with n ( <digit> k ) nondeterministic bits is not contained in deterministic n ( ck ) time with subpolynomial space . this is used to prove that satisfiability of boolean circuits with n inputs and n ( k ) size can not be solved by deterministic multitape turing machines running in n ( k.ck ) time and subpolynomial space .",
    "target": "lower bounds;diagonalization;satisfiability;bounded nondeterminism;time space tradeoffs;polynomial time hierarchy"
  },
  {
    "source": "general form of lattice valued fuzzy sets under the cutworthy approach . <eos> in this note a new solution of problem of synthesis of fuzzy sets is presented . in other words , necessary and sufficient conditions are formulated , under which for a given family of subsets f of a set x and a fixed complete lattice l there is a fuzzy set it x > l , such that the collection of cuts of it coincides with f. moreover , it is proved that the general form of lattice valued fuzzy sets ( considering families of cuts ) are the type of fuzzy sets having the codomain ( <digit> , <digit> ( c ) for a suitable chosen cardinal c. ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "lattice valued fuzzy sets;cuts"
  },
  {
    "source": "mechanotransduction in cardiac myocytes . <eos> cardiac myocytes react to diverse mechanical demands with a multitude of transient and long term responses to normalize the cellular mechanical environment . several stretch activated signaling pathways have been identified , most prominently guanine nucleotide binding proteins ( g proteins ) , mitogen activated protein kinases ( mapk ) , janus associated kinase signal transducers and activators of transcription ( jak stat ) , protein kinase c ( pkc ) , calcineurin , intracellular calcium regulation , and several autocrine and paracrine factors . multiple levels of crosstalk exist between pathways . the cellular response to changes in the mechanical environment can lead to cardiac myocyte hypertrophy , cellular growth that can be accompanied by pathological myocyte dysfunction , and tissue fibrosis . several candidates for the primary mechanosensor in cardiac myocytes have been identified , ranging from stretch activated ion channels in the membrane to yet unknown mechanosensitive mechanisms in the nucleus . new and refined experimental techniques will exploit advances in molecular biology and biological imaging to study mechanotransduction in isolated cells and genetically engineered mice to explore the function of individual proteins .",
    "target": "mechanotransduction;cardiac myocytes;cardiac hypertrophy"
  },
  {
    "source": "design of a sliding window scheme for detecting high packet rate flows via random packet sampling . <eos> we discuss the design of a sliding window scheme for detecting high packet rate flows via random packet sampling . we determine the values of control parameters , such as the sampling rate and window length , to minimize the false positive ratio , while keeping the false negative ratio sufficiently low and making the on line processing possible . under mild assumptions , we formulate this problem as a nonlinear program and provide its numerically feasible global optimal solution . we then conduct sampling experiments with public trace data and discuss the fundamental characteristics of the sliding window scheme with random packet sampling .",
    "target": "sliding window scheme;high packet rate flows;random packet sampling"
  },
  {
    "source": "the fuzzy metric truth reasoning approach to decision making in soft computing milieux . <eos> this article considers fuzzy approximate reasoning utilizing the metric ruth approach . this approach assesses the truth of a sentence on the basis of its distance from the respective true one . the author has previously examined this subject matter from the logico methodological point of view . this article focuses on the aspects typical of fuzzy if then rules within control and decision making .",
    "target": "decision making;soft computing;fuzzy reasoning"
  },
  {
    "source": "querying a summary of database . <eos> for some years , data summarization techniques have been developed to handle the growth of databases . however these techniques are usually not provided with tools for end users to efficiently use the produced summaries . this paper presents a first attempt to develop a querying tool for the saintetiq summarization model . the proposed search algorithm takes advantage of the hierarchical structure of the saintetiq summaries to efficiently answer questions such as how are , on some attributes , the tuples which have specific characteristics moreover , this algorithm can be seen both as a boolean querying mechanism over a hierarchy of summaries , and as a flexible querying mechanism over the underlying relational tuples .",
    "target": "data summarization;flexible querying;linguistic summaries;summary querying;relational database;fuzzy labels"
  },
  {
    "source": "subband domain coding of binary textual images for document archiving . <eos> in this work , a subband domain textual image compression method is developed . the document image is first decomposed into subimages using binary subband decompositions . next , the character locations in the subbands and the symbol library consisting of the character images are encoded , the method is suitable for keyword search in the compressed data . it is observed that very high compression ratios are obtained with this method . simulation studies are presented .",
    "target": "textual image compression;binary subband decomposition;binary image coding;document retrieval"
  },
  {
    "source": "on site volume rendering with gpu enabled devices . <eos> now that high performance computing systems can rely more on a cloud based infrastructure , it becomes much more important to have ubiquitous data processing and visualization capability . this will allow data sharing among numerous clients using shared data repositories through a secure web server . thanks to the wide availability of gpu support in todays mobile devices such as smart phones and tablets , as well as the recently published webgl standard , pervasive computing for high quality and real time volume rendering may be realized on such high performance platforms . we have invented two high performance volume renderers , namely , single pass gpu ray caster and fast 3d texture slicer , for both mobile and desktop platforms . rigorous experiments and performance assessments reveal that the proposed mobile 3d image rendering system outperforms the existing approaches in the literature .",
    "target": "gpu;webgl;pervasive computing;3d rendering;mobile computing"
  },
  {
    "source": "multipartite priority queues . <eos> we introduce a framework for reducing the number of element comparisons performed in priority queue operations . in particular , we give a priority queue which guarantees the worst case cost of o ( <digit> ) per minimum finding and insertion , and the worst case cost of o ( log n ) with at most log n o ( <digit> ) element comparisons per deletion , improving the bound of <digit> log n o ( <digit> ) known for binomial queues . here , n denotes the number of elements stored in the data structure prior to the operation in question , and log n equals log ( <digit> ) ( max <digit> , n ) . as an immediate application of the priority queue developed , we obtain a sorting algorithm that is optimally adaptive with respect to the inversion measure of disorder , and that sorts a sequence having n elements and i inversions with at most n log ( i n ) o ( n ) element comparisons .",
    "target": "priority queues;heaps;meticulous analysis;constant factors"
  },
  {
    "source": "bounds on codes derived by counting components in varshamov graphs . <eos> we are interested in improving the varshamov bound for finite values of length n and minimum distance d. we employ a counting lemma to this end which we find particularly useful in relation to varshamov graphs . since a varshamov graph consists of components corresponding to low weight vectors in the cosets of a code it is a useful tool when trying to improve the estimates involved in the varshamov bound . we consider how the graph can be iteratively constructed and using our observations are able to achieve a reduction in the over counting which occurs . this tightens the lower bound for any choice of parameters n , k , d or q and is not dependent on information such as the weight distribution of a code .",
    "target": "varshamov graph;varshamov bound;greedy codes"
  },
  {
    "source": "forecasting of circuit breaker behaviour in high voltage electrical power systems necessity for future maintenance management . <eos> two research projects were started in order to investigate new methods of maintenance management . the first project was finished in april <digit> , dealing with the problem of relating the information of individual devices with their importance in the complete system . the combination of both sets of information is the aim of reliability centred maintenance ( rcm ) . more important devices can be maintained more frequently than those of less importance , leading to reduced maintenance costs but retaining a high level of reliability of the system . the question of how new methods of maintenance influence behaviour in future , can not be answered right now . the forecasting of the behaviour of circuit breakers will now be investigated in a further project in the field of maintenance management . a software model will be developed in strong co ordination with partners of some electrical power system utilities . the model shall include the simulation of the behaviour of circuit breakers in the future regarding maintenance activities as well as the operational stresses of the present . this paper will give an overview of the actual activities and aims of the project . main activities have been the definition of investigated circuit breaker types and the methodology for the starting steps of the project .",
    "target": "maintenance management;state forecasting"
  },
  {
    "source": "quality as empowerment going around in circles . <eos> the article introduces a new international educational community based on students quality circles , in which industry and education have learned to collaborate for mutual benefit . in each country represented in this special issue , there have been distinctive bottom up initiatives , informed by the experience of collaboration . we emphasise quality as empowerment .",
    "target": "empowerment;students quality circles;bottom up;collaborative advantage;compliance;continuous improvement;top down"
  },
  {
    "source": "a computer vision based precision seed drill guidance assistance . <eos> this paper presents a control mechanism aiming to position seed drills relative to the previous lines , while sowing . the position was measured by a machine vision system and used in a feedback control loop . an articulated mechanism was used to ensure the lateral displacement of the drill relative to the tractor . the behaviour of the whole outfit was studied during several field tests . the standard deviation of the error , measured as the difference between the observed inter row distance and its set value , was <digit> mm and its range was less than <digit> mm , which was sufficient to fulfil the requirements of the application . sources of systematic errors were also identified as linked to the geometric considerations . their correction requires an accurate mounting of the camera , which may be possible for a serial montage .",
    "target": "seed drill;machine vision;automatic guidance;hough transform"
  },
  {
    "source": "rulebased regulatory and metabolic model for quorum sensing in p. aeruginosa . <eos> in the pathogen p. aeruginosa , the formation of virulence factors is regulated via quorum sensing signaling pathways . due to the increasing number of strains that are resistant to antibiotics , there is a high interest to develop novel antiinfectives . in the combat of resistant bacteria , selective blockade of the bacterial celltocell communication ( quorum sensing ) has gained special interest as antivirulence strategy . here , we modeled the las , rhl , and pqs quorum sensing systems by a multilevel logical approach to analyze how enzyme inhibitors and receptor antagonists effect the formation of autoinducers and virulence factors .",
    "target": "quorum sensing;multilevel logical approach;inhibitor;boolean network;generegulatory network;pseudomonas aeruginosa;pqs system"
  },
  {
    "source": "strong and ultra separation axioms on fuzzy bitopological spaces . <eos> given a fuzzy bitopological space ( x , tau ( <digit> ) , tau ( <digit> ) ) , we introduce a new notion of fuzzy pairwise separation axioms by using the family of its level bitopologies l alpha ( tau ( <digit> ) ) , l alpha ( tau ( <digit> ) ) , alpha is an element of 0,1 ) . we prove that these concepts are good extension and we compare them with its corresponding fpti ( kandil and el shafee , <digit> ) and fpt ( i ) ( abu safiya et al. , <digit> ) ( i <digit> , 1,2 , <digit> , <digit> ) , respectively . we show that these notions are not equivalent and we give a number of examples which illustrate this fact . ( c ) <digit> elsevier science b.v. all rights reserved .",
    "target": "fuzzy bitopologicai spaces;alpha level bitopological spaces;strong fuzzy pairwise separation axioms;alpha bitopologically generated"
  },
  {
    "source": "secure and scalable mobility management scheme for the internet of things integration in the future internet architecture . <eos> internet of things is becoming a reality with the rapid development of communication technologies . this evolution presents an enrichment of the users ' experiences , but also challenges regarding network scalability , security , privacy vulnerabilities , and mobility support . mobility support for the future internet is focused on id locator split architectures since the limitations of the current internet . this work analyses the security challenges for the himalis ( heterogeneity inclusion and mobility adaptation through locator id separation ) architecture for the particularities from the internet of things and the id locator management messages vulnerable to attacks . this work proposes a secure and scalable mobility management scheme that considers the constraints from the internet of things , solving the possible security and privacy vulnerabilities of the himalis architecture . the proposed scheme supports scalable inter domain authentication and secure location update and binding transfer for the mobility process . the proposed scheme has been verified and evaluated successfully with the avispa framework .",
    "target": "security;mobility;internet of things;future internet architecture;privacy"
  },
  {
    "source": "influence of different shoulder elbow configurations on steering precision and steering velocity in automotive context . <eos> influence of posture on driving precision and steering velocity was investigated . arm posture influences steering precision and steering velocity . steering precision and velocity are significantly increased in mid positions . driver safety can be enhanced by implementing these data in the design process . subjective comfort rating confirmed experimental results .",
    "target": "steering precision;steering velocity;optimum driving posture"
  },
  {
    "source": "modelling procedures for directed network of data blocks . <eos> here are presented procedures for modelling data in a network . the methods are extensions of pca or pls regression to a forward network of data blocks . it is assumed that the data blocks are organised in a network such that one data block leads to one or more other data blocks . the procedures are stepwise ones . at each step a passage through the network is carried out . from the input weight vectors of the input or starting blocks , the score and loading vectors of all data blocks are computed . it is investigated if some score loading vectors are not significant . if some are , they are deleted and revised estimation of the input weights are carried out . when one step is finished , all data matrices are adjusted for score and loading vectors found . a new passage through the network is carried out on the reduced matrices . if no significant loading score vectors are found for a given set of input weights , the modelling stops . in case of one data block , the algorithm reduces to pca . in case of two data blocks it reduces to pls regression . most methods used in pca or pls regression can be applied to this procedure , e.g. , cross validation and re sampling procedures . it is pointed out , how these methods can be used to extend other regression methods than pca and pls regression to a network regression . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "pls;forward network;linear regression;path models;multi block data"
  },
  {
    "source": "a novel joint processing adaptive nonlinear equalizer using a modular recurrent neural network for chaotic communication systems . <eos> to eliminate nonlinear channel distortion in chaotic communication systems , a novel joint processing adaptive nonlinear equalizer based on a pipelined recurrent neural network ( jprnn ) is proposed , using a modified real time recurrent learning ( rtrl ) algorithm . furthermore , an adaptive amplitude rtrl algorithm is adopted to overcome the deteriorating effect introduced by the nesting process . computer simulations illustrate that the proposed equalizer outperforms the pipelined recurrent neural network ( prnn ) and recurrent neural network ( rnn ) equalizers . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "recurrent neural network;pipelined architecture;channel equalizer;chaotic signal"
  },
  {
    "source": "dynamics of connected vehicle systems with delayed acceleration feedback . <eos> acceleration based connected cruise control ( ccc ) is implemented for heterogeneous platoons . the ad hoc nature of wireless vehicle to vehicle ( v2v ) communication is exploited . the design is robust against variation of human parameters and is scalable for large systems . delays are used as design parameters in order to ensure string stability . it is demonstrated that acceleration feedback shall be used in a selective manner .",
    "target": "<digit>;<digit>"
  },
  {
    "source": "exact solution for nonlinear schrodinger equation by he 's frequency formulation . <eos> in this work , we apply he 's frequency formulation to search for the solution to nonlinear schrodinger equation . three examples are given and the solutions obtained are in good accordance with wazwaz 's solution abdul majid wazwaz , a study on linear and nonlinear schrodinger equations by the variational iteration method , chaos solitons fractals <digit> ( <digit> ) ( <digit> ) <digit> <digit> . it is shown that he 's frequency formulation is of utter straightforward and effective . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "exact solution;nonlinear schrodinger equation;he 's frequency formulation"
  },
  {
    "source": "on the role of trust in collaborative web search . <eos> recommender systems combine ideas from information retrieval , user modelling , and artificial intelligence to focus on the provision of more intelligent and proactive information services . as such , recommender systems play an important role when it comes to assisting the user during both routine and specialised information retrieval tasks . like any good assistant it is important that users can trust in the ability of a recommender system to respond with timely and relevant suggestions . in this paper , we will look at a collaborative recommendation system operating in the domain of web search . we will show how explicit models of trust can help to inform more reliable recommendations that translate into more relevant search results . moreover , we demonstrate how the availability of this trust model facilitates important interface enhancements that provide a means to declare the provenance of result recommendations in a way that will allow searchers to evaluate their likely relevance based on the reputation and trustworthiness of the recommendation partners behind these suggestions .",
    "target": "trust;collaborative web search;user modelling"
  },
  {
    "source": "an intelligent learning diagnosis system for web based thematic learning platform . <eos> this work proposes an intelligent learning diagnosis system that supports a web based thematic learning model , which aims to cultivate learners ' ability of knowledge integration by giving the learners the opportunities to select the learning topics that they are interested , and gain knowledge on the specific topics by surfing on the internet to search related learning courseware and discussing what they have learned with their colleagues . based on the log files that record the learners ' past online learning behavior , an intelligent diagnosis system is used to give appropriate learning guidance to assist the learners in improving their study behaviors and grade online class participation for the instructor . the achievement of the learners ' final reports can also be predicted by the diagnosis system accurately . our experimental results reveal that the proposed learning diagnosis system can efficiently help learners to expand their knowledge while surfing in cyberspace web based theme based learning model . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "learning diagnosis;theme based learning;web based learning;fuzzy expert system;k nearest neighbor;naive bayesian classifier;support vector machines"
  },
  {
    "source": "the longest common extension problem revisited and applications to approximate string searching . <eos> the longest common extension ( lce ) problem considers a string s and computes , for each pair ( i , j ) ( i , j ) , the longest substring of s that starts at both i and j. it appears as a subproblem in many fundamental string problems and can be solved by linear time preprocessing of the string that allows ( worst case ) constant time computation for each pair . the two known approaches use powerful algorithms either constant time computation of the lowest common ancestor in trees or constant time computation of range minimum queries in arrays . we show here that , from practical point of view , such complicated approaches are not needed . we give two very simple algorithms for this problem that require no preprocessing . the first is <digit> times faster than the best previous algorithms on the average whereas the second is faster on virtually all inputs . as an application , we modify the landauvishkin algorithm for approximate matching to use our simplest lce algorithm . the obtained algorithm is <digit> to <digit> times faster than the original . we compare it with the more widely used ukkonen 's cutoff algorithm and show that it behaves better for a significant range of error thresholds .",
    "target": "longest common extension;approximate string search;string;algorithm"
  },
  {
    "source": "lumbar spine segmentation using a statistical multi vertebrae anatomical shape plus pose model . <eos> segmentation of the spinal column from computed tomography ( ct ) images is a preprocessing step for a range of image guided interventions . one intervention that would benefit from accurate segmentation is spinal needle injection . previous spinal segmentation techniques have primarily focused on identification and separate segmentation of each vertebra . recently , statistical multi object shape models have been introduced to extract common statistical characteristics between several anatomies . these models can be used for segmentation purposes because they are robust , accurate , and computationally tractable . in this paper , we develop a statistical multi vertebrae shape pose model and propose a novel registration based technique to segment the ct images of spine . the multi vertebrae statistical model captures the variations in shape and pose simultaneously , which reduces the number of registration parameters . we validate our technique in terms of accuracy and robustness of multi vertebrae segmentation of ct images acquired from lumbar vertebrae of <digit> subjects . the mean error of the proposed technique is below <digit> mm , which is sufficient for many spinal needle injection procedures , such as facet joint injections .",
    "target": "segmentation;computed tomography;registration;multi vertebrae anatomical model;spinal intervention;statistical shape plus pose model"
  },
  {
    "source": "relative blocking in posets . <eos> poset theoretic generalizations of set theoretic committee constructions are presented . the structure of the corresponding subposets is described . sequences of irreducible fractions associated to the principal order ideals of finite bounded posets are considered and those related to the boolean lattices are explored it is shown that such sequences inherit all the familiar properties of the farey sequences .",
    "target": "poset;committee;lattice;farey sequence;antichain;blocker;blocker map;clutter"
  },
  {
    "source": "mixed finite elements for numerical weather prediction . <eos> we show how mixed finite element methods that satisfy the conditions of finite element exterior calculus can be used for the horizontal discretisation of dynamical cores for numerical weather prediction on pseudo uniform grids . this family of mixed finite element methods can be thought of in the numerical weather prediction context as a generalisation of the popular polygonal c grid finite difference methods . there are a few major advantages the mixed finite element methods do not require an orthogonal grid , and they allow a degree of flexibility that can be exploited to ensure an appropriate ratio between the velocity and pressure degrees of freedom so as to avoid spurious mode branches in the numerical dispersion relation . these methods preserve several properties of the c grid method when applied to linear barotropic wave propagation , namely ( a ) energy conservation , ( b ) mass conservation , ( c ) no spurious pressure modes , and ( d ) steady geostrophic modes on the f plane . we explain how these properties are preserved , and describe two examples that can be used on pseudo uniform grids the recently developed modified rtk q ( k <digit> ) element pairs on quadrilaterals and the bdfm1 p1dg p <digit> dg element pair on triangles . all of these mixed finite element methods have an exact <digit> <digit> ratio of velocity degrees of freedom to pressure degrees of freedom . finally we illustrate the properties with some numerical examples .",
    "target": "mixed finite elements;numerical weather prediction;stability;steady geostrophic states;geophysical fluid dynamics"
  },
  {
    "source": "dc offset control with application in a zero if 0.18 mu m cmos bluetooth receiver chain . <eos> a compact dc offset correction circuit based on the intrinsic properties of quasi floating gate ( qfg ) transistors is presented . the proposed scheme uses a tuning mechanism to make its initial response faster improving the traditional large settling time of these circuits . a zero if baseband receiver chain suitable for bluetooth that includes the proposed dc offset correction has been designed in a 0.18 mu m cmos technology at 1.2 v supply voltage .",
    "target": "dc offset;direct conversion receivers;low power and low voltage circuits;qfg transistors"
  },
  {
    "source": "combining accuracy and success rate to improve the performance of extended classifier system ( xcs ) for data mining and control applications . <eos> the emergence of extended classifier systems ( xcs ) raised the bar for learning classifier systems by incorporating the accuracies of the rules in the lcs 's traditional reinforcement mechanism . however , neither xcs nor its extensions take into account the nature of a classifier 's experience of attending the action set . we introduce an experienceevaluation mechanism that , once added to the traditional xcs , would assigns to each member of the action set a success rate indicating how effectively the classifier has contributed to the correct responding of the system to the environment 's queries . application of the augmented system ( called srxcs ) to several benchmark problems shows that the proposed mechanism enhances xcs ' classification capability and its rate of convergence at the same time . application results indicate that srxcs performs notably better on both pattern association and pattern recognition tasks . the applicability and efficiency of the proposed mechanism is further demonstrated through solving a fairly complex path planning problem for an autonomous mobile robot in a dynamic environment .",
    "target": "classifier systems;xcs;rule experience;rule elimination;reinforcement policy"
  },
  {
    "source": "an experimental study of field dependency in altered gz environments . <eos> failure to address extreme environments constraints at the human computer interaction level may lead to the commission of critical and potentially fatal errors . this experimental study addresses gaps in our current theoretical understanding of the impact of gz accelerations and field dependency independency on task performance in human computer interaction . it investigates the effects of gz accelerations and field dependency independency on human performance in the completion of perceptual motor tasks on a personal digital assistant ( pda ) . we report the results of a controlled experiment , conducted in an aerobatic aircraft under multiple gz conditions , showing that cognitive style significantly impacts latency and accuracy in target acquisition for perceptual motor tasks in altered gz environments and propose design guidelines as countermeasures . based on the results , we argue that developing design requirements taking into account cognitive differences in extreme environments will allow users to execute perceptual motor tasks efficiently without unnecessarily increasing cognitive load and the probability of critical errors .",
    "target": "extreme environments;target acquisition;mobile devices;perceptual style;comparative informatics"
  },
  {
    "source": "locality discriminating indexing for document classification . <eos> this paper introduces a locality discriminating indexing ( ldi ) algorithm for document classification . based on the hypothesis that samples from different classes reside in class specific manifold structures , ldi seeks for a projection which best preserves the within class local structures while suppresses the between class overlap . comparative experiments show that the proposed method isable to derives compact discriminating document representations for classification .",
    "target": "document classification;document indexing;manifold analysis"
  },
  {
    "source": "accelespell , a gestural interactive game to learn and practice finger spelling . <eos> in this paper , an interactive computer game for learning and practicing continuous fingerspelling is described . the game is controlled by an instrumented glove known as acceleglove and a recognition algorithm based on decision trees . the graphical user interface is designed to allow beginners to remember the correct hand shapes and start finger spelling words sooner than traditional methods of learning .",
    "target": "interactive games;finger spelling;instrumented gloves"
  },
  {
    "source": "a systematic optimization approach for assembly sequence planning using taguchi method , doe , and bpnn . <eos> research in assembly planning can be categorised into three types of approach graph based , knowledge based and artificial intelligence approaches . the main drawbacks of the above approaches are as follows the first is time consuming in the second approach it is difficult to find the optimal solution and the third approach requires a high computing efficiency . to tackle these problems , this study develops a novel approach integrated with some graph based heuristic working rules , robust back propagation neural network ( bpnn ) engines via taguchi method and design of experiment ( doe ) , and a knowledge based engineering ( kbe ) system to assist the assembly engineers in promptly predicting a near optimal assembly sequence . three real world examples are dedicated to evaluating the feasibility of the proposed model in terms of the differences in assembly sequences . the results show that the proposed model can efficiently generate bpnn engines , facilitate assembly sequence optimisation and allow the designers to recognise the contact relationships , assembly difficulties and assembly constraints of three dimensional ( 3d ) components in a virtual environment type .",
    "target": "assembly sequence planning;taguchi method;neural networks;design of experiment;assembly precedence diagrams"
  },
  {
    "source": "a fuzzy neural network controller with adaptive learning rates for nonlinear slider crank mechanism . <eos> a fuzzy neural network ( fnn ) controller with adaptive learning rates is proposed to control a nonlinear mechanism system in this study . first , the network structure and the on line learning algorithm of the fnn is described . to guarantee the convergence of the tracking error , analytical methods based on a discrete type lyapunov function are proposed to determine the adaptive learning rates of the fnn . next , a slider crank mechanism , which is driven by a permanent magnet ( pm ) synchronous motor , is studied as an example to demonstrate the effectiveness of the proposed control technique the fnn controller is implemented to control the slider position of the motor slider crank nonlinear mechanism . the robust control performance and learning ability of the proposed fnn controller with adaptive learning rates is demonstrated by simulation and experimental results .",
    "target": "fuzzy neural network;adaptive learning rates;slider crank mechanism;synchronous motor;position control"
  },
  {
    "source": "simulated analysis for ingap gaas heterostructure emitter bipolar transistor with ingaas gaas superlattice base structure . <eos> a novel ingap gaas heterostructure emitter bipolar transistor ( hebt ) with ingaas gaas superlattice base structure is proposed and demonstrated by two dimensional analysis . as compared with the traditional hebt , the studied superlattice base device exhibits a higher collector current , a higher current gain of <digit> , and a lower baseemitter ( be ) turn on voltage of 0.966 v at a current level of <digit> a , attributed to the increased charge storage of minority carriers in the ingaas gaas superlattice base region by tunneling behavior . the low turn on voltage can reduce the operating voltage and collectoremitter offset voltage for low power consumption in circuit applications .",
    "target": "ingap gaas;heterostructure emitter;superlattice base;turn on voltage;offset voltage"
  },
  {
    "source": "why it has become more difficult to predict nobel prize winners a bibliometric analysis of nominees and winners of the chemistry and physics prizes ( <digit> ) . <eos> we propose a comprehensive bibliometric study of the profile of nobel prize winners in chemistry and physics from <digit> to <digit> , based on citation data available over the same period . the data allows us to observe the evolution of the profiles of winners in the years leading up toand followingnominations and awarding of the nobel prize . the degree centrality and citation rankings in these fields confirm that the prize is awarded at the peak of the winners citation history , despite a brief halo effect observable in the years following the attribution of the prize . changes in the size and organization of the two fields result in a rapid decline of predictive power of bibliometric data over the century . this can be explained not only by the growing size and fragmentation of the two disciplines , but also , at least in the case of physics , by an implicit hierarchy in the most legitimate topics within the discipline , as well as among the scientists selected for the nobel prize . furthermore , the lack of readily identifiable dominant contemporary physicists suggests that there are few new paradigm shifts within the field , as perceived by the scientific community as a whole .",
    "target": "nobel prize;citation;centrality;scientific disciplines"
  },
  {
    "source": "an interactive documentation system . <eos> most chronic users of time sharing computer systems are familiar with programs that allow the creation and manipulation of text files . less often they have at their disposal programs that will format the document described by a text file , generating output such as a typist might produce . rarely is there any mechanism by which graphics can be integrated with text . lawrence livermore laboratory has a powerful , flexible and interactive computer based documentation system that will format a source document description according to user specifications and incorporate illustrations to produce online documents , offset reproduction masters , <digit> mm color slides , movie titles , or viewgraphs . the flexibility of the system is greatly enhanced by the use of a device independent graphics library . text may be plotted using the hardware characters specific to a device ( when possible ) , or may be drawn as hershey characters or polygonally outlined symbols . illustrations may be defined in a simple 2d graphics language , and graphical output from application programs may also be incorporated directly into a document .",
    "target": "interaction;documentation;systems;users;user;timing;sharing;computation;program;manipulation;text;graphics;integrability;laboratory;flexibility;online;color;use;device;libraries;hardware;character;language;applications;printing;documentation graphics;text processing;color graphics"
  },
  {
    "source": "robust time varying filtering and separation of some nonstationary signals in low snr environments . <eos> the proposed algorithm improves filtering performance for monocomponent signals . the proposed algorithm separates multicomponent signals into individual components . the requirement of high sampling rates is significantly relaxed . the proposed algorithm is implemented with low complexity .",
    "target": "time varying filtering;multi component separation;instantaneous frequency estimation;sinusoidal timefrequency distribution"
  },
  {
    "source": "towards a general neural controller for quadrupedal locomotion . <eos> our study aims at the design and implementation of a general controller for quadruped locomotion , allowing the robot to use the whole range of quadrupedal gaits ( i.e.from low speed walking to fast running ) . a general legged locomotion controller must integrate both posture control and rhythmic motion control and have the ability to shift continuously from one control method to the other according to locomotion speed . we are developing such a general quadrupedal locomotion controller by using a neural model involving a cpg ( central pattern generator ) utilizing ground reaction force sensory feedback . we used a biologically faithful musculoskeletal model with a spine and hind legs , and computationally simulated stable stepping motion at various speeds using the neuro mechanical system combining the neural controller and the musculoskeletal model . we compared the changes of the most important locomotion characteristics ( stepping period , duty ratio and support length ) according to speed in our simulations with the data on real cat walking . we found similar tendencies for all of them . in particular , the swing period was approximately constant while the stance period decreased with speed , resulting in a decreasing stepping period and duty ratio . moreover , the support length increased with speed due to the posterior extreme position that shifted progressively caudally , while the anterior extreme position was approximately constant . this indicates that we succeeded in reproducing to some extent the motion of a cat from the kinematical point of view , even though we used a 2d bipedal model . we expect that such computational models will become essential tools for legged locomotion neuroscience in the future .",
    "target": "neural controller;quadruped;posture;rhythmic motion;cpg;computational simulation"
  },
  {
    "source": "on the superlinear local convergence of a filter sqp method . <eos> transition to superlinear local convergence is shown for a modified version of the trust region filter sqp method for nonlinear programming introduced by fletcher , leyffer , and toint <digit> . hereby , the original trust region sqp steps can be used without an additional second order correction . the main modification consists in using the lagrangian function value instead of the objective function value in the filter together with an appropriate infeasibility measure . moreover , it is shown that the modified trust region filter sqp method has the same global convergence properties as the original algorithm in <digit> .",
    "target": "filter;sqp;nonlinear programming;global convergence;superlinear convergence"
  },
  {
    "source": "multiway covariates regression models . <eos> an abundance of methods exist to regress a y variable on a set of x variables collected in a matrix x. in the chemical sciences a growing number of problems translate into arrays of measurements x and y , where x and y are three way arrays or multiway arrays . in this paper a general model is described for regressing such a multiway y on a multiway x , while taking into account three way structures in x and y. a global least squares optimization problem is formulated to estimate the parameters of the model . the model is described and illustrated with a real industrial example from batch process operation . an algorithm is given in an appendix . copyright ( c ) <digit> john wiley sons , ltd .",
    "target": "principal covariates regression;partial least squares;multilinear pls;three way methods;multiway methods"
  },
  {
    "source": "a note on the iterative object symmetry transform . <eos> this paper introduces a new operator named the iterated object transform that is computed by combining the object symmetry transform with the morphological operator erosion . this new operator has been applied on both binary and gray levels images showing the ability to grasp the internal structure of a digital object . we present also some experiments on artificial and real images and potential applications .",
    "target": "symmetry transforms;mathematical morphology;image classification;feature extraction"
  },
  {
    "source": "an anelastic allspeed projection method for gravitationally stratified flows . <eos> this paper looks at gravitationally stratified atmospheric flows at low mach and fronde numbers and proposes a new algorithm to solve the compressible euler equations , in which the asymptotic limits are recovered numerically and the boundary conditions for block structured local refinement methods are well posed . the model is non hydrostatic and the numerical algorithm uses a splitting to separate the fast acoustic dynamics from the slower anelastic dynamics . the acoustic waves are treated implicitly while the anelastic dynamics is treated semi implicitly and an embedded boundary method is used to represent orography . we present an example that verifies our asymptotic analysis and a set of results that compares very well with the classical gravity wave results presented by durran . ( c ) <digit> elsevier inc. all rights reserved .",
    "target": "projection method;embedded boundary method;gravity waves;non hydrostatic atmospheric model"
  },
  {
    "source": "improvement of cardiac ct reconstruction using local motion vector fields . <eos> the motion of the heart is a major challenge for cardiac imaging using ct. a novel approach to decrease motion blur and to improve the signal to noise ratio is motion compensated reconstruction which takes motion vector fields into account in order to correct motion . the presented work deals with the determination of local motion vector fields from high contrast objects and their utilization within motion compensated filtered back projection reconstruction . image registration is applied during the quiescent cardiac phases . temporal interpolation in parameter space is used in order to estimate motion during strong motion phases . the resulting motion vector fields are during image reconstruction . the method is assessed using a software phantom and several clinical cases for calcium scoring . as a criterion for reconstruction quality , calcium volume scores were derived from both , gated cardiac reconstruction and motion compensated reconstruction throughout the cardiac phases using low pitch helical cone beam ct acquisitions . the presented technique is a robust method to determine and utilize local motion vector fields . motion compensated reconstruction using the derived motion vector fields leads to superior image quality compared to gated reconstruction . as a result , the gating window can be enlarged significantly , resulting in increased snr , while reliable hounsfield units are achieved due to the reduced level of motion artefacts . the enlargement of the gating window can be translated into reduced dose requirements .",
    "target": "cardiac ct;motion compensated reconstruction;calcium scoring;motion model"
  },
  {
    "source": "sharing the costs of maintaining environmental resources a comparison of different programmes . <eos> suppose state a controls some resource such as a rainforest and there are some other agents in the international system that wish to see this resource preserved . these agents are prepared to make a contribution towards sharing the costs of maintaining the resource . what would be the long term trajectory of the resource level under different programmes what type of cost sharing programme would maintain the highest level of the resource which programme would give the best value for money for the contributing player this paper attempts to answer these questions . this is done by examining a dynamic model with an infinite time horizon .",
    "target": "rainforests;resource preservation;dynamic models;environment;international environmental agreements;optimal control;debt for equity"
  },
  {
    "source": "regression based d optimality experimental design for sparse kernel density estimation . <eos> this paper derives an efficient algorithm for constructing sparse kernel density ( skd ) estimates . the algorithm first selects a very small subset of significant kernels using an orthogonal forward regression ( ofr ) procedure based on the d optimality experimental design criterion . the weights of the resulting sparse kernel model are then calculated using a modified multiplicative nonnegative quadratic programming algorithm . unlike most of the skd estimators , the proposed d optimality regression approach is an unsupervised construction algorithm and it does not require an empirical desired response for the kernel selection task . the strength of the d optimality ofr is owing to the fact that the algorithm automatically selects a small subset of the most significant kernels related to the largest eigenvalues of the kernel design matrix , which counts for the most energy of the kernel training data , and this also guarantees the most accurate kernel weight estimate . the proposed method is also computationally attractive , in comparison with many existing skd construction algorithms . extensive numerical investigation demonstrates the ability of this regression based approach to efficiently construct a very sparse kernel density estimate with excellent test accuracy , and our results show that the proposed method compares favourably with other existing sparse methods , in terms of test accuracy , model sparsity and complexity , for constructing kernel density estimates .",
    "target": "d optimality;optimal experimental design;orthogonal forward regression;sparse kernel modelling;probability density function;parzen window estimate"
  },
  {
    "source": "pulmonary nodule registration in serial ct scans using global rib matching and nodule template matching . <eos> we propose an automatic nodule registration method between baseline and follow up chest ct scans . initial alignment using the center of the lung volume corrects the gross translational mismatch , and rigid registration using coronal and sagittal maximum intensity projection images effectively refines the rigid motion of the lungs . nodule correspondences are established by finding the most similar region in terms of density as well as the geometrical constraint . the proposed nodule registration method increased the nodule hit rate ( the ratio of the number of successfully matched nodules to total nodule number ) from <digit> % to <digit> % .",
    "target": "pulmonary nodule registration;rib matching;nodule template matching;computed tomography;follow up ct study;geometrical constraint using log polar image"
  },
  {
    "source": "robotics in special needs education . <eos> the purpose of this study is to explore the potential of robotics as an educational tool in special needs education . qualitative case studies are used to increase knowledge about programmable lego nxt and topobo robotics constructions kits in special needs education , and about the social robot and topobo that are used in early childhood education when possible learning disabilities have not yet been diagnosed . this study aims to provide suggestions about how robotics might be used to recognize disabilities at an early stage of education and to compensate for them in learning .",
    "target": "robotics;special needs education;social robot;educational technology;programmable construction kit"
  },
  {
    "source": "applying fisher 's filter to select kdd connections ' features and using neural networks to classify and detect attacks . <eos> most of the neural networks based intrusion detection systems ( ids ) examine all data features to detect intrusion or misuse patterns . some of the features may be redundant or contribute little ( if anything ) to the detection process . that is why the purpose of this study is to identify important kdd features which will be used to train a neural network ( nn ) , in order to best classify and detect attacks . four nns were studied modular , recurrent , principal component analysis ( pca ) , and time lag recurrent ( tlr ) nns . we investigated the performance of combining the fisher 's filter used as a feature selection technique , with one of the previously cited nns . our simulations show that using fisher 's filter improves largely the performance of the four considered nns in terms of detection rate , attack classification , and computational time .",
    "target": "neural networks;intrusion detection systems;misuse detection;fisher 's anova ranking;knowledge discovery and data mining dataset;kdd feature reduction"
  },
  {
    "source": "tight upper bounds on the minimum precision required of the divisor and the partial remainder in high radix division . <eos> digit recurrence binary dividers are sped up via two complementary methods keeping the partial remainder in redundant form and selecting the quotient digits in a radix higher than <digit> . use of a redundant partial remainder replaces the standard addition in each cycle by a carry free addition , thus making the cycles shorter . deriving the quotient in high radix reduces the number of cycles ( by a factor of about h for radix <digit> ( h ) ) . to make the redundant partial remainder scheme work , quotient digits must be chosen from a redundant set , such as <digit> , <digit> in radix <digit> . the redundancy provides some tolerance to imprecision so that the quotient digits can be selected based on examining truncated versions of the partial remainder and divisor . no closed form formula for the required precision in the partial remainder and divisor , as a function of the quotient digit set and the range of the partial remainders is known . in this paper , we establish tight upper bounds on the required precision for the partial remainder and divisor . the bounds are tight in the sense that each is only one bit over a well known simple lower bound . we also discuss the implications of these bounds for the quotient digit selection process .",
    "target": "high radix division;quotient digit selection;digit recurrence division;digit selector pla;p d plot;srt division"
  },
  {
    "source": "the effect of viewing angle on wrist posture estimation from photographic images using novice raters . <eos> observational assessment of wrist posture using photographic methods is theoretically affected by camera view angle . a study was conducted to investigate whether wrist flexion extension and radial ulnar deviation postures were estimated differently by raters depending on the viewing angle and compared to predictions using a quantitative 2d model of parallax . novice raters ( n <digit> ) estimated joint angles from images of wrist postures photographed from ten different viewing angles . results indicated that ideal views , orthogonal to the plane of motion , produced more accurate estimates of posture compared to non ideal views . the neutral ( <digit> ) posture was estimated the most accurately even at different viewing angles . raters were more accurate than model predictions . findings demonstrate a need for more systematic methods for collecting and analyzing photographic data for observational studies of posture . renewed caution in interpreting existing studies of wrist posture where viewing angle was not controlled is advised .",
    "target": "viewing angle;wrist;posture;parallax"
  },
  {
    "source": "new proposals for the design of steel beam columns in case of fire , including a new approach for the lateraltorsional buckling . <eos> the possibility of having , in parts <digit> <digit> and <digit> <digit> of eurocode <digit> , the same approach for the design of beam columns and for lateraltorsional buckling , was investigated by the authors in previous papers using a numerical approach , where it was concluded that those assumptions could be made . in the present paper , a new approach for lateraltorsional buckling has been used with the formulae for the design of beam columns at elevated temperature based on pren <digit> <digit> <digit> combined with the formulae from pren <digit> <digit> <digit> . in both cases the results obtained are much better than the current design expressions , when compared with those obtained in the numerical calculations .",
    "target": "steel;beam column;fire;lateraltorsional buckling;eurocode <digit>;numerical modelling"
  },
  {
    "source": "a note on the article fuzzy less strongly semiopen sets and fuzzy less strong semicontinuity . <eos> in this note we show that some results in the article by fang jing ming are incorrect . ( c ) <digit> elsevier science b.v. all rights reserved .",
    "target": "fuzzy topology;fuzzy strongly semiopen set"
  },
  {
    "source": "differential fault analysis on camellia . <eos> camellia is a <digit> bit block cipher published by ntt and mitsubishi in <digit> . on the basis of the byte oriented model and the differential analysis principle , we propose a differential fault attack on the camellia algorithm . mathematical analysis and simulating experiments show that our attack can recover its <digit> bit , <digit> bit or <digit> bit secret key by introducing <digit> faulty ciphertexts . thus our result in this study describes that camellia is vulnerable to differential fault analysis . this work provides a new reference to the fault analysis of other block ciphers .",
    "target": "differential fault analysis;camellia;block ciphers;side channel attacks"
  },
  {
    "source": "the l ( 2,1 ) l ( <digit> , <digit> ) labeling of unigraphs . <eos> the l ( 2,1 ) l ( <digit> , <digit> ) labeling problem consists of assigning colors from the integer set <digit> , , <digit> , , to the nodes of a graph g g in such a way that nodes at a distance of at most two get different colors , while adjacent nodes get colors which are at least two apart . the aim of this problem is to minimize and it is in general np complete . in this paper the problem of l ( 2,1 ) l ( <digit> , <digit> ) labeling unigraphs , i.e. graphs uniquely determined by their own degree sequence up to isomorphism , is addressed and a <digit> <digit> <digit> <digit> approximate algorithm for l ( 2,1 ) l ( <digit> , <digit> ) labeling unigraphs is designed . this algorithm runs in o ( n ) o ( n ) time , improving the time of the algorithm based on the greedy technique , requiring o ( m ) o ( m ) time , that may be near to ( n2 ) ( n <digit> ) for unigraphs .",
    "target": "l ( <digit>;<digit> );unigraphs;<digit> ) l ( <digit>;<digit> ) labeling l ( <digit>;<digit> ) l ( <digit>;<digit> ) l ( <digit>;<digit> ) l ( <digit>;<digit> ) l ( <digit>;frequency assignment"
  },
  {
    "source": "ground control station embedded mission planning for uas . <eos> as the unmanned aerial system ( uas ) level of automation increases , mission planning relevance raises . a mission plan can be defined as all the information needed to reach the assigned goals , and it is composed by several sub plans . in particular , the mission plan core is represented by the routes . since the route creation process is very complex , the introduction of route creation and verification algorithms is required . these algorithms enhance the crew replan performances during the mission execution , and permit to implement autonomous on board replanning . furthermore , planning replanning processes could also have a key role in the integration of uas in the civil airspace . according to these considerations , a mission planner embedded in the alenia aermacchi uas ground control station ( gcs ) has been developed , comprised of advanced planning algorithms .",
    "target": "mission plan;route creation validation algorithms;stanag <digit>"
  },
  {
    "source": "automatic frechet differentiation for the numerical solution of boundary value problems . <eos> a new solver for nonlinear boundary value problems ( bvps ) in matlab is presented , based on the chebfun software system for representing functions and operators automatically as numerical objects . the solver implements newton 's method in function space , where instead of the usual jacobian matrices , the derivatives involved are frechet derivatives . a major novelty of this approach is the application of automatic differentiation ( ad ) techniques to compute the operator valued frechet derivatives in the continuous context . other novelties include the use of anonymous functions and numbering of each variable to enable a recursive , delayed evaluation of derivatives with forward mode ad . the ad techniques are applied within a new chebfun class called chebop which allows users to set up and solve nonlinear bvps , both scalar and systems of coupled equations , in a few lines of code , using the nonlinear backslash operator ( ) . this framework enables one to study the behaviour of newton 's method in function space .",
    "target": "chebfun;newton 's method in function space;algorithms;design;performance;linearization of boundary value problems;object oriented matlab"
  },
  {
    "source": "asynchronous parallel finite automaton a new mechanism for deep packet inspection in cloud computing . <eos> security is quite an important issue in cloud computing . the general security mechanisms applied in the cloud are always passive defense methods such as encryption . besides these , it 's necessary to utilize real time active monitoring , detection and defense technologies . according to the published researches , deep packets inspection ( dpi ) is the most effective technology to realize active inspection and defense . however , most of the works on dpi focus on its performance in general application scenarios and make improvement for space reduction , which could not meet the demands of high speed and stability in the cloud . therefore it is meaningful to improve the common mechanisms of dpi , making it more suitable for cloud computing . in this paper , an asynchronous parallel finite automaton ( fa ) is proposed . the applying of asynchronous parallelization and heuristic forecast mechanism decreases the time consumed in matching significantly , while still reduces the memory required . moreover , it is immune to overlapping problem , also enhancing the stability . the final evaluation results show that asynchronous parallel fa has higher stability , better performance on both time and memory , and is more suitable for cloud computing .",
    "target": "asynchronous parallel finite automaton;deep packet inspection;cloud computing;lock free fifo"
  },
  {
    "source": "dynamic of a non autonomous predatorprey system with infinite delay and diffusion . <eos> in the present paper , a nonlinear non autonomous predatorprey dispersion model with continuous delay is studied . sufficient conditions which guarantee the existence of a periodic positive solution are obtained by using gaines and mawhins continuation theorem of coincidence degree theory . moreover , globally asymptotically stability of the system is also obtained by means of a suitable lyapunov functional . the applications show that these criteria are easily verified .",
    "target": "dispersion;time delay;periodic solution;persistent;globally asymptotically stable"
  },
  {
    "source": "secure and lightweight network admission and transmission protocol for body sensor networks . <eos> a body sensor network ( bsn ) is a wireless network of biosensors and a local processing unit , which is commonly referred to as the personal wireless hub ( pwh ) . personal health information ( phi ) is collected by biosensors and delivered to the pwh before it is forwarded to the remote healthcare center for further processing . in a bsn , it is critical to only admit eligible biosensors and pwh into the network . also , securing the transmission from each biosensor to pwh is essential not only for ensuring safety of phi delivery , but also for preserving the privacy of phi . in this paper , we present the design , implementation , and evaluation of a secure network admission and transmission subsystem based on a polynomial based authentication scheme . the procedures in this subsystem to establish keys for each biosensor are communication efficient and energy efficient . moreover , based on the observation that an adversary eavesdropping in a bsn faces inevitable channel errors , we propose to exploit the adversary 's uncertainty regarding the phi transmission to update the individual key dynamically and improve key secrecy . in addition to the theoretical analysis that demonstrates the security properties of our system , this paper also reports the experimental results of the proposed protocol on resource limited sensor platforms , which show the efficiency of our system in practice .",
    "target": "security;network admission and transmission;body sensor networks;efficiency;key update"
  },
  {
    "source": "time domain orthogonal finite element reduction recovery method for electromagnetics based analysis of large scale integrated circuit and package problems . <eos> a time domain orthogonal finite element reduction recovery method is developed to overcome the large problem sizes encountered in the simulation of large scale integrated circuit and package problems . in this method , a set of orthogonal prism vector basis functions is developed . based on this set of bases , an arbitrary <digit> d multilayered system such as a combined package and die is reduced to a single layer system with negligible computational cost . more importantly , the reduced single layer system is diagonal and , hence , can be solved readily . from the solution of the reduced system , the solution of the other unknowns is recovered in linear complexity . the method entails no theoretical approximation . it applies to any arbitrarily shaped multilayer structure involving inhomogeneous materials or any structure that can be geometrically modeled by triangular prism elements . in addition , it permits nonlinear device modeling and broadband simulation within one run . numerical and experimental results have demonstrated its accuracy and high capacity in simulating on chip , package , and die package interface problems .",
    "target": "time domain;large scale;package;on chip;die package cosimulation;electromagnetic simulation;finite element methods"
  },
  {
    "source": "the role of social network sites in romantic relationships effects on jealousy and relationship happiness . <eos> on social network sites ( sns ) , information about one 's romantic partner is readily available and public for friends . the paper focuses on the negative ( sns jealousy ) and positive ( sns relationship happiness ) consequences of sns use for romantic relationships . we examined whether relationship satisfaction , trait jealousy , sns use and need for popularity predicted these emotional consequences of sns use and tested the moderating role of self esteem . for low self esteem individuals , need for popularity predicted jealousy and relationship happiness . for high self esteem individuals , sns use for grooming was the main predictor . low self esteem individuals try to compensate their low self esteem by creating an idealized picture . undesirable information threatens this picture , and especially individuals with a high need for popularity react with sns jealousy .",
    "target": "networks;relationship;interpersonal;psychological factors"
  },
  {
    "source": "nonlinear data projection on non euclidean manifolds with controlled trade off between trustworthiness and continuity . <eos> this paper presents a framework for nonlinear dimensionality reduction methods aimed at projecting data on a non euclidean manifold , when their structure is too complex to be embedded in an euclidean space . the methodology proposes an optimization procedure on manifolds to minimize a pairwise distance criterion that implements a control of the trade off between trustworthiness and continuity , two criteria that , respectively , represent the risks of flattening and tearing the projection . the methodology is presented as general as possible and is illustrated in the specific case of the sphere .",
    "target": "trade off between trustworthiness and continuity;nonlinear dimensionality reduction;distance based data projection method;optimization on manifolds"
  },
  {
    "source": "quantum computation for action selection using reinforcement learning . <eos> this paper proposes a novel action selection method based on quantum computation and reinforcement learning ( rl ) . inspired by the advantages of quantum computation , the state action in a rl system is represented with quantum superposition state . the probability of action eigenvalue is denoted by probability amplitude , which is updated according to rewards . and the action selection is carried out by observing quantum state according to collapse postulate of quantum measurement . the results of simulated experiments show that quantum computation can be effectively used to action selection and decision making through speeding up learning . this method also makes a good tradeoff between exploration and exploitation for rl using probability characteristics of quantum theory .",
    "target": "quantum computation;action selection;reinforcement learning;grover iteration"
  },
  {
    "source": "principal agent theory and its application to analyze outsourcing of software development . <eos> much has been written on process models , project management or tool support to increase the return on investment in software through higher quality of the development process and the resulting software or system . yet , we lack understanding in the underlying economic principles e.g. , an external firm paid to develop software for someone else tries to maximize their own profit instead of the contractor 's . these divergences of interests result in projects that consume more time and money and meet fewer requirements than expected . in this paper , we try to fill the gap by providing an insight into the theory and presenting applicable suggestions how to diminish or avoid the problems that arise when selecting the ' best ' contractor and during the project . basic advises on the formulation of contracts can be derived .",
    "target": "principal agent theory;outsourcing"
  },
  {
    "source": "saccular projections in the human cerebral cortex . <eos> abstract the cerebral cortical areas processing saccular information were investigated in human subjects using the fmri method and loud clicks , which selectively activate the saccule . the results were compared with previous vestibular evoked potential ( vep ) studies in anesthetized patients following vestibular nerve stimulation . nine normal subjects participated in fmri studies . by comparing the cortical areas activated by a click at <digit> db ( auditory activation ) with those activated by <digit> db ( auditory plus saccular activation ) , the following cortical areas were selectively activated by saccular stimulation intraparietal sulcus , frontal eye fields , prefrontal cortex , and postcentral gyrus , in addition to insula , supplementary motor area , and anterior and posterior cingulate cortex . previous vep studies also revealed similar activation areas by vestibular nerve stimulation with latencies at <digit> ms , suggesting that the shortest pathways for activation of cerebral cortical neurons from the labyrinth are trisynaptic , with a relay in the thalamus . the activated areas are also consistent with results in previous studies using caloric stimulation , which primarily activates horizontal semicircular canals . these results suggest that canal and otolith information is processed largely by similar cortical areas in humans . multiple cortical areas activated by these studies suggest that these areas are involved in different aspects of processing vestibular information . the saccular projections to the prefrontal and frontal cortex suggest that these areas are involved in planning motor synergies to counteract loss of equilibrium .",
    "target": "human;fmri;vestibular evoked potentials;prefrontal"
  },
  {
    "source": "state density functions over dbm domains in the analysis of non markovian models . <eos> quantitative evaluation of models with generally distributed transitions requires the analysis of non markovian processes that may be not isomorphic to their underlying untimed models and may include any number of concurrent nonexponential timers . the analysis of stochastic time petri nets ( stpns ) copes with the problem by covering the state space with stochastic classes , which extend the theory of difference bounds matrix ( dbm ) with a state probability density function . as a core step , the analysis process requires symbolic manipulation of density functions supported over dbm domains . we characterize and engineer the critical steps of this derivation . we first show that the state density function accepts a continuous piecewise representation over a partition in dbm shaped subdomains . we then develop a closed form symbolic calculus of state density functions under the assumption that transitions in the stpn model have expolynomial distributions over possibly bounded intervals . the calculus shows that within each subdomain , the state density function is a multivariate expolynomial function , and it makes explicit the way in which this form evolves and grows in complexity as the state accumulates memory through subsequent transitions . this enables an efficient implementation of the analysis process and provides the formal basis that supports the introduction of an imprecise analysis based on the approximation of state density functions through bernstein polynomials . the approximation attacks practical and theoretical limits in the applicability of stochastic state classes and devises a new approach to the analysis of non markovian models , relying on approximations in the state space rather than in the structure of the model .",
    "target": "quantitative evaluation;stochastic time petri nets;difference bounds matrix;bernstein polynomials;correctness verification;performance and dependability;dense time state space analysis;markov renewal theory;approximate state space representation;density function approximation"
  },
  {
    "source": "reanalysis and sensitivity reanalysis by combined approximations . <eos> one of the main obstacles in the solution of structural optimization problems is the need to repeat solutions of the analysis and sensitivity analysis equations . in large scale structures , having complex analysis models , the computational effort may become prohibitive . to alleviate this difficulty a general approach for repeated analysis and repeated sensitivity analysis , called combined approximations , was developed during the last <digit> years . the solution is based on the integration of several algorithms and methods . as a result , accurate results can be achieved efficiently . in previous studies , solution procedures for various particular problems were developed . this article summarizes the various formulations and solution procedures for reanalysis and sensitivity reanalysis of linear , nonlinear , static and dynamic systems . it is shown that the various solution procedures are based on applications of similar basic algorithms . numerical examples demonstrate the efficiency of the calculations and the accuracy of the results .",
    "target": "sensitivity reanalysis;combined approximations;approximate reanalysis;efficient reanalysis"
  },
  {
    "source": "making on line logistics training sustainable through e learning . <eos> the purpose of this study is to investigate the possibility of using an online logistics certification learning environment as a training tool to equip future logisticians with required logistics skills . this study incorporates an online logistics certification website that was constructed for college students to familiarize themselves with the certification . in addition , this study also performed comparison tests on students before and after their interaction with the web based learning environment system to ascertain the systems effectiveness . our findings suggest that such a system might motivate students to familiarize themselves with logistics related certification information and can enhance students professional capabilities . in addition , the web based learning environment might possibly motivate students to join logistics related industries in the future .",
    "target": "logistics;training;e learning;motivation;capability;self learning"
  },
  {
    "source": "panorama weaving fast and flexible seam processing . <eos> a fundamental step in stitching several pictures to form a larger mosaic is the computation of boundary seams that minimize the visual artifacts in the transition between images . current seam computation algorithms use optimization methods that may be slow , sequential , memory intensive , and prone to finding suboptimal solutions related to local minima of the chosen energy function . moreover , even when these techniques perform well , their solution may not be perceptually ideal ( or even good ) . such an inflexible approach does not allow the possibility of user based improvement . this paper introduces the panorama weaving technique for seam creation and editing in an image mosaic . first , panorama weaving provides a procedure to create boundaries for panoramas that is fast , has low memory requirements and is easy to parallelize . this technique often produces seams with lower energy than the competing global technique . second , it provides the first interactive technique for the exploration of the seam solution space . this powerful editing capability allows the user to automatically extract energy minimizing seams given a sparse set of constraints . with a variety of empirical results , we show how panorama weaving allows the computation and editing of a wide range of digital panoramas including unstructured configurations .",
    "target": "digital panoramas;panorama editing;panorama seams;interactive image boundaries"
  },
  {
    "source": "mining service abstractions ( nier track ) . <eos> several lines of research rely on the concept of service abstractions to enable the organization , the composition and the adaptation of services . however , what is still missing , is a systematic approach for extracting service abstractions out of the vast amount of services that are available all over the web . to deal with this issue , we propose an approach for mining service abstractions , based on an agglomerative clustering algorithm . our experimental findings suggest that the approach is promising and can serve as a basis for future research .",
    "target": "services;agglomerative clustering;abstraction recovery"
  },
  {
    "source": "average voice based speech synthesis using hsmm based speaker adaptation and adaptive training . <eos> in speaker adaptation for speech synthesis , it is desirable to convert both voice characteristics and prosodic features such as f0 and phone duration . for simultaneous adaptation of spectrum , f0 and phone duration within the hmm framework , we need to transform not only the state output distributions corresponding to spectrum and f0 but also the duration distributions corresponding to phone duration . however , it is not straightforward to adapt the state duration because the original hmm does not have explicit duration distributions . therefore , we utilize the framework of the hidden semi markov model ( hsmm ) , which is an hmm having explicit state duration distributions , and we apply an hsmm based model adaptation algorithm to simultaneously transform both the state output and state duration distributions . furthermore , we propose an hsmm based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model . we incorporate these techniques into our hsmm based speech synthesis system , and show their effectiveness from the results of subjective and objective evaluation tests .",
    "target": "speaker adaptation;hidden semi markov model;hmm based speech synthesis;speaker adaptive training;maximum likelihood linear regression;voice conversion"
  },
  {
    "source": "a two parameter continuation algorithm for vortex pinning in rotating boseeinstein condensates . <eos> we describe an efficient two parameter continuation algorithm combined with spectral collocation methods for computing the ground state and central vortex state solutions of rotating boseeinstein condensates in optical lattices , where the first kind and second kind chebyshev polynomials are used as the basis functions for the trial function space . by treating the chemical potential and angular velocity as the continuation parameters simultaneously under the additional constraint of normalization condition , the proposed algorithm can effectively compute numerical solutions for a rich variety of physical phenomena observed in physical experiments with very little cost . comparisons with various numerical methods on some sample test problems are reported .",
    "target": "spectral collocation methods;grosspitaevskii equation;ground state solution"
  },
  {
    "source": "complexity and endogeneity in economic modeling . <eos> purpose the concepts of complexity , endogeneity and circular causation myrdal 's term was cumulative causation are shown to be interrelated ones in configuring an economic model in the framework of systemic embedding and its empirical application . design methodology approach the ensuing framework of economic modeling with complexity provides a controllable and predictable overarching worldview . anomie in the economic universe and its embedded world system are analytically rejected . this consequence is due to the epistemic nature of modeling that combines complexity , endogeneity , and circular causation for attaining predictability and controllability , even in the face of complex systemic perturbations . the epistemology of unity of knowledge contrasted with rationalism is treated as the foundational worldview . an illustrative empirical work is given to convey the conceptual model and its applied viability . findings both the theoretical and empirical results point out how the induced effects of knowledge flows in reference to the epistemology of unity of knowledge continuously improves the complementary relationships of the evolutionary learning fields , and rejects marginalism as being logically non sequiter in such epistemic systems . research limitations implications more variables and data would increase the explanation of the continuous simulation in the evolutionary learning world system model . practical implications more data would increase the versatility of the empirical exercise . social implications the study is based on the idea of social and economic interface in extending the scope of economic modeling . originality value the paper is very original in the area of heterodox economics that questions orthodox economic postulates and presents the complex methodology by circular causation method instead .",
    "target": "complexity;economics;modelling;causality;consciousness;cybernetics;creativity"
  },
  {
    "source": "dyfram dynamic fragmentation and replica management in distributed database systems . <eos> in distributed database systems , tables are frequently fragmented and replicated over a number of sites in order to reduce network communication costs . how to fragment , when to replicate and how to allocate the fragments to the sites are challenging problems that has previously been solved either by static fragmentation , replication and allocation , or based on a priori query analysis . many emerging applications of distributed database systems generate very dynamic workloads with frequent changes in access patterns from different sites . in such contexts , continuous refragmentation and reallocation can significantly improve performance . in this paper we present dyfram , a decentralized approach for dynamic table fragmentation and allocation in distributed database systems based on observation of the access patterns of sites to tables . the approach performs fragmentation , replication , and reallocation based on recent access history , aiming at maximizing the number of local accesses compared to accesses from remote sites . we show through simulations and experiments on the dascosa distributed database system that the approach significantly reduces communication costs for typical access patterns , thus demonstrating the feasibility of our approach .",
    "target": "fragmentation;replication;distributed dbms;physical database design"
  },
  {
    "source": "the anatomy of decision support during inpatient care provider order entry ( cpoe ) empirical observations from a decade of cpoe experience at vanderbilt . <eos> the authors describe a pragmatic approach to the introduction of clinical decision support at the point of care , based on a decade of experience in developing and evolving vanderbilts inpatient wizorder care provider order entry ( cpoe ) system . the inpatient care setting provides a unique opportunity to interject cpoe based decision support features that restructure clinical workflows , deliver focused relevant educational materials , and influence how care is delivered to patients . from their empirical observations , the authors have developed a generic model for decision support within inpatient cpoe systems . they believe that the models utility extends beyond vanderbilt , because it is based on characteristics of end user workflows and on decision support considerations that are common to a variety of inpatient settings and cpoe systems . the specific approach to implementing a given clinical decision support feature within a cpoe system should involve evaluation along three axes what type of intervention to create ( for which the authors describe <digit> general categories ) when to introduce the intervention into the users workflow ( for which the authors present <digit> categories ) , and how disruptive , during use of the system , the intervention might be to end users workflows ( for which the authors describe <digit> categories ) . framing decision support in this manner may help both developers and clinical end users plan future alterations to their systems when needs for new decision support features arise .",
    "target": "cpoe;clinical decision support"
  },
  {
    "source": "the method of approximate fundamental solutions for axisymmetric problems with laplace operator . <eos> the paper presents a new numerical technique for solving axisymmetric problems with laplace operator . it is similar to the method of fundamental solutions but it is based on the use of special basis functions which satisfy the majority of the boundary conditions of the problem considered . this reduces the number of unknowns and the size of the collocation matrix considerably . as it is shown in the paper , this technique can also be applied successfully in the cases when the solution domain has infinite boundaries in z z or r r directions . numerical examples justifying the method are presented .",
    "target": "fundamental solutions;axisymmetric problems;laplace equation;infinite domain"
  },
  {
    "source": "bayesian ordinal and binary regression models with a parametric family of mixture links . <eos> an ordinal and binary regression model with parametric link is introduced . the link is a member of a one parameter family of mixture links , a family that comprises smooth mixtures of the extreme minimum value , extreme maximum value , and logistic distributions . a bayesian version of this flexible model serves as a vehicle for introducing a priori information regarding the choice of link . owing to non conjugacy , posterior and predictive distributions are approximated using markov chain monte carlo simulation methods . link independent , bayesian interpretations of covariate effects are described . the method is illustrated through the analyses of several data sets .",
    "target": "markov chain monte carlo;categorical data;cumulative link model;ld50;logistic regression;mixture distribution;metropolishastings algorithm;ordinal data;predictive posterior distribution;tolerance distribution"
  },
  {
    "source": "a generic implementation of replica exchange with solute tempering ( rest2 ) algorithm in namd for complex biophysical simulations . <eos> replica exchange with solute tempering ( rest2 ) is a powerful sampling enhancement algorithm of molecular dynamics ( md ) in that it needs significantly smaller number of replicas but achieves higher sampling efficiency relative to standard temperature exchange algorithm . in this paper , we extend the applicability of rest2 for quantitative biophysical simulations through a robust and generic implementation in greatly scalable md software namd . the rescaling procedure of force field parameters controlling rest2 hot region is implemented into namd at the source code level . a user can conveniently select hot region through vmd and write the selection information into a pdb file . the rescaling keyword parameter is written in namd tcl script interface that enables an on the fly simulation parameter change . our implementation of rest2 is within communication enabled tcl script built on top of charm , thus communication overhead of an exchange attempt is vanishingly small . such a generic implementation facilitates seamless cooperation between rest2 and other modules of namd to provide enhanced sampling for complex biomolecular simulations . three challenging applications including native rest2 simulation for peptide foldingunfolding transition , free energy perturbation rest2 for absolute binding affinity of proteinligand complex and umbrella sampling rest2 hamiltonian exchange for free energy landscape calculation were carried out on ibm blue gene q supercomputer to demonstrate efficacy of rest2 based on the present implementation . program title rest2 namd catalogue identifier aexx_v1_0 program summary url http cpc.cs.qub.ac.uk summaries aexx_v1_0.html program obtainable from cpc program library , queens university , belfast , n. ireland licensing provisions standard cpc licence , http cpc.cs.qub.ac.uk licence licence.html no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language c c , tcl8 .5 . computer not computer specific . operating system any . has the code been vectorized or parallelized yes , mpi and or pami parallelized depending on machine system software <digit> cores used on ibm blue gene q classification <digit> . external routines namd 2.10 ( http www.ks.uiuc.edu research namd ) nature of problem a generic implementation providing user friendly api including input file preparation and performing replica exchange , and high frequency exchange attempt frequency with minimal communication overhead . solution method the rescaling procedure of force field parameters controlling rest2 is implemented into namd at the source code level . a user can conveniently select hot region through vmd and write the selection information into a pdb file . the rescaling keyword parameter is written in namd tcl script interface that enables an on the fly simulation parameter change . the implementation of rest2 is within communication enabled tcl script built on top of charm , thus communication overhead of an exchange attempt is vanishingly small . running time <digit> min60 min",
    "target": "rest2;namd;tcl;free energy calculation"
  },
  {
    "source": "physically based modeling , simulation and rendering of fire for computer animation . <eos> we give an up to date survey on techniques and methods for fire simulation in computer graphics . physically based method prevails over traditional non physical methods for realistic visual effect . in this paper , we explore visual simulation of fire related phenomena in terms of physically modeling , numerical simulation and visual rendering . firstly , we introduce a physical and chemical coupled mathematical model to explain fire behavior and motion . several assumptions and constrains are put forward to simplify their implementations in computer graphics . we then give an overview of present methods to solve the most complicated processes in numerical simulation velocity advection and pressure projection . in addition , comparisons of these methods are also presented respectively . since fire is a participating medium as well as a visual radiator , we discuss techniques and problems of these issues as well . we conclude by addressing several open challenges and possible future research directions in fire simulation .",
    "target": "fire;physically based simulation;navierstokes equations;chemicalreaction;blackbody radiation;visual adaption"
  },
  {
    "source": "evaluating the success of an emergency response medical information system . <eos> statpack is an information system used to aid in the diagnosis of pathogens in hospitals and state public health laboratories . statpack is used as a communication and telemedicine diagnosis tool during emergencies . this paper explores the success of this emergency response medical information system ( ermis ) using a well known framework of information systems success developed by delone and mclean . using an online survey , the entire population of statpack users evaluated the success of the information system by considering system quality , information quality , system use , intention to use , user satisfaction , individual impact , and organizational impact . the results indicate that the overall quality of this ermis ( i.e. , system quality , information quality , and service quality ) has a positive impact on both user satisfaction and intention to use the system . however , given the nature of ermis , overall quality does not necessarily predict use of the system . moreover , the user 's satisfaction with the information system positively affected the intention to use the system . user satisfaction , intention to use , and system use had a positive influence on the system 's impact on the individual . finally , the organizational impacts of the system were positively influenced by use of the system and the system 's individual impact on the user . the results of the study demonstrate how to evaluate the success of an ermis as well as introduce potential changes in how one applies the delone and mclean success model in an emergency response medical information system context .",
    "target": "emergencies;clinical laboratory information systems;program evaluation"
  },
  {
    "source": "easy cases of probabilistic satisfiability . <eos> the probabilistic satisfiability problem ( psat ) can be considered as a probabilistic counterpart of the classical sat problem . in a psat instance , each clause in a cnf formula is assigned a probability of being true the problem consists in checking the consistency of the assigned probabilities . actually , psat turns out to be computationally much harder than sat , e.g. , it remains difficult for some classes of formulas where sat can be solved in polynomial time . a column generation approach has been proposed in the literature , where the pricing sub problem reduces to a weighted max sat problem on the original formula . here we consider some easy cases of psat , where it is possible to give a compact representation of the set of consistent probability assignments . we follow two different approaches , based on two different representations of cnf formulas . first we consider a representation based on directed hypergraphs . by extending a well known integer programming formulation of sat and max sat , we solve the case in which the hypergraph does not contain cycles a linear time algorithm is provided for this case . then we consider the co occurrence graph associated with a formula . we provide a solution method for the case in which the co occurrence graph is a partial <digit> tree , and we show how to extend this result to partial k trees with k > <digit> .",
    "target": "probabilistic satisfiability;cnf formulas;directed hypergraphs;partial k trees;balanced matrices"
  },
  {
    "source": "face description with local binary patterns application to face recognition . <eos> this paper presents a novel and efficient facial image representation based on local binary pattern ( lbp ) texture features . the face image is divided into several regions from which the lbp feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor . the performance of the proposed method is assessed in the face recognition problem under different challenges . other applications and several extensions are also discussed .",
    "target": "local binary pattern;facial image representation;texture features;component based face recognition;face misalignment"
  },
  {
    "source": "low complexity adaptive decision feedback equalization of mimo channels . <eos> a new adaptive mimo channel equalizer is proposed based on adaptive generalized decision feedback equalization and ordered successive interference cancellation . the proposed equalizer comprises equal length subequalizers , enabling any adaptive filtering algorithm to be employed for coefficient updates . a recently proposed computationally efficient recursive least squares algorithm based on dichotomous coordinate descents is utilized to solve the normal equations associated with the adaptation of the new equalizer . convergence of the proposed algorithm is examined analytically and simulations show that the proposed equalizer is superior to the previously proposed adaptive mimo channel equalizers by providing both enhanced bit error rate performance and reduced computational complexity . furthermore , the proposed algorithm exhibits stable numerical behavior and can deliver a trade off between performance and complexity .",
    "target": "adaptive generalized decision feedback equalization;ordered successive interference cancelation;mimo systems;rlsdcd algorithm;v blast"
  },
  {
    "source": "performance of hsr and qpp based interleavers for turbo coding on power line communication systems . <eos> in this paper , the performance of different type and length interleavers for turbo codes is analyzed in the context of power line communication systems . this system typically operates in very noisy environments the noise , in this channel , is a combination of colored , narrow band and impulsive noises it has also strong amplitude attenuations . the digital modulation frequently employed in power line communication to counteract the channels noise effects is the orthogonal frequency division multiplexing due to its high spectral efficiency and robustness in multipath fading environments hence , it is also considered in our experimentation . we report the performance of turbo codes with the two types of interleavers the high spread random and the based quadratic permutation polynomial . the constituent codes are part of the 3gpp standard . finally , it is used a punctured matrix in order to achieve a coding rate of <digit> <digit> . the performance is evaluated in terms of bit error rate , through the way of simulations .",
    "target": "interleavers;turbo codes;punctured matrix;plc;ofdm"
  },
  {
    "source": "a numerical method for a model of two phase flow in a coupled free flow and porous media system . <eos> in this article , we study two phase fluid flow in coupled free flow and porous media regions . the model consists of coupled cahnhilliard and navierstokes equations in the free fluid region and the two phase darcy law in the porous medium region . we propose a robinrobin domain decomposition method for the coupled navierstokes and darcy system with the generalized beaversjosephsaffman condition on the interface between the free flow and the porous media regions . numerical examples are presented to illustrate the effectiveness of this method .",
    "target": "two phase flow;porous media;robinrobin domain decomposition;darcy 's law"
  },
  {
    "source": "circular elm for the reduced reference assessment of perceived image quality . <eos> providing a satisfactory visual experience is one of the main goals for present day electronic multimedia devices . all the enabling technologies for storage , transmission , compression , rendering should preserve , and possibly enhance , the quality of the video signal to do so , quality control mechanisms are required . these mechanisms rely on systems that can assess the visual quality of the incoming signal consistently with human perception . computational intelligence ( ci ) paradigms represent a suitable technology to tackle this challenging problem . the present research introduces an augmented version of the basic extreme learning machine ( elm ) , the circular elm ( c elm ) , which proves effective in addressing the visual quality assessment problem . the c elm model derives from the original circular backpropagation ( cbp ) architecture , in which the input vector of a conventional multilayer perceptron ( mlp ) is augmented by one additional dimension , the circular input this paper shows that c elm can actually benefit from the enhancement provided by the circular input without losing any of the fruitful properties that characterize the basic elm framework . in the proposed framework , c elm handles the actual mapping of visual signals into quality scores , successfully reproducing perceptual mechanisms . its effectiveness is proved on recognized benchmarks and for four different types of distortions .",
    "target": "extreme learning machine;circular backpropagation;image quality assessment"
  },
  {
    "source": "distributed reinforcement learning control for batch sequencing and sizing in just in time manufacturing systems . <eos> this paper presents an approach that is suitable for just in time ( jit ) production for multi objective scheduling problem in dynamically changing shop floor environment . the proposed distributed learning and control ( dlc ) approach integrates part driven distributed arrival time control ( datc ) and machine driven distributed reinforcement learning based control . with datc , part controllers adjust their associated parts ' arrival time to minimize due date deviation . within the restricted pattern of arrivals , machine controllers are concurrently searching for optimal dispatching policies . the machine control problem is modeled as semi markov decision process ( smdp ) and solved using q learning . the dlc algorithms are evaluated using simulation for two types of manufacturing systems family scheduling and dynamic batch sizing . results show that dlc algorithms achieve significant performance improvement over usual dispatching rules in complex real time shop floor control problems for jit production .",
    "target": "scheduling;machine learning;just in time production"
  },
  {
    "source": "algorithm for calculating the noncentral chi square distribution . <eos> this correspondence presents a new algorithm for evaluating the noncentral chi square distribution based on parl 's method of neumann series expansion , it is applicable to both even and odd degrees of freedom , unlike most prior work , which has been directed at the even eases . convergence tests and procedures for detection of loss of precision are given . the overall method is extremely simple to program , accurate to many decimal places where applicable , and efficient over a wide range of parameters . the method is reliable provided the proper expansion is chosen based on the parameters .",
    "target": "algorithm;chi square distribution;neumann series;q function"
  },
  {
    "source": "new hierarchical architecture for ubiquitous wireless sensing and access with improved coverage using cwdm rof links . <eos> a novel hierarchical architecture for hybrid wireless sensor and access networks has been proposed based on cost effective radio over fiber ( rof ) links with the coarse wavelength division multiplexing ( cwdm ) technique . wireless fidelity ( wifi ) signals are distributed to the remote radio units transparently over optical fibers in a star shaped network topology . the wireless access traffic together with the perceiving usage scenarios including video monitoring and temperature sensing has been successfully demonstrated in the hybrid ieee 802.11 and 802.15.4 networks . the transmission performance of the cwdm rof links is evaluated in terms of the error vector magnitude ( evm ) and data throughput for both uplinks and downlinks . the results show that the wifi signals are successfully delivered through the cwdm rof links including a 4.5 km fiber and a <digit> m wireless channel with a <digit> % evm penalty . this cwdm rof technology can expand the application range of wireless sensor networks with the advantages of better capacity , larger coverage area , and lower investment on wired infrastructure .",
    "target": "radio over fiber;coarse wavelength division multiplexing;wireless sensor network;wireless local area network"
  },
  {
    "source": "bifurcation analysis of an inductorless chaos generator using 1d piecewise smooth map . <eos> in this work we investigate the dynamics of a one dimensional piecewise smooth map , which represents the model of a chaos generator circuit . in a particular ( symmetric ) case analytic results can be given showing that the chaotic region is wide and robust . in the general model only the border collision bifurcation can be analytically determined . however , the dynamics behave in a similar way , leading effectively to robust chaos . ( c ) <digit> published by elsevier b.v. on behalf of imacs .",
    "target": "chaos generator;border collision;piecewise smooth continuous map"
  },
  {
    "source": "a hierarchical approach for energy efficient scheduling of large workloads in multicore distributed systems . <eos> definition of a novel multi objective problem for energy efficient scheduling in distributed data centers . design of a hierarchical two level scheduler that allows dividing the problem into simpler and smaller sub problems . evaluation and comparison of <digit> different variants of the scheduler on large sets of workflows . accurate solutions found by the best performing schedulers , achieving important improvements over classical strategies .",
    "target": "energy efficiency;multicore;workflows;scheduling heuristics"
  },
  {
    "source": "collaborative feature based design via operations with a fine grain product database . <eos> this paper reports a collaborative product design framework and a prototype system that supports multiple cad systems . the key contribution is an ' operation ' based , multi application oriented , and near real time collaboration mechanism which can significantly reduce collaboration communication load over the network . the mechanism is discussed and demonstrated with examples . to support the proposed multi application collaboration system , a fine grain feature oriented product database is used . this research is a continued effort based on a shared common product modeling scheme , which covers fundamental issues of generic feature , feature level interoperability , engineering intent and operation definitions . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "fine grain product database;collaborative engineering;feature based engineering"
  },
  {
    "source": "unified architecture for reed solomon decoder combined with burst error correction . <eos> reed solomon ( rs ) codes are widely used as forward correction codes ( fec ) in digital communication and storage systems . correcting random errors of rs codes have been extensively studied in both academia and industry . however , for burst error correction , the research is still quite limited due to its ultra high computation complexity . in this brief , starting from a recent theoretical work , a low complexity reformulated inversionless burst error correcting ( ribc ) algorithm is developed for practical applications . then , based on the proposed algorithm , a unified vlsi architecture that is capable of correcting burst errors , as well as random errors and erasures , is firstly presented for multi mode decoding requirements . this new architecture is denoted as unified hybrid decoding ( uhd ) architecture . it will be shown that , being the first rs decoder owning enhanced burst error correcting capability , it can achieve significantly improved error correcting capability than traditional hard decision decoding ( hdd ) design .",
    "target": "unified architecture;burst errors;vlsi;reed solomon codes"
  },
  {
    "source": "implementation aspects of 3d lattice bgk boundaries , accuracy , and a new fast relaxation method . <eos> in many realistic fluid dynamical simulations the specification of the boundary conditions , the error sources , and the number of time steps to reach a steady state are important practical considerations . in this paper we study these issues in the case of the lattice bgk model . the objective is to present a comprehensive overview of some pitfalls and shortcomings of the lattice bgk method and to introduce some new ideas useful in practical simulations . we begin with an evaluation of the widely used bounce back boundary condition in staircase geometries by simulating flow in an inclined tube . it is shown that the bounce back scheme is first order accurate in space when the location of the non slip wall is assumed to be at the boundary nodes . moreover , for a specific inclination angle of <digit> degrees , the scheme is found to be second order accurate when the location of the non slip velocity is fitted halfway between the last fluid nodes and the first solid nodes . the error as a function of the relaxation parameter is in that case qualitatively similar to that of flat walls . next , a comparison of simulations of fluid flow by means of pressure boundaries and by means of body force is presented . a good agreement between these two boundary conditions has been found in the creeping flow regime . for higher reynolds numbers differences have been found that are probably caused by problems associated with the pressure boundaries . furthermore , two widely used 3d models , namely d ( <digit> ) q ( <digit> ) and d ( <digit> ) q ( <digit> ) , are analysed . it is shown that the d ( <digit> ) q ( <digit> ) model may induce artificial checkerboard invariants due to the connectivity of the lattice . finally , a new iterative method , which significantly reduces the saturation time , is presented and validated on different benchmark problems . ( c ) <digit> academic press .",
    "target": "accuracy;boundary conditions;lattice bgk model"
  },
  {
    "source": "partition refinement of component interaction automata . <eos> component interaction automata provide a fitting model to capture and analyze the temporal facets of hierarchically structured component oriented software systems . however , the rules governing composition typically suffer from combinatorial state explosion , an effect that can impede modeling languages , like component interaction automata , from being successful in real world scenarios . we must , therefore , find some appropriate ways to counteract state explosion , one of which is partition refinement through bisimulation , in particular , weak bisimulation . while this technique can yield the desired state space reduction , it does not consider synchronization cliques , that is , groups of states that are interconnected solely by internal synchronization transitions . synchronization cliques give rise to action prefixes , local states that encapsulate preconditions for a component 's ability to interact with the environment . furthermore , both the existence and the size of synchronization cliques can be used as an indicator for the success of partition refinement . in particular , the more frequent synchronization cliques are and the more states they entail , the more likely it is that partition refinement can reduce the state space . but , there may be other factors that impact the refinement process . for this reason , we study , in this paper , how partition refinement behaves under weak bisimulation , how synchronization cliques emerge when using weak bisimulation , how we make state space reduction through partition refinement aware of the existence of synchronization cliques , and what other attributes of component interaction automata specifications can provides us with additional cues to forecast the possible outcome of the partition refinement process . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "partition refinement;automata based specification;emergent properties"
  },
  {
    "source": "an on line replication strategy to increase availability in data grids . <eos> data is typically replicated in a data grid to improve the job response time and data availability . strategies for data replication in a data grid have previously been proposed , but they typically assume unlimited storage for replicas . in this paper , we address the system wide data availability problem assuming limited replica storage . we describe two new metrics to evaluate the reliability of the system , and propose an on line optimizer algorithm that can minimize the data missing rate ( mindmr ) in order to maximize the data availability . based on mindmr , we develop four optimizers associated with four different file access prediction functions . simulation results utilizing the optorsim show our mindmr strategies achieve better performance overall than other strategies in terms of the goal of data availability using the two new metrics .",
    "target": "data grid;data availability;data missing rate;limited storage;replica strategy"
  },
  {
    "source": "power assignment for k connectivity in wireless ad hoc networks . <eos> the problem min power k connectivity seeks a power assignment to the nodes in a given wireless ad hoc network such that the produced network topology is k connected and the total power is the lowest . in this paper , we present several approximation algorithms for this problem . specifically , we propose a 3k approximation algorithm for any k ge <digit> , a ( k 12h ( k ) ) approximation algorithm for k ( 2k <digit> ) le n where n is the network size , a ( k <digit> ( k <digit> ) <digit> ) approximation algorithm for <digit> le k le <digit> , a <digit> approximation algorithm for k <digit> , and a <digit> approximation algorithm for k <digit> .",
    "target": "power assignment;k connectivity;wireless ad hoc sensor networks"
  },
  {
    "source": "comparison of anova f and anom tests with regard to type i error rate and test power . <eos> a monte carlo simulation was conducted to compare the type i error rate and test power of the analysis of means ( anom ) test to the one way analysis of variance f test ( anova f ) . simulation results showed that as long as the homogeneity of the variance assumption was satisfied , regardless of the shape of the distribution , number of group and the combination of observations , both anova f and anom test have displayed similar type i error rates . however , both tests have been negatively affected from the heterogeneity of the variances . this case became more obvious when the variance ratios increased . the test power values of both tests changed with respect to the effect size ( ) , variance ratio and sample size combinations . as long as the variances are homogeneous , anova f and anom test have similar powers except unbalanced cases . under unbalanced conditions , the anova f was observed to be powerful than the anom test . on the other hand , an increase in total number of observations caused the power values of anova f and anom test approach to each other . the relations between effect size ( ) and the variance ratios affected the test power , especially when the sample sizes are not equal . as anova f has become to be superior in some of the experimental conditions being considered , anom is superior in the others . however , generally , when the populations with large mean have larger variances as well , anom test has been seen to be superior . on the other hand , when the populations with large mean have small variances , generally , anova f has observed to be superior . the situation became clearer when the number of the groups is <digit> or <digit> .",
    "target": "anom;type i error;test power;simulation;analysis of variance"
  },
  {
    "source": "on mining multi time interval sequential patterns . <eos> sequential pattern mining is essential in many applications , including computational biology , consumer behavior analysis , web log analysis , etc. although sequential patterns can tell us what items are frequently to be purchased together and in what order , they can not provide information about the time span between items for decision support . previous studies dealing with this problem either set time constraints to restrict the patterns discovered or define time intervals between two successive items to provide time information . accordingly , the first approach falls short in providing clear time interval information while the second can not discover time interval information between two non successive items in a sequential pattern . to provide more time related knowledge , we define a new variant of time interval sequential patterns , called multi time interval sequential patterns , which can reveal the time intervals between all pairs of items in a pattern . accordingly , we develop two efficient algorithms , called the mi apriori and mi prefixspan algorithms , to solve this problem . the experimental results show that the mi prefixspan algorithm is faster than the mi apriori algorithm , but the mi apriori algorithm has better scalability in long sequence data .",
    "target": "multi time interval;time interval;sequential pattern;data mining;knowledge discovery"
  },
  {
    "source": "fpga implementation of a wavelet neural network with particle swarm optimization learning . <eos> this paper introduces implementation of a wavelet neural network ( wnn ) with learning ability on field programmable gate array ( fpga ) . a learning algorithm using gradient descent method is not easy to implement in an electronic circuit and has local minimum . a more suitable method is the particle swarm optimization ( pso ) that is a population based optimization algorithm . the pso is similar to the ga , but it has no evolution operators such as crossover and mutation . in the approximation of a nonlinear activation function , we use a taylor series and a look up table ( lut ) to achieve a more accurate approximation . the results of the two experiments demonstrate the successful hardware implementation of the wavelet neural networks with the pso algorithm using fpga . from the results of the experiment , it can be seen that the performance of the pso is better than that of the simultaneous perturbation algorithm at sufficient particle sizes . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "wavelet neural networks;particle swarm optimization;field programmable gate array;prediction;identification"
  },
  {
    "source": "interface synthesis for heterogeneous multi core systems from transaction level models . <eos> this paper presents a tool for automatic synthesis of rtl interfaces for heterogeneous mpsoc from transaction level models ( tlms ) . the tool captures the communication parameters in the platform and generates interface modules called universal bridges between buses in the design . the design and configuration of the bridges depend on several platform components including heterogeneity of the components , traffic on the bus , size of messages and so on . we define these parameters and show how the synthesizable rtl code for the bridge can be automatically derived based on these parameters . we use industrial strength design drivers such as an mp3 decoder to test our automatically generated bridges for a variety of platforms and compare them to manually designed bridges on different quality metrics . our experimental results show that performance of automatically generated bridges are within <digit> % of manual design for simple platforms but surpasses them for more complex platforms . the area and rtl code size is consistently better than manual design while giving <digit> orders of improvement in development time .",
    "target": "interface synthesis;transaction level model;universal bridge;design;experimentation;performance;hw sw co design;communication synthesis;channel;reliability;verification"
  },
  {
    "source": "efficient implementations of construction heuristics for the rectilinear block packing problem . <eos> the rectilinear block packing problem is a problem of packing a set of rectilinear blocks into a larger rectangular container , where a rectilinear block is a polygonal block whose interior angle is either <digit> or <digit> . there exist many applications of this problem , such as vlsi design , timber glass cutting , and newspaper layout . in this paper , we design efficient implementations of two construction heuristics for rectilinear block packing . the proposed algorithms are tested on a series of instances , which are generated from nine benchmark instances . the computational results show that the proposed algorithms are especially efficient for large instances with repeated shapes .",
    "target": "efficient implementation;construction heuristics;rectilinear blocks;strip packing"
  },
  {
    "source": "the effect of finite lattice size in lattice boltzmann model . <eos> in this paper , numerical results on two dimensional vapor liquid equilibrium calculated by lattice boltzmann method have been presented . artefacts resulted by the finite lattice size have been reviewed . a set of criteria for minimal lattice size to avoid lattice artefacts is given .",
    "target": "lattice boltzmann method;phase transition;finite size effect"
  },
  {
    "source": "estimating vignetting function from a single image for image authentication . <eos> vignetting is the phenomenon of reduced brightness in an image at the peripheral region compared to the central region . as patterns of vignetting are characteristics of lens models , they can be used to authenticate digital images for forensic analysis . in this paper , we describe a new method for model based single image vignetting estimation and correction . we use the statistical properties of natural images in the discrete derivative domains and formulate the vignetting estimation problem as a maximum likelihood estimation . we further provide a simple and efficient procedure for better initialization of the numerical optimization . empirical evaluations of the proposed method using synthesized and real vignetted images show significant gain in both performance and running efficiency in correcting vignetting from digital images , and the estimated vignetting functions are shown to be effective in classifying different lens models .",
    "target": "vignetting function estimation;camera identification"
  },
  {
    "source": "effective utility mining with the measure of average utility . <eos> frequent itemset mining only considers the frequency of occurrence of the items but does not reflect any other factors , such as price or profit . utility mining is an extension of frequent itemset mining , considering cost , profit or other measures from user preference . traditionally , the utility of an itemset is the summation of the utilities of the itemset in all the transactions regardless of its length . the average utility measure is thus adopted in this paper to reveal a better utility effect of combining several items than the original utility measure . it is defined as the total utility of an itemset divided by its number of items within it . the average utility itemsets , as well as the original utility itemsets , does not have the downward closure property . a mining algorithm is then proposed to efficiently find the high average utility itemsets . it uses the summation of the maximal utility among the items in each transaction with the target itemset as the upper bound to overestimate the actual average utilities of the itemset and processes it in two phases . as expected , the mined high average utility itemsets in the proposed way will be fewer than the high utility itemsets under the same threshold . the proposed approach can thus be executed under a larger threshold than the original , thus with a more significant and relevant criterion . experimental results also show the performance of the proposed algorithm . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "utility mining;average utility;downward closure;two phase mining"
  },
  {
    "source": "surgical workflow management schemata for cataract procedures process model based design and validation of workflow schemata . <eos> objective workflow guidance of surgical activities is a challenging task . because of variations in patient properties and applied surgical techniques , surgical processes have a high variability . the objective of this study was the design and implementation of a surgical workflow management system ( swfms ) that can provide a robust guidance for surgical activities . we investigated how many surgical process models are needed to develop a swfms that can guide cataract surgeries robustly . methods we used <digit> cases of cataract surgeries and acquired patient individual surgical process models ( ispms ) from them . of these , randomized subsets ispms were selected as learning sets to create a generic surgical process model ( gspm ) . these gspms were mapped onto workflow nets as workflow schemata to define the behavior of the swfms . finally , <digit> ispms from the disjoint set were simulated to validate the workflow schema for the surgical processes . the measurement was the successful guidance of an ispm . results we demonstrated that a swfms with a workflow schema that was generated from a subset of <digit> ispms is sufficient to guide approximately <digit> % of all surgical processes in the total set , and that a subset of <digit> ispms is sufficient to guide approx. <digit> % of all processes . conclusion we designed a swfms that is able to guide surgical activities on a detailed level . the study demonstrated that the high inter patient variability of surgical processes can be considered by our approach .",
    "target": "workflow;surgical process model;operative surgical procedures;computer assisted decision making;computer assisted surgery"
  },
  {
    "source": "temspol a matlab thermal model for deep subduction zones including major phase transformations . <eos> temspol is an open matlab code suitable for calculating temperature and lateral anomaly of density distributions in deep subduction zones , taking into account the olivine to spinel phase transformation in a self consistent manner . the code solves , by means of a finite difference scheme , the heat transfer equation including adiabatic heating , radioactive heat generation , latent heat associated with phase changes and frictional heating . we show , with a few simulations , that temspol can be a useful tool for researchers studying seismic velocity , stress and seismicity distribution in deep subduction zones . deep earthquakes in subducting slabs are thought to be caused by shear instabilities associated with the olivine to spinel phase transition in metastable olivine wedges . we investigate the kinematic and thermal conditions of the subducting plate that lead to the formation of metastable olivine wedges . moreover , temspol calculates lateral anomalies of density within subducting slabs , which can be used to evaluate buoyancy forces that determine the dynamics of subduction and the stress distribution within the slab . we use temspol to evaluate the effects of heat sources such as shear heating and latent heat release , which are neglected in commonly used thermal models of subduction . we show that neglecting these heat sources can lead to significant overestimation of the depth reached by the metastable olivine wedge .",
    "target": "subduction;temperature;olivine;spinel;phase transitions;density anomaly"
  },
  {
    "source": "online social advertising via influential endorsers . <eos> in recent years , many web based services such as facebook and myspace have been making great progress and creating new opportunities . because online advertising is the main business model for social networking sites , in this paper we propose a social endorser based advertising system formulated on network influence and user preference analyses . by utilizing the social network and user preference analysis techniques , the theories of dynamic social influence and celebrity endorsement are realized in the proposed advertising approach . experiments show that our mechanism significantly improves advertising effectiveness and efficiency and outperforms other advertising approaches .",
    "target": "social network;endorser advertising;ewom;influence model;word of mouth"
  },
  {
    "source": "similarity measures between type <digit> fuzzy sets . <eos> in this paper , we give similarity measures between type <digit> fuzzy sets and provide the axiom definition and properties of these measures . for practical use , we show how to compute the similarities between gaussian type <digit> fuzzy sets . yang and shih 's <digit> algorithm , a clustering method based on fuzzy relations by beginning with a similarity matrix , is applied to these gaussian type <digit> fuzzy sets by beginning with these similarities . the clustering results are reasonable consisting of a hierarchical tree according to different levels .",
    "target": "similarity measure;type <digit> fuzzy sets;gaussian type <digit> fuzzy sets;hausdorff distance"
  },
  {
    "source": "a mechanization of unity in pc nqthm <digit> . <eos> this paper presents in detail how the unity logic for reasoning about concurrent programs was formalized within the mechanized theorem prover pc nqthm <digit> . most of unity 's proof rules were formalized in the unquantified logic of nqthm , and the proof system has been used to mechanically verify several concurrent programs . the mechanized proof system is sound by construction , since unity 's proof rules were proved about an operational semantics of concurrency , also presented here . skolem functions are used instead of quantifiers , and the paper describes how proof rules containing skolem function are used instead of unity 's quantified proof rules when verifying concurrent programs . this formalization includes several natural extensions to unity , including nondeterministic statements . the paper concludes with a discussion of the cost and value of mechanization .",
    "target": "unity;pc nqthm;concurrency;theorem proving;parallelism"
  },
  {
    "source": "reconciling while tolerating disagreement in collaborative data sharing . <eos> in many data sharing settings , such as within the biological and biomedical communities , global data consistency is not always attainable different sites ' data may be dirty , uncertain , or even controversial . collaborators are willing to share their data , and in many cases they also want to selectively import data from others but must occasionally diverge when they disagree about uncertain or controversial facts or values . for this reason , traditional data sharing and data integration approaches are not applicable , since they require a globally consistent data instance . additionally , many of these approaches do not allow participants to make updates if they do , concurrency control algorithms or inconsistency repair techniques must be used to ensure a consistent view of the data for all users.in this paper , we develop and present a fully decentralized model of collaborative data sharing , in which participants publish their data on an ad hoc basis and simultaneously reconcile updates with those published by others . individual updates are associated with provenance information , and each participant accepts only updates with a sufficient authority ranking , meaning that each participant may have a different ( though conceptually overlapping ) data instance . we define a consistency semantics for database instances under this model of disagreement , present algorithms that perform reconciliation for distributed clusters of participants , and demonstrate their ability to handle typical update and conflict loads in settings involving the sharing of curated data .",
    "target": "collaboration;collaborative data sharing;data;data sharing;sharing;communities;global;consistency;values;data integration;participant;updates;algorithm;paper;decentralization;model;publish;ad hoc;association;proven;informal;author;ranking;mean;semantic;database;reconciliation;distributed;cluster;demonstrate;conflict;users;peer to peer;transactions"
  },
  {
    "source": "application of evolutionary strategies for 3d graphical model categorization and retrieval . <eos> in multimedia information processing , while the previous focus was on image video retrieval , content based categorization and retrieval of 3d computer graphics model is becoming increasingly important . this is due to the increased adoption of 3d graphics representations in multimedia applications and the resulting need for rapid virtual scene assembly from a repository of 3d models . motivated by these requirements , the main focus of this paper is on the content based classification and retrieval of 3d computer graphics models based on a histogram feature representation , and the search for an adaptive transformation of this representation such that the resulting classification and retrieval accuracies are optimized . observing that a histogram is basically an approximation of the probability density function of an underlying random variable , and that a suitable transformation , when applied to the random variable , will allow the classifier to attain better accuracy based on this new representation , we propose an evolutionary optimization approach to search for this set of optimal transformations due to the large size of the search space . in particular , we consider the special class of transformations that take the form of a piecewise continuous mapping . in this case , the transformed variable is a mixed random variable , with both discrete and continuous components , which provides added flexibility for modeling a number of more diverse random variable types . with a suitably defined fitness function for evolutionary strategies ( es ) that measures the capability of the transformed histogram representation to induce the correct class structure , our proposed approach is capable of improving the head model classification performance , which in turn allows , in the case of content based retrieval , the correct preassignment of a query object to its correct class for more efficient search , even in those cases where the query is ambiguous and difficult to characterize .",
    "target": "evolutionary strategies;pattern classification;3d head model;evolutionary computation;multiple classifier system"
  },
  {
    "source": "face recognition based on a novel linear discriminant criterion . <eos> as an effective technique for feature extraction and pattern classification fisher linear discriminant ( fld ) has been successfully applied in many fields . however , for a task with very high dimensional data such as face images , conventional fld technique encounters a fundamental difficulty caused by singular within class scatter matrix . to avoid the trouble , many improvements on the feature extraction aspect of fld have been proposed . in contrast , studies on the pattern classification aspect of fld are quiet few . in this paper , we will focus our attention on the possible improvement on the pattern classification aspect of fld by presenting a novel linear discriminant criterion called maximum scatter difference ( msd ) . theoretical analysis demonstrates that msd criterion is a generalization of fisher discriminant criterion , and is the asymptotic form of discriminant criterion large margin linear projection . the performance of msd classifier is tested in face recognition . experiments performed on the orl , yale , feret and ar databases show that msd classifier can compete with top performance linear classifiers such as linear support vector machines , and is better than or equivalent to combinations of well known facial feature extraction methods , such as eigenfaces , fisherfaces , orthogonal complementary space , nullspace , direct linear discriminant analysis , and the nearest neighbor classifier .",
    "target": "face recognition;pattern classification;fisher linear discriminant;small sample size problem;multi objective programming;binary linear classifier"
  },
  {
    "source": "multi channel sampling on shift invariant spaces with frame generators . <eos> let phi be a continuous function in l <digit> ( r ) such that the sequence phi ( t n ) ( n is an element of z ) is a frame sequence in l <digit> ( r ) and assume that the shift invariant space v ( phi ) generated by phi has a multi banded spectrum sigma ( v ) . the main aim in this paper is to derive a multi channel sampling theory for the shift invariant space v ( phi ) . by using a type of fourier duality between the spaces v ( phi ) and l <digit> <digit> , <digit> pi we find necessary and sufficient conditions allowing us to obtain stable multi channel sampling expansions in v ( phi ) .",
    "target": "multi channel sampling;shift invariant spaces;frames"
  },
  {
    "source": "focus of b to b e commerce initiatives and related benefits in manufacturing small and medium sized enterprises . <eos> empirical research into business to business e commerce issues involving manufacturing small and medium sized enterprises ( smes ) is still embryonic . in an attempt to partially fill this gap , this paper presents empirical data from an electronic survey conducted among <digit> manufacturing smes to investigate e commerce initiatives and their related benefits . e commerce initiatives are assessed using a set of <digit> business processes that can be conducted electronically . these processes were classified according to their focus customer ( downstream ) , supplier ( upstream ) or in house . the research findings point to four main profiles of manufacturing smes with different e commerce focuses . the first group seems to lack any focus or may still be exploring e commerce opportunities . the second and third groups are supplier and customer focused , respectively . the fourth group consists of the more involved smes that have leveraged their e commerce initiatives with both their customers and their suppliers . results also suggest the existence of a close alignment between e commerce focus and related benefits .",
    "target": "b to b e commerce;smes;business processes"
  },
  {
    "source": "incomplete information based decentralized cooperative control strategy for distributed energy resources of vsi based microgrids . <eos> this paper presents an effective method to control distributed energy resources ( ders ) installed in a microgrid ( mg ) to guarantee its stability after islanding occurrence . considering voltage and frequency variations after islanding occurrence and based on stability criteria , mg pre islanding conditions are divided into secure and insecure classes . it is shown that insecure mg can become secure , if appropriate preventive control is applied on the ders in different operating conditions of the mg . to select the most important variables of mg , which can estimate proper values of output power set points of ders , a feature selection procedure known as symmetrical uncertainty is used in this paper . among all the mg variables , critical ones are selected to calculate the appropriate output power of different ders for different conditions of the mg . the values of selected features are transmitted by the communication system to the control unit installed on each der to control its output power set point . in order to decrease the communication system cost , previous researchers have used local variables to control the set point of different ders . this approach decreases the accuracy of the controller because the controller uses incomplete information . in this paper , multi objective approach is used in order to decrease the cost of the communication system , while keeping the accuracy of the preventive control strategy in an allowable margin . the results demonstrate the effectiveness of the proposed method in comparison with other methods .",
    "target": "incomplete information;cooperative control;distributed energy resources;decentralized control;ann based control"
  },
  {
    "source": "modeling and reasoning with paraconsistent rough sets . <eos> we present a language for defining paraconsistent rough sets and reasoning about them . our framework relates and brings together two major fields rough sets <digit> and paraconsistent logic programming <digit> . to model inconsistent and incomplete information we use a four valued logic . the language discussed in this paper is based on ideas of our previous work <digit> , <digit> , <digit> developing a four valued framework for rough sets . in this approach membership function , set containment and set operations are four valued , where logical values are t ( true ) , f ( false ) , i ( inconsistent ) and u ( unknown ) . we investigate properties of paraconsistent rough sets as well as develop a paraconsistent rule language , providing basic computational machinery for our approach .",
    "target": "rough sets;four valued logics;approximate reasoning;paraconsistent reasoning"
  },
  {
    "source": "a dynamical tikhonov regularization for solving ill posed linear algebraic systems . <eos> the tikhonov method is a famous technique for regularizing ill posed linear problems , wherein a regularization parameter needs to be determined . this article , based on an invariant manifold method , presents an adaptive tikhonov method to solve ill posed linear algebraic problems . the new method consists in building a numerical minimizing vector sequence that remains on an invariant manifold , and then the tikhonov parameter can be optimally computed at each iteration by minimizing a proper merit function . in the optimal vector method ( ovm ) three concepts of optimal vector , slow manifold and hopf bifurcation are introduced . numerical illustrations on well known ill posed linear problems point out the computational efficiency and accuracy of the present ovm as compared with classical ones .",
    "target": "dynamical tikhonov regularization;tikhonov regularization;adaptive tikhonov method;optimal vector method;ill posed linear system;steepest descent method;conjugate gradient method;barzilai borwein method;65f10;65f22"
  },
  {
    "source": "mapping transit based access integrating gis , routes and schedules . <eos> accessibility is a concept that is not entirely easy to define . gould ( <digit> ) once stated that it is a ' slippery notion ... one of those common terms that everyone uses until faced with the problem of defining and measuring it ' . considerable research over the last <digit> years has been devoted to defining and measuring accessibility , ranging from access to jobs within an hour 's travel time to the ease at which given places can be reached . this article is concerned with the measurement of access provided by transit . it includes a review of past work on measuring accessibility in general and with respect to transit services in particular . from this overview of the literature , it can be seen that current methods fall short in measuring transit service access in several meaningful aspects . based on this review and critique , we propose new refinements that can be used to help overcome some of these shortcomings . as a part of this , we define an extended gis data structure to handle temporal elements of transit service . to demonstrate the value of these new measures , examples are presented with respect to mapping accessibility of transit services in santa barbara , california . finally , we show how these measures can be used to develop a framework for supporting transit service analysis and planning .",
    "target": "accessibility;public transit;schedule and route information;geographic information systems;urban applications"
  },
  {
    "source": "the complexity of compressing subsegments of images described by finite automata . <eos> we investigate how the compression size of the compressed version of a two dimensional image changes when we cut off a part of it , e.g. extract a photo of one person from a photo of a group of people , when compression is considered in terms of finite automata . denote by c ( t ) the compression size of a square image t in terms of deterministic automata , it is the smallest size of a deterministic acyclic automaton a describing t. the corresponding alphabet of a has only four letters , corresponding to four quadrants . we consider an independent useful combinatorial interpretation of c ( t ) in terms of regular subsquares of t. denote by ( n ) the largest compression size c ( r ) of a square subsegment r of the image t such that c ( t ) n. we show that there is a constant c > <digit> such that we also show how to construct efficiently ( in linear time w.r.t. the total size of the input and the produced output ) the compressed representation of subsegments given the compressed representation of the whole image .",
    "target": "complexity;finite automata;square images"
  },
  {
    "source": "accomplishing universal access through system reachabilitya management perspective . <eos> the aim of this paper is to describe the need of a method by which we can estimate the return on accessibility investments in information technology ( it ) systems . this paper reveals some of the reasons why accessibility still is a secondhand criterion when designing digital services . it also describes the authors experiences regarding the concept of accessibility and how it must develop in order to obtain the status of a basic business criterion for the benefit of disabled people who are currently excluded from public services and labour markets . the paper also questions the need of a separate accessibility standard . additionally , we discuss some of the hindering in the market and limiting perspectives that are blocking further development . one of the problems in the market seems to be that accessibility as a concept has been more of an issue about creating equal opportunities and therefore probably does not have the quality of a business criterion . in order to bridge that gap , we argue for replacing accessibility with reachability , which is a concept based on a measure used by media when estimating the reached percentage of a population or target group .",
    "target": "accessibility;management;standards;guidelines;usability"
  },
  {
    "source": "pothmf a program for computing potential curves and matrix elements of the coupled adiabatic radial equations for a hydrogen like atom in a homogeneous magnetic field . <eos> a fortran <digit> program is presented which calculates with the relative machine precision potential curves and matrix elements of the coupled adiabatic radial equations for a hydrogen like atom in a homogeneous magnetic field . the potential curves are eigenvalues corresponding to the angular oblate spheroidal functions that compose adiabatic basis which depends on the radial variable as a parameter . the matrix elements of radial coupling are integrals in angular variables of the following two types product of angular functions and the first derivative of angular functions in parameter , and product of the first derivatives of angular functions in parameter , respectively . the program calculates also the angular part of the dipole transition matrix elements ( in the length form ) expressed as integrals in angular variables involving product of a dipole operator and angular functions . moreover , the program calculates asymptotic regular and irregular matrix solutions of the coupled adiabatic radial equations at the end of interval in radial variable needed for solving a multi channel scattering problem by the generalized r matrix method . potential curves and radial matrix elements computed by the pothmf program can be used for solving the bound state and multichannel scattering problems . as a test desk , the program is applied to the calculation of the energy values , a short range reaction matrix and corresponding wave functions with the help of the kantbp program . benchmark calculations for the known photoionization cross sections are presented .",
    "target": "eigenvalue and multi channel scattering problems;kantorovich method;finite element method;r matrix calculation;multi channel adiabatic approximation;ordinary differential equations;high order accuracy approximation"
  },
  {
    "source": "completely lazy learning . <eos> local classifiers are sometimes called lazy learners because they do not train a classifier until presented with a test sample . however , such methods are generally not completely lazy because the neighborhood size k ( or other locality parameter ) is usually chosen by cross validation on the training set , which can require significant preprocessing and risks overfitting . we propose a simple alternative to cross validation of the neighborhood size that requires no preprocessing instead of committing to one neighborhood size , average the discriminants for multiple neighborhoods . we show that this forms an expected estimated posterior that minimizes the expected bregman loss with respect to the uncertainty about the neighborhood choice . we analyze this approach for six standard and state of the art local classifiers , including discriminative adaptive metric knn ( dann ) , a local support vector machine ( svm knn ) , hyperplane distance nearest neighbor ( hknn ) , and a new local bayesian quadratic discriminant analysis ( local bda ) . the empirical effectiveness of this technique versus cross validation is confirmed with experiments on seven benchmark data sets , showing that similar classification performance can be attained without any training .",
    "target": "lazy learning;cross validation;quadratic discriminant analysis;bayesian estimation;local learning"
  },
  {
    "source": "a tabu search approach for scheduling hazmat shipments . <eos> vehicle routing and scheduling are two main issues in the hazardous material ( hazmat ) transportation problem . in this paper , we study the problem of managing a set of hazmat transportation requests in terms of hazmat shipment route selection and actual departure time definition . for each hazmat shipment , a set of minimum and equitable risk alternative routes from origin to destination points and a preferred departure time are given . the aim is to assign a route to each hazmat shipment and schedule these shipments on the assigned routes in order to minimize the total shipment delay , while equitably spreading the risk spatially and preventing the risk induced by vehicles traveling too close to each other . we model this hazmat shipment scheduling problem as a job shop scheduling problem with alternative routes . no wait constraints arise in the scheduling model as well , since , supposing that no safe area is available , when a hazmat vehicle starts traveling from the given origin it can not stop until it arrives at the given destination . a tabu search algorithm is proposed for the problem , which is experimentally evaluated on a set of realistic test problems over a regional area , evaluating the provided solutions also with respect to the total route risk and length .",
    "target": "job shop scheduling;tabu search algorithm;hazmat transportation problem"
  },
  {
    "source": "a comparative study of direct forcing immersed boundary lattice boltzmann methods for stationary complex boundaries . <eos> in this study , we assess several interface schemes for stationary complex boundary flows under the direct forcing immersed boundary lattice boltzmann methods ( ib lbm ) based on a split forcing lattice boltzmann equation ( lbe ) . our strategy is to couple various interface schemes , which were adopted in the previous direct forcing immersed boundary methods ( ibm ) , with the split forcing lbe , which enables us to directly use the direct forcing concept in the lattice boltzmann calculation algorithm with a second order accuracy without involving the navier stokes equation . in this study , we investigate not only common diffuse interface schemes but also a sharp interface scheme . for the diffuse interface scheme , we consider explicit and implicit interface schemes . in the calculation of velocity interpolation and force distribution , we use the <digit> and <digit> point discrete delta functions , which give the second order approximation . for the sharp interface scheme , we deal with the exterior sharp interface scheme , where we impose the force density on exterior ( solid ) nodes nearest to the boundary . all tested schemes show a second order overall accuracy when the simulation results of the taylor green decaying vortex are compared with the analytical solutions . it is also confirmed that for stationary complex boundary flows , the sharper the interface scheme , the more accurate the results are . in the simulation of flows past a circular cylinder , the results from each interface scheme are comparable to those from other corresponding numerical schemes . copyright ( c ) <digit> john wiley sons , ltd .",
    "target": "immersed boundary lattice boltzmann method;stationary complex boundary;interface scheme;split forcing lattice boltzmann equation;taylor green decaying vortex;flow past a circular cylinder;direct forcing method"
  },
  {
    "source": "precise euclidean distance transforms in 3d from voxel coverage representation . <eos> we propose a method for computing euclidean distance transform ( edt ) in 3d images . the method utilizes voxel coverage information to increase precision of edt . the method can be used with any vector propagation based edt in 3d . synthetic tests confirm significant improvement in achieved precision . both the related binary and the existing coverage based methods are outperformed .",
    "target": "precision;distance transform;coverage representation;vector propagation dt algorithm;sub voxel accuracy"
  },
  {
    "source": "can we trust digital image forensics . <eos> compared to the prominent role digital images play in nowadays multimedia society , research in the field of image authenticity is still in its infancy . only recently , research on digital image forensics has gained attention by addressing tamper detection and image source identification . however , most publications in this emerging field still lack rigorous discussions of robustness against strategic counterfeiters , who anticipate the existence of forensic techniques . as a result , the question of trustworthiness of digital image forensics arises . this work will take a closer look at two state of the art forensic methods and proposes two counter techniques one to perform resampling operations undetectably and another one to forge traces of image origin . implications for future image forensic systems will be discussed .",
    "target": "digital image forensics;tamper detection;image source identification;tamper hiding"
  },
  {
    "source": "max optimal and sum optimal labelings of graphs . <eos> given a graph g , a function f v ( g ) > <digit> , <digit> , ... , k is a k ranking of g if f ( u ) f ( v ) implies that every u v path contains a vertex w such that f ( w ) > f ( u ) . a k ranking is minimal if the reduction of any label greater than <digit> violates the described ranking property . we consider two norms for minimal rankings . the max optimal norm parallel to f ( g ) parallel to ( infinity ) is the smallest k for which g has a minimal k ranking . this value is also referred to as the rank number chi ( r ) ( g ) . in this paper we introduce the sum optimal norm parallel to f ( g ) parallel to ( <digit> ) which is the minimum sum of all labels over all minimal rankings . we investigate similarities and differences between the two norms . in particular we show rankings for paths and cycles that are sum optimal are also max optimal . ( c ) <digit> elsevier b.v. all rights reserved .",
    "target": "rank number;graph algorithms;vertex coloring"
  },
  {
    "source": "<digit> ( ( v , k ,1 ) ) designs with a point primitive rank <digit> automorphism group of affine type . <eos> <digit> ( ( v , k ,1 ) ) designs with a point primitive rank <digit> automorphism group of affine type are investigated and several new examples are provided .",
    "target": "<digit> ( ( v;k;<digit> ) ) designs;rank <digit> group;orbit;segre variety;05b25;20b25"
  },
  {
    "source": "increase in the releasable pool of synaptic vesicles underlies facilitation . <eos> facilitation is the ability of presynaptic terminals to release neurotransmitter more efficiently following repetitive stimulation . we demonstrated that facilitation can be explained by ca2 dependent vesicles priming and the increase in the number of synaptic vesicles activated for release . employing the model with two ca2 sensors , we computed ca2 concentration at the sites of priming and release , the size of the releasable pool of vesicles , and the rate of transmitter release during repetitive nerve stimulation . the calculated rates of vesicle release and the increase in the releasable pool during facilitation were in agreement with the results of electrophysiology experiments .",
    "target": "lobster neuromuscular junction;synaptic plasticity;neurosecretion;calcium"
  },
  {
    "source": "viscoelastic fracture of multiple cracks in functionally graded materials . <eos> in this paper , the viscoelastic fracture of multiple cracks in a functionally graded strip is studied . the solution of linear elastic crack tip field is investigated at first , using the finite element method . both applied stress load and applied strain load are taken into account . the effects of the crack length , crack spacing , material gradient index and the loading condition on the crack tip field intensity factor are plotted and discussed . according to the correspondence principle , the viscoelastic crack tip field under applied strain is obtained from the linear elastic results . variation of stress intensity factor of the viscoelastic functionally graded strip is analyzed . some useful information for the design of functionally graded materials is provided .",
    "target": "viscoelasticity;linear elasticity;correspondence principle;stress intensity factor"
  },
  {
    "source": "power distribution system optimization by an algorithm for capacitated steiner tree problems with complex flows and arbitrary cost functions . <eos> an algorithm called genetic shortest path algorithm is presented to solve capacitated minimal steiner tree problems in graphs with complex flows and arbitrary arc cost functions , but without negative cycles . voltage constraint can also been taken into consideration by the algorithm . hence , it can solve various power distribution system optimization problems with detailed mathematical models . in the proposed algorithm , a local optimization method based on shortest path algorithm and heuristics is used to find the local optimums , in which the minimum cost objective and all constraints are considered and the specialties of the problems are made good use of . genetic operations are only used to search the global optimum from the local optimums . therefore , this algorithm overcomes the disadvantage of general genetic algorithm in local searching . an example for distribution system planning problem with large scale is given to demonstrate the power of the algorithm .",
    "target": "steiner tree problem;genetic algorithm;power distribution system planning;power distribution system reconfiguration"
  },
  {
    "source": "qmbr ( i ) inverse quantization of minimum bounding rectangles for spatial data compression . <eos> in this paper , we propose qmbr ( i ) , the inverse representation of the quantized minimum bounding rectangles ( mbrs ) scheme , which compresses a minimum bounding rectangle key into one byte for spatial data compression . qmbr ( i ) is a novel spatial data compression scheme that is based on inverse quantization and overcomes the shortcomings of conventional relative coordination or quantization schemes . if a spatial data is far from the starting point of the search region , the relative coordination scheme does not guarantee compression . in a quantization scheme , since the mbrs are expanded , the overlapping of mbrs is increased and the search performance is reduced . the proposed scheme overcomes these shortcomings , and simulation results suggest that it performs better than other schemes .",
    "target": "qmbr;spatial data;spatial data compression;mbr;rmbr;hmbr"
  },
  {
    "source": "two dimensional model of base force element method ( bfem ) on complementary energy principle for geometrically nonlinear problems . <eos> based on the concept of the base forces by gao , a new finite element methodthe base force element method ( bfem ) on complementary energy principle for two dimensional geometrically nonlinear problems is presented using arbitrary meshes . an arbitrary convex polygonal element model of the bfem for geometrically nonlinear problem is derived by assuming that the stress is uniformly distributed on each edges of a plane element . the explicit formulations of the control equations for the bfem are derived using the modified complementary energy principle . the bfem is naturally universal for small displacement and large displacement problems . a number of example problems are solved using the bfem and the results are compared with corresponding analytical solutions . a good agreement of the results using the arbitrary convex polygonal element model of bfem in the large displacement and large rotation calculations , are observed .",
    "target": "two dimensional;base forces;complementary energy;geometrically nonlinear;finite element"
  },
  {
    "source": "theoretical study of xn5 ( ) ( x o , s , se , te ) systems . <eos> a series of xn5 ( ) ( x o , s , se , te ) compounds has been examined with ab initio and density functional theory ( dft ) methods . the five membered nitrogen ring series of structures are global minima and may exist or be characterized due to their significant dissociation barriers ( 29.7 32.7 kcal mol ( <digit> ) ) . nucleus independent chemical shifts ( nics ) criteria and the presence of ( 4n <digit> ) pi electrons confirmed that the five membered nitrogen ring in their structures exhibits characteristics of aromaticity . thus , the strong stability of the five membered nitrogen ring structures may be attributed partially to their aromaticity .",
    "target": "xn5;ab initio;nics;aromaticity;hedms"
  },
  {
    "source": "incremental fault diagnosis . <eos> fault diagnosis is important in improving the circuit design process and the manufacturing yield . diagnosis of today 's complex defects is a challenging problem due to the explosion of the underlying solution space with the increasing number of fault locations and fault models . to tackle this complexity , an incremental diagnosis method is proposed . this method captures faulty lines one at a time using the novel linear time single fault diagnosis algorithms . to capture complex fault effects , a model free incremental diagnosis algorithm is outlined , which alleviates the need for an explicit fault model . to demonstrate the applicability of the proposed method , experiments on multiple stuck at faults , open interconnects and bridging faults are performed . extensive results on combinational and full scan sequential benchmark circuits confirm its resolution and performance .",
    "target": "fault diagnosis;open interconnect;circuit simulation;very large scale integration"
  },
  {
    "source": "dim , a portable , light weight package for information publishing , data transfer and inter process communication . <eos> the real time systems of hep experiments are presently highly distributed , possibly on heterogeneous cpus . in many applications , there is an important need to make information available to a large number of other processes in a transparent way . for this purpose the rpc like systems are not suitable , since most of them rely on polling from the client and one to one connections . dim is a very powerful alternative to those systems . it provides a named space for processes to publish information ( publishers ) and a very simple api for processes willing to use this information ( subscribers ) . it fully handles error recovery at the publisher and subscriber level , without additional software in the application , dim is available on a large variety of platforms and operating systems with c and c bindings . it is presently used in several hep experiments , while it was developed in the delphi experiment and is maintained at cern . we shall present its capabilities and examples of its use in hep experiments in domains ranging from simple data publishing to event transfer , process control or communication layer for an experiment control package ( smi ) . we shall also present prospectives for using it as communications layer for future experiment 's control systems . ( c ) <digit> elsevier science b.v. all rights reserved .",
    "target": "asynchronous communications;heterogeneous distributed systems"
  },
  {
    "source": "roswel workflow language a declarative , resource oriented approach . <eos> well defined business processes are a crucial success factor for deploying soa soku architectures . in this paper , the declarative business process description language roswel which supports applications compatible with roa , is discussed . roswel provides a declarative , reliable and semi automatic composition of restful web services , enriched by the knowledge representation . the paper discusses benefits of roswel , and presents an example of a simple workow that captures essential roswel features .",
    "target": "business process;soku;rest;declarative workflow language"
  },
  {
    "source": "the ( <digit> <digit> ) dimensional burgers equation and its comparative solutions . <eos> in this paper , we will carry out an analytic comparative study between the adomian decomposition method and the differential transformation method . this is achieved by handling the ( <digit> <digit> ) dimensional burgers equation . two numerical simulations have also been carried out to validate and demonstrate efficiency of the two methods . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "adomian decomposition method;differential transformation method;the dimensional burgers equation"
  },
  {
    "source": "modeling the development of goal specificity in mirror neurons . <eos> neurophysiological studies have shown that parietal mirror neurons encode not only actions but also the goal of these actions . although some mirror neurons will fire whenever a certain action is perceived ( goal independently ) , most will only fire if the motion is perceived as part of an action with a specific goal . this result is important for the action understanding hypothesis as it provides a potential neurological basis for such a cognitive ability . it is also relevant for the design of artificial cognitive systems , in particular robotic systems that rely on computational models of the mirror system in their interaction with other agents . yet , to date , no computational model has explicitly addressed the mechanisms that give rise to both goal specific and goal independent parietal mirror neurons . in the present paper , we present a computational model based on a self organizing map , which receives artificial inputs representing information about both the observed or executed actions and the context in which they were executed . we show that the map develops a biologically plausible organization in which goal specific mirror neurons emerge . we further show that the fundamental cause for both the appearance and the number of goal specific neurons can be found in geometric relationships between the different inputs to the map . the results are important to the action understanding hypothesis as they provide a mechanism for the emergence of goal specific parietal mirror neurons and lead to a number of predictions ( <digit> ) learning of new goals may mostly reassign existing goal specific neurons rather than recruit new ones ( <digit> ) input differences between executed and observed actions can explain observed corresponding differences in the number of goal specific neurons and ( <digit> ) the percentage of goal specific neurons may differ between motion primitives .",
    "target": "mirror neurons;action understanding hypothesis;computational model;self organizing map;neural activation patterns"
  },
  {
    "source": "a framework for designing and implementing the ada standard container library . <eos> an open issue of the ada language is the definition of a standard container library . containers in this library ( e.g. , sets , maps and lists ) shall offer some core functionalities that characterise their behaviour ( i.e. , different strategies for managing the elements stored therein ) as well as other general functionalities . among these general functionalities , we are interested in alternative ways for accessing the containers , namely direct access by position and traversals using iterators . in this paper , we present the shortcut based framework ( sbf ) , a framework aimed at providing suitable , uniform , accurate and secure access by position and iterators , while keeping other nice properties such as comprehensibility and changeability . the sbf should be considered as a baseline upon which the ada standard container library can be built . we assess the feasibility of our proposal defining a quality model for container libraries and evaluating the sbf using some metrics defined with the goal question metric approach .",
    "target": "container libraries;access by position;iterators;quality models"
  },
  {
    "source": "stability of block lu factorization for block tridiagonal matrices . <eos> it is showed that if a is i block diagonally dominant ( ii block diagonally dominant ) , then the reduced matrix s preserves the same property . we also give a sufficient condition for the reduced matrix s also to be a block h matrix when a is a block h matrix , and some properties on the comparison matrices mu ( i ) ( a ( k ) ) , mu ( ii ) ( a ( k ) ) , mu ( i ) ( l ) , mu ( i ) ( u ) are obtained . finally , error analysis of block lu factroization for block tridiagonal matrix is presented . ( c ) <digit> elsevier ltd. all rights reserved .",
    "target": "stability;block lu factorization;block tridiagonal matrix;block h matrices;i block diagonally dominant matrices;ii block diagonally dominant matrices"
  },
  {
    "source": "russian dutch double degree masters programme in computational science in the age of global education . <eos> we present a new double degree graduate ( masters ) programme in computational science launched in <digit> by the itmo university , russia and university of amsterdam , the netherlands . we discuss the global aspects of integration of different educational systems and list some funding opportunities . we describe our double degree program curriculum , suggest the timeline of enrollment and studies , and give some examples of student research topics . finally , we discuss the issues of joint programs with russia and suggest possible solutions , analyze the results of the first three student intakes and reflect on the lessons learnt , and share our thoughts and experiences that could be of interest to the international community expanding the educational markets to the vast countries like russia , china or india . the paper is written for education professionals and contains useful information for potential students .",
    "target": "double degree;masters programme;computational science;funding opportunities;curriculum;enrollment;student research;graduate program"
  }
]